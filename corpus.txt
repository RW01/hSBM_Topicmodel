in chaos theory the butterfly effect is the sensitive dependence on initial conditions where a small change at one place in a nonlinear system can result in large differences to a later state the name of the effect coined by edward lorenz is derived from the theoretical example of a hurricane s formation being contingent on whether or not a distant butterfly had flapped its wings several weeks before although the butterfly effect may appear to be an esoteric and unlikely behavior it is exhibited by very simple systems for example a ball placed at the crest of a hill may roll into any of several valleys fully depending on slight differences in initial position the butterfly effect is a common trope in fiction when presenting scenarios involving time travel and with hypotheses where one storyline diverges at the moment of a seemingly minor event resulting in two significantly different outcomes origin of the concept and the term chaos theory and the sensitive dependence on initial conditions was described in the literature in a particular case of the three body problem by henri in he later proposed that such phenomena could be common for example in meteorology in jacques hadamard noted general divergence of trajectories in spaces of negative curvature pierre duhem discussed the possible general significance of this in the idea that one butterfly could eventually have a far reaching ripple effect on subsequent historic events first appears in a sound of thunder a short story by ray bradbury about time travel see literature and print here in lorenz was using a numerical computer model to rerun a weather prediction when as a shortcut on a number in the sequence he entered the decimal instead of entering the full the result was a completely different weather scenario in lorenz published a theoretical study of this effect in a well known paper called deterministic nonperiodic flow elsewhere he said that one meteorologist remarked that if the theory were correct one flap of a seagull s wings could change the course of weather forever following suggestions from colleagues in later speeches and papers lorenz used the more poetic butterfly according to lorenz when he failed to provide a title for a talk he was to present at the meeting of the american association for the advancement of science in philip merilees concocted does the flap of a wings in brazil set off a tornado in texas as a title although a butterfly flapping its wings has remained constant in the expression of this concept the location of the butterfly the consequences and the location of the consequences have varied widely the phrase refers to the idea that a butterfly s wings might create tiny changes in the atmosphere that may ultimately alter the path of a tornado or delay accelerate or even prevent the occurrence of a tornado in another location the flapping wing represents a small change in the initial condition of the system which causes a chain of events leading to large scale alterations of events compare domino effect had the butterfly not flapped its wings the trajectory of the system might have been vastly different note that the butterfly does not cause the tornado the flap of the wings is a part of the initial conditions one set of conditions leads to a tornado while the other set of conditions doesn t it s possible that the set of conditions without the butterfly flapping its wings is the set that leads to a tornado theory and mathematical definition recurrence the approximate return of a system towards its initial conditions together with sensitive dependence on initial conditions are the two main ingredients for chaotic motion they have the practical consequence of making complex systems such as the weather difficult to predict past a certain time range approximately a week in the case of weather since it is impossible to measure the starting atmospheric conditions completely accurately a dynamical system displays sensitive dependence on initial conditions if points arbitrarily close together separate over time at an exponential rate the definition is not topological but essentially metrical if m is the state space for the map formula then formula displays sensitive dependence to initial conditions if for any x in m and any there are y in m with formula such that the definition does not require that all points from a neighborhood separate from the base point x but it requires one positive lyapunov exponent examples the butterfly effect is most familiar in terms of weather it can easily be demonstrated in standard weather prediction models for example the potential for sensitive dependence on initial conditions the butterfly effect has been studied in a number of cases in semiclassical and quantum physics including atoms in strong fields and the anisotropic kepler problem some authors have argued that extreme exponential dependence on initial conditions is not expected in pure quantum treatments however the sensitive dependence on initial conditions demonstrated in classical motion is included in the semiclassical treatments developed by martin gutzwiller and delos and co workers other authors suggest that the butterfly effect can be observed in quantum systems karkuszewski et al consider the time evolution of quantum systems which have slightly different hamiltonians they investigate the level of sensitivity of quantum systems to small changes in their given hamiltonians poulin et al present a quantum algorithm to measure fidelity decay which the rate at which identical initial states diverge when subjected to slightly different dynamics they consider fidelity decay to be closest quantum analog to the purely classical butterfly effect whereas the classical butterfly effect considers the effect of a small change in the position and or velocity of an object in a given hamiltonian system the quantum butterfly effect considers the effect of a small change in the hamiltonian system with a given initial position and velocity this quantum butterfly effect has been demonstrated experimentally quantum and semiclassical treatments of system sensitivity to initial conditions are known as quantum chaos 
chaos theory is a field of study in mathematics with applications in several disciplines including physics engineering economics biology and philosophy chaos theory studies the behavior of dynamical systems that are highly sensitive to initial conditions an effect which is popularly referred to as the butterfly effect small differences in initial conditions such as those due to rounding errors in numerical computation yield widely diverging outcomes for chaotic systems rendering long term prediction impossible in general this happens even though these systems are deterministic meaning that their future behavior is fully determined by their initial conditions with no random elements involved in other words the deterministic nature of these systems does not make them predictable this behavior is known as deterministic chaos or simply chaos chaotic behavior can be observed in many natural systems such as weather explanation of such behavior may be sought through analysis of a chaotic mathematical model or through analytical techniques such as recurrence plots and maps applications chaos theory is applied in many scientific disciplines including geology mathematics microbiology biology computer science economics engineering finance meteorology philosophy physics politics population dynamics psychology and robotics chaotic behavior has been observed in the laboratory in a variety of systems including electrical circuits lasers oscillating chemical reactions fluid dynamics and mechanical and magneto mechanical devices as well as computer models of chaotic processes observations of chaotic behavior in nature include changes in weather the dynamics of satellites in the solar system the time evolution of the magnetic field of celestial bodies population growth in ecology the dynamics of the action potentials in neurons and molecular vibrations there is some controversy over the existence of chaotic dynamics in plate tectonics and in economics chaos theory is currently being applied to medical studies of epilepsy specifically to the prediction of seemingly random seizures by observing initial conditions quantum chaos theory studies how the correspondence between quantum mechanics and classical mechanics works in the context of chaotic systems relativistic chaos describes chaotic systems under general relativity the motion of a system of three or more stars interacting gravitationally the gravitational n body problem is generically chaotic in electrical engineering chaotic systems are used in communications random number generators and encryption systems in numerical analysis the newton raphson method of approximating the roots of a function can lead to chaotic iterations if the function has no real roots chaotic dynamics the requirement for sensitive dependence on initial conditions implies that there is a set of initial conditions of positive measure which do not converge to a cycle of any length sensitivity to initial conditions sensitivity to initial conditions means that each point in such a system is arbitrarily closely approximated by other points with significantly different future trajectories thus an arbitrarily small perturbation of the current trajectory may lead to significantly different future behaviour however it has been shown that the last two properties in the list above actually imply sensitivity to initial conditions and if attention is restricted to intervals the second property implies the other two an alternative and in general weaker definition of chaos uses only the first two properties in the above list it is interesting that the most practically significant condition that of sensitivity to initial conditions is actually redundant in the definition being implied by two or for intervals one purely topological conditions which are therefore of greater interest to mathematicians sensitivity to initial conditions is popularly known as the butterfly effect so called because of the title of a paper given by edward lorenz in to the american association for the advancement of science in washington d c entitled predictability does the flap of a wings in brazil set off a tornado in texas the flapping wing represents a small change in the initial condition of the system which causes a chain of events leading to large scale phenomena had the butterfly not flapped its wings the trajectory of the system might have been vastly different a consequence of sensitivity to initial conditions is that if we start with only a finite amount of information about the system as is usually the case in practice then beyond a certain time the system will no longer be predictable this is most familiar in the case of weather which is generally predictable only about a week ahead the lyapunov exponent characterises the extent of the sensitivity to initial conditions quantitatively two trajectories in phase space with initial separation formula diverge where is the lyapunov exponent the rate of separation can be different for different orientations of the initial separation vector thus there is a whole spectrum of lyapunov exponents the number of them is equal to the number of dimensions of the phase space it is common to just refer to the largest one i e to the maximal lyapunov exponent mle because it determines the overall predictability of the system a positive mle is usually taken as an indication that the system is chaotic there are also measure theoretic mathematical conditions discussed in ergodic theory such as mixing or being a k system which relate to sensitivity of initial conditions and chaos topological mixing topological mixing or topological transitivity means that the system will evolve over time so that any given region or open set of its phase space will eventually overlap with any other given region this mathematical concept of mixing corresponds to the standard intuition and the mixing of colored dyes or fluids is an example of a chaotic system topological mixing is often omitted from popular accounts of chaos which equate chaos with sensitivity to initial conditions however sensitive dependence on initial conditions alone does not give chaos for example consider the simple dynamical system produced by repeatedly doubling an initial value this system has sensitive dependence on initial conditions everywhere since any pair of nearby points will eventually become widely separated however this example has no topological mixing and therefore has no chaos indeed it has extremely simple behaviour all points except tend to infinity density of periodic orbits density of periodic orbits means that every point in the space is approached arbitrarily closely by periodic orbits the one dimensional logistic map defined by x x x is one of the simplest systems with density of periodic orbits for example formula formula formula or approximately is an unstable orbit of period and similar orbits exist for periods etc indeed for all the periods specified by sharkovskii s theorem sharkovskii s theorem is the basis of the li and yorke proof that any one dimensional system which exhibits a regular cycle of period three will also display regular cycles of every other length as well as completely chaotic orbits strange attractors some dynamical systems like the one dimensional logistic map defined by x x x are chaotic everywhere but in many cases chaotic behaviour is found only in a subset of phase space the cases of most interest arise when the chaotic behaviour takes place on an attractor since then a large set of initial conditions will lead to orbits that converge to this chaotic region an easy way to visualize a chaotic attractor is to start with a point in the basin of attraction of the attractor and then simply plot its subsequent orbit because of the topological transitivity condition this is likely to produce a picture of the entire final attractor and indeed both orbits shown in the figure on the right give a picture of the general shape of the lorenz attractor this attractor results from a simple three dimensional model of the lorenz weather system the lorenz attractor is perhaps one of the best known chaotic system diagrams probably because it was not only one of the first but it is also one of the most complex and as such gives rise to a very interesting pattern which looks like the wings of a butterfly unlike fixed point attractors and limit cycles the attractors which arise from chaotic systems known as strange attractors have great detail and complexity strange attractors occur in both continuous dynamical systems such as the lorenz system and in some discrete systems such as the map other discrete dynamical systems have a repelling structure called a julia set which forms at the boundary between basins of attraction of fixed points julia sets can be thought of as strange repellers both strange attractors and julia sets typically have a fractal structure and a fractal dimension can be calculated for them minimum complexity of a chaotic system discrete chaotic systems such as the logistic map can exhibit strange attractors whatever their dimensionality however the bendixson theorem shows that a strange attractor can only arise in a continuous dynamical system specified by differential equations if it has three or more dimensions finite dimensional linear systems are never chaotic for a dynamical system to display chaotic behaviour it has to be either nonlinear or infinite dimensional the theorem states that a two dimensional differential equation has very regular behavior the lorenz attractor discussed above is generated by a system of three differential equations with a total of seven terms on the right hand side five of which are linear terms and two of which are quadratic and therefore nonlinear another well known chaotic attractor is generated by the rossler equations with seven terms on the right hand side only one of which is quadratic nonlinear sprott found a three dimensional system with just five terms on the right hand side and with just one quadratic nonlinearity which exhibits chaos for certain parameter values zhang and heidel showed that at least for dissipative and conservative quadratic systems three dimensional quadratic systems with only three or four terms on the right hand side cannot exhibit chaotic behavior the reason is simply put that solutions to such systems are asymptotic to a two dimensional surface and therefore solutions are well behaved while the theorem means that a continuous dynamical system on the euclidean plane cannot be chaotic two dimensional continuous systems with non euclidean geometry can exhibit chaotic behaviour perhaps surprisingly chaos may occur also in linear systems provided they are infinite dimensional a theory of linear chaos is being developed in the functional analysis a branch of mathematical analysis history an early proponent of chaos theory was henri in the while studying the three body problem he found that there can be orbits which are nonperiodic and yet not forever increasing nor approaching a fixed point in jacques hadamard published an influential study of the chaotic motion of a free particle gliding frictionlessly on a surface of constant negative curvature in the system studied hadamard s billiards hadamard was able to show that all trajectories are unstable in that all particle trajectories diverge exponentially from one another with a positive lyapunov exponent much of the earlier theory was developed almost entirely by mathematicians under the name of ergodic theory later studies also on the topic of nonlinear differential equations were carried out by g d birkhoff m l cartwright and j e littlewood and stephen smale except for smale these studies were all directly inspired by physics the three body problem in the case of birkhoff turbulence and astronomical problems in the case of kolmogorov and radio engineering in the case of cartwright and littlewood although chaotic planetary motion had not been observed experimentalists had encountered turbulence in fluid motion and nonperiodic oscillation in radio circuits without the benefit of a theory to explain what they were seeing despite initial insights in the first half of the twentieth century chaos theory became formalized as such only after mid century when it first became evident for some scientists that linear theory the prevailing system theory at that time simply could not explain the observed behaviour of certain experiments like that of the logistic map what had been beforehand excluded as measure imprecision and simple noise was considered by chaos theories as a full component of the studied systems the main catalyst for the development of chaos theory was the electronic computer much of the mathematics of chaos theory involves the repeated iteration of simple mathematical formulas which would be impractical to do by hand electronic computers made these repeated calculations practical while figures and images made it possible to visualize these systems an early pioneer of the theory was edward lorenz whose interest in chaos came about accidentally through his work on weather prediction in lorenz was using a simple digital computer a royal mcbee lgp to run his weather simulation he wanted to see a sequence of data again and to save time he started the simulation in the middle of its course he was able to do this by entering a printout of the data corresponding to conditions in the middle of his simulation which he had calculated last time to his surprise the weather that the machine began to predict was completely different from the weather calculated before lorenz tracked this down to the computer printout the computer worked with digit precision but the printout rounded variables off to a digit number so a value like was printed as this difference is tiny and the consensus at the time would have been that it should have had practically no effect however lorenz had discovered that small changes in initial conditions produced large changes in the long term outcome lorenz s discovery which gave its name to lorenz attractors showed that even detailed atmospheric modelling cannot in general make long term weather predictions weather is usually predictable only about a week ahead in mandelbrot found recurring patterns at every scale in data on cotton prices beforehand he had studied information theory and concluded noise was patterned like a cantor set on any scale the proportion of noise containing periods to error free periods was a constant thus errors were inevitable and must be planned for by incorporating redundancy mandelbrot described both the noah effect in which sudden discontinuous changes can occur and the joseph effect in which persistence of a value can occur for a while yet suddenly change afterwards this challenged the idea that changes in price were normally distributed in he published how long is the coast of britain statistical self similarity and fractional dimension showing that a coastline s length varies with the scale of the measuring instrument resembles itself at all scales and is infinite in length for an infinitesimally small measuring device arguing that a ball of twine appears to be a point when viewed from far away dimensional a ball when viewed from fairly near dimensional or a curved strand dimensional he argued that the dimensions of an object are relative to the observer and may be fractional an object whose irregularity is constant over different scales self similarity is a fractal for example the menger sponge the gasket and the koch curve or snowflake which is infinitely long yet encloses a finite space and has a fractal dimension of circa in mandelbrot published the fractal geometry of nature which became a classic of chaos theory biological systems such as the branching of the circulatory and bronchial systems proved to fit a fractal model chaos was observed by a number of experimenters before it was recognized e g in by van der pol and in by r l ives however as a graduate student in chihiro hayashi s laboratory at kyoto university yoshisuke ueda was experimenting with analog computers and noticed on nov what he called randomly transitional phenomena yet his advisor did not agree with his conclusions at the time and did not allow him to report his findings until in december the new york academy of sciences organized the first symposium on chaos attended by david ruelle robert may james a yorke coiner of the term chaos as used in mathematics robert shaw a physicist part of the eudaemons group with j doyne farmer and norman packard who tried to find a mathematical method to beat roulette and then created with them the dynamical systems collective in santa cruz california and the meteorologist edward lorenz the following year mitchell feigenbaum published the noted article quantitative universality for a class of nonlinear transformations where he described logistic maps feigenbaum notably discovered the universality in chaos permitting an application of chaos theory to many different phenomena in albert j libchaber during a symposium organized in aspen by pierre hohenberg presented his experimental observation of the bifurcation cascade that leads to chaos and turbulence in convection systems he was awarded the wolf prize in physics in along with mitchell j feigenbaum for his brilliant experimental demonstration of the transition to turbulence and chaos in dynamical systems then in the new york academy of sciences co organized with the national institute of mental health and the office of naval research the first important conference on chaos in biology and medicine there bernardo huberman presented a mathematical model of the eye tracking disorder among schizophrenics this led to a renewal of physiology in the through the application of chaos theory for example in the study of pathological cardiac cycles in per bak chao tang and kurt wiesenfeld published a paper in physical review letters describing for the first time self organized criticality soc considered to be one of the mechanisms by which complexity arises in nature alongside largely lab based approaches such as the sandpile many other investigations have focused on large scale natural or social systems that are known or suspected to display scale invariant behaviour although these approaches were not always welcomed at least initially by specialists in the subjects examined soc has nevertheless become established as a strong candidate for explaining a number of natural phenomena including earthquakes which long before soc was discovered were known as a source of scale invariant behaviour such as the law describing the statistical distribution of earthquake sizes and the omori law describing the frequency of aftershocks solar flares fluctuations in economic systems such as financial markets references to soc are common in econophysics landscape formation forest fires landslides epidemics and biological evolution where soc has been invoked for example as the dynamical mechanism behind the theory of punctuated equilibria put forward by niles eldredge and stephen jay gould given the implications of a scale free distribution of event sizes some researchers have suggested that another phenomenon that should be considered an example of soc is the occurrence of wars these applied investigations of soc have included both attempts at modelling either developing new models or adapting existing ones to the specifics of a given natural system and extensive data analysis to determine the existence and or characteristics of natural scaling laws the same year james gleick published chaos making a new science which became a best seller and introduced the general principles of chaos theory as well as its history to the broad public though his history under emphasized important soviet contributions at first the domain of work of a few isolated individuals chaos theory progressively emerged as a transdisciplinary and institutional discipline mainly under the name of nonlinear systems analysis alluding to thomas kuhn s concept of a paradigm shift exposed in the structure of scientific revolutions many chaologists as some described themselves claimed that this new theory was an example of such a shift a thesis upheld by j gleick the availability of cheaper more powerful computers broadens the applicability of chaos theory currently chaos theory continues to be a very active area of research involving many different disciplines mathematics topology physics population biology biology meteorology astrophysics information theory etc distinguishing random from chaotic data it can be difficult to tell from data whether a physical or other observed process is random or chaotic because in practice no time series consists of pure signal there will always be some form of corrupting noise even if it is present as round off or truncation error thus any real time series even if mostly deterministic will contain some randomness define the error as the difference between the time evolution of the test state and the time evolution of the nearby state a deterministic system will have an error that either remains small stable regular solution or increases exponentially with time chaos a stochastic system will have a randomly distributed error essentially all measures of determinism taken from time series rely upon finding the closest states to a given test state e g correlation dimension lyapunov exponents etc to define the state of a system one typically relies on phase space embedding methods typically one chooses an embedding dimension and investigates the propagation of the error between two nearby states if the error looks random one increases the dimension if you can increase the dimension to obtain a deterministic looking error then you are done though it may sound simple it is not really one complication is that as the dimension increases the search for a nearby state requires a lot more computation time and a lot of data the amount of data required increases exponentially with embedding dimension to find a suitably close candidate if the embedding dimension number of measures per state is chosen too small less than the true value deterministic data can appear to be random but in theory there is no problem choosing the dimension too large the method will work when a non linear deterministic system is attended by external fluctuations its trajectories present serious and permanent distortions furthermore the noise is amplified due to the inherent non linearity and reveals totally new dynamical properties statistical tests attempting to separate noise from the deterministic skeleton or inversely isolate the deterministic part risk failure things become worse when the deterministic component is a non linear feedback system in presence of interactions between nonlinear deterministic components and noise the resulting nonlinear series can display dynamics that traditional tests for nonlinearity are sometimes not able to capture the question of how to distinguish deterministic chaotic systems from stochastic systems has also been discussed in philosophy cultural references chaos theory has been mentioned in numerous movies and works of literature for instance it was mentioned extensively in michael chrichton s novel jurassic park and more briefly in its sequel other examples include the film chaos the butterfly effect the sitcom the big bang theory tom stoppard s play arcadia and the video games tom clancy s and assassin s creed video game the influence of chaos theory in shaping the popular understanding of the world we live in was the subject of the bbc documentary high anxieties the mathematics of chaos directed by david malone chaos theory is also the subject of discussion in the bbc documentary the secret life of chaos presented by the physicist jim al khalili 
in philosophy systems theory science and art emergence is the way complex systems and patterns arise out of a multiplicity of relatively simple interactions emergence is central to the theories of integrative levels and of complex systems definitions the concept has been in use since at least the time of aristotle john stuart mill and julian huxley are just some of the historical luminaries who have written on the concept every resultant is either a sum or a difference of the co operant forces their sum when their directions are the same their difference when their directions are contrary further every resultant is clearly traceable in its components because these are homogeneous and commensurable it is otherwise with emergents when instead of adding measurable motion to measurable motion or things of one kind to other individuals of their kind there is a co operation of things of unlike kinds the emergent is unlike its components insofar as these are incommensurable and it cannot be reduced to their sum or their difference jeffrey goldstein in the school of business at adelphi university provides a current definition of emergence in the journal emergence goldstein initially defined emergence as the arising of novel and coherent structures patterns and properties during the process of self organization in complex systems the common characteristics are radical novelty features not previously observed in systems coherence or correlation meaning integrated wholes that maintain themselves over some period of time a global or macro level i e there is some property of wholeness it is the product of a dynamical process it evolves and it is ostensive it can be perceived for good measure goldstein throws in supervenience downward causation rules or laws have no causal efficacy they do not in fact anything they serve merely to describe regularities and consistent relationships in nature these patterns may be very illuminating and important but the underlying causal agencies must be separately specified though often they are not but that aside the game of chess illustrates precisely why any laws or rules of emergence and evolution are insufficient even in a chess game you cannot use the rules to predict i e the course of any given game indeed you cannot even reliably predict the next move in a chess game why because the involves more than the rules of the game it also includes the players and their unfolding moment by moment decisions among a very large number of available options at each choice point the game of chess is inescapably historical even though it is also constrained and shaped by a set of rules not to mention the laws of physics moreover and this is a key point the game of chess is also shaped by teleonomic cybernetic feedback driven influences it is not simply a self ordered process it involves an organized activity strong and weak emergence the usage of the notion emergence may generally be subdivided into two perspectives that of weak emergence and strong emergence weak emergence describes new properties arising in systems as a result of the interactions at an elemental level emergence in this case is merely part of the language or model that is needed to describe a system s behaviour but if on the other hand systems can have qualities not directly traceable to the system s components but rather to how those components interact and one is willing to accept that a system supervenes on its components then it is difficult to account for an emergent property s cause these new qualities are irreducible to the system s constituent parts the whole is greater than the sum of its parts this view of emergence is called strong emergence some fields in which strong emergence is more widely used include etiology epistemology and ontology although strong emergence is logically possible it is uncomfortably like magic how does an irreducible but supervenient downward causal power arise since by definition it cannot be due to the aggregation of the micro level potentialities such causal powers would be quite unlike anything within our scientific ken this not only indicates how they will discomfort reasonable forms of materialism their mysteriousness will only heighten the traditional worry that emergence entails illegitimately getting something from nothing however the debate about whether or not the whole can be predicted from the properties of the parts misses the point wholes produce unique combined effects but many of these effects may be co determined by the context and the interactions between the whole and its environment s along that same thought arthur koestler stated it is the synergistic effects produced by wholes that are the very cause of the evolution of complexity in nature and used the metaphor of janus to illustrate how the two perspectives strong or holistic vs weak or reductionistic should be treated as perspectives not exclusives and should work together to address the issues of emergence further the ability to reduce everything to simple fundamental laws does not imply the ability to start from those laws and reconstruct the universe the constructionist hypothesis breaks down when confronted with the twin difficulties of scale and complexity at each level of complexity entirely new properties appear psychology is not applied biology nor is biology applied chemistry we can now see that the whole becomes not merely more but very different from the sum of its parts objective or subjective quality the properties of complexity and organization of any system are considered by crutchfield to be subjective qualities determined by the observer defining structure and detecting the emergence of complexity in nature are inherently subjective though essential scientific activities despite the difficulties these problems can be analysed in terms of how model building observers infer from measurements the computational capabilities embedded in non linear processes an notion of what is ordered what is random and what is complex in its environment depends directly on its computational resources the amount of raw measurement data of memory and of time available for estimation and inference the discovery of structure in an environment depends more critically and subtly though on how those resources are organized the descriptive power of the chosen or implicit computational model class for example can be an overwhelming determinant in finding regularity in data on the other hand peter corning argues must the synergies be perceived observed in order to qualify as emergent effects as some theorists claim most emphatically not the synergies associated with emergence are real and measurable even if nobody is there to observe them these are not necessarily incompatible however since while an observer is free to choose the definition of order that they wish to take once it is chosen that definition applies objectively to any system independently of observation emergence in philosophy religion art and human sciences in philosophy emergence is often understood to be a much stronger claim about the etiology of a system s properties an emergent property of a system in this context is one that is not a property of any component of that system but is still a feature of the system as a whole nicolai hartmann one of the first modern philosophers to write on emergence termed this categorial novum new category in religion emergence grounds expressions of religious naturalism in which a sense of the sacred is perceived in the workings of entirely naturalistic processes by which more complex forms arise or evolve from simpler forms examples are detailed in a essay titled the sacred emergence of nature by ursula goodenough and terrence deacon and a essay titled beyond reductionism reinventing the sacred by stuart kauffman in art emergence is used to explore the origins of novelty creativity and authorship some art literary theorists wheeler alexander have proposed alternatives to postmodern understandings of authorship using the complexity sciences and emergence theory they contend that artistic selfhood and meaning are emergent relatively objective phenomena the concept of emergence has also been applied to the theory of literature and art history linguistics cognitive sciences etc by the teachings of jean marie grassin et the university of limoges v esp j fontanille b westphal j vion dury l de l en aux travaux de jean marie grassin bern berlin etc and the article emergence in the international dictionary of literary terms ditl in postcolonial studies the term emerging literature refers to a contemporary body of texts that gaining momentum in the global literary landscape v esp j m grassin ed emerging literatures bern berlin etc peter lang by opposition emergent literature is rather a concept used in the theory of literature emergent properties and processes an emergent behavior or emergent property can appear when a number of simple entities agents operate in an environment forming more complex behaviors as a collective if emergence happens over disparate size scales then the reason is usually a causal relation across different scales in other words there is often a form of top down feedback in systems with emergent properties the processes from which emergent properties result may occur in either the observed or observing system and can commonly be identified by their patterns of accumulating change most generally called growth why emergent behaviours occur include intricate causal relations across different scales and feedback known as interconnectivity the emergent property itself may be either very predictable or unpredictable and unprecedented and represent a new level of the system s evolution the complex behaviour or properties are not a property of any single such entity nor can they easily be predicted or deduced from behaviour in the lower level entities they are irreducible the shape and behaviour of a flock of birds http www newscientist com article flying in vformation gives best view for least effort html or school of fish are also good examples one reason why emergent behaviour is hard to predict is that the number of interactions between components of a system increases combinatorially with the number of components thus potentially allowing for many new and subtle types of behaviour to emerge for example the possible interactions between groups of molecules grows enormously with the number of molecules such that it is impossible for a computer to even list the arrangements for a system as small as molecules on the other hand merely having a large number of interactions is not enough by itself to guarantee emergent behaviour many of the interactions may be negligible or irrelevant or may cancel each other out in some cases a large number of interactions can in fact work against the emergence of interesting behaviour by creating a lot of noise to drown out any emerging signal the emergent behaviour may need to be temporarily isolated from other interactions before it reaches enough critical mass to be self supporting thus it is not just the sheer number of connections between components which encourages emergence it is also how these connections are organised a hierarchical organisation is one example that can generate emergent behaviour a bureaucracy may behave in a way quite different from that of the individual humans in that bureaucracy but perhaps more interestingly emergent behaviour can also arise from more decentralized organisational structures such as a marketplace in some cases the system has to reach a combined threshold of diversity organisation and connectivity before emergent behaviour appears unintended consequences and side effects are closely related to emergent properties luc steels writes a component has a particular functionality but this is not recognizable as a subfunction of the global functionality instead a component implements a behaviour whose side effect contributes to the global functionality each behaviour has a side effect and the sum of the side effects gives the desired functionality in other words the global or macroscopic functionality of a system with emergent functionality is the sum of all side effects of all emergent properties and functionalities systems with emergent properties or emergent structures may appear to defy entropic principles and the second law of thermodynamics because they form and increase order despite the lack of command and central control this is possible because open systems can extract information and order out of the environment emergence helps to explain why the fallacy of division is a fallacy emergent structures in nature emergent structures are patterns that cannot result from a small set of rules or events however there are those who disagree rules are not causal commands natural systems obey rather the interaction of each part with its immediate surroundings causes a complex chain of processes leading to order in some form one might conclude per aristotle that emergent structures are more than the sum of their parts because the emergent order will not arise if the various parts are simply coexisting the interaction of these parts is central emergent structures can be found in many natural phenomena from the physical to the biological domain for example the shape of weather phenomena such as hurricanes are emergent structures the development and growth of complex orderly crystals as driven by the random motion of water molecules within a conducive natural environment is another example of an emergent process where randomness can give rise to complex and deeply attractive orderly structures however crystalline structure and hurricanes are said to have a self organizing phase it is useful to distinguish three forms of emergent structures a first order emergent structure occurs as a result of shape interactions for example hydrogen bonds in water molecules lead to surface tension a second order emergent structure involves shape interactions played out sequentially over time for example changing atmospheric conditions as a snowflake falls to the ground build upon and alter its form finally a third order emergent structure is a consequence of shape time and heritable instructions for example an organism s genetic code sets boundary conditions on the interaction of biological systems in space and time non living physical systems in physics emergence is used to describe a property law or phenomenon which occurs at macroscopic scales in space or time but not at microscopic scales despite the fact that a macroscopic system can be viewed as a very large ensemble of microscopic systems an emergent property need not be more complicated than the underlying non emergent properties which generate it for instance the laws of thermodynamics are remarkably simple even if the laws which govern the interactions between component particles are complex the term emergence in physics is thus used not to signify complexity but rather to distinguish which laws and concepts apply to macroscopic scales and which ones apply to microscopic scales temperature is sometimes used as an example of an emergent macroscopic behaviour in classical dynamics a snapshot of the instantaneous momenta of a large number of particles at equilibrium is sufficient to find the average kinetic energy per degree of freedom which is proportional to the temperature for a small number of particles the instantaneous momenta at a given time are not statistically sufficient to determine the temperature of the system however using the ergodic hypothesis the temperature can still be obtained to arbitrary precision by further averaging the momenta over a long enough time convection in a liquid or gas is another example of emergent macroscopic behaviour that makes sense only when considering differentials of temperature convection cells particularly cells are an example of a self organizing system more specifically a dissipative system whose structure is determined both by the constraints of the system and by random perturbations the possible realizations of the shape and size of the cells depends on the temperature gradient as well as the nature of the fluid and shape of the container but which configurations are actually realized is due to random perturbations thus these systems exhibit a form of symmetry breaking in some theories of particle physics even such basic structures as mass space and time are viewed as emergent phenomena arising from more fundamental concepts such as the higgs boson or strings in some interpretations of quantum mechanics the perception of a deterministic reality in which all objects have a definite position momentum and so forth is actually an emergent phenomenon with the true state of matter being described instead by a wavefunction which need not have a single position or momentum most of the laws of physics themselves as we experience them today appear to have emerged during the course of time making emergence the most fundamental principle in the universe and raising the question of what might be the most fundamental law of physics from which all others emerged chemistry can in turn be viewed as an emergent property of the laws of physics biology including biological evolution can be viewed as an emergent property of the laws of chemistry finally psychology could at least theoretically be understood as an emergent property of neurobiological laws living biological systems emergence and evolution life is a major source of complexity and evolution is the major process behind the varying forms of life in this view evolution is the process describing the growth of complexity in the natural world and in speaking of the emergence of complex living beings and life forms this view refers therefore to processes of sudden changes in evolution synergistic effects of various kinds have played a major causal role in the evolutionary process generally and in the evolution of cooperation and complexity in particular natural selection is often portrayed as a or is personified as a causal agency in reality the differential of a trait or an adaptation is a consequence of the functional effects it produces in relation to the survival and reproductive success of a given organism in a given environment it is these functional effects that are ultimately responsible for the trans generational continuities and changes in nature evolutionary processes causation is iterative effects are also causes and this is equally true of the synergistic effects produced by emergent systems in other words emergence itself has been the underlying cause of the evolution of emergent phenomena in biological evolution it is the synergies produced by organized systems that are the key swarming is a well known behaviour in many animal species from marching locusts to schooling fish to flocking birds emergent structures are a common strategy found in many animal groups colonies of ants mounds built by termites swarms of bees shoals schools of fish flocks of birds and herds packs of mammals an example to consider in detail is an ant colony the queen does not give direct orders and does not tell the ants what to do instead each ant reacts to stimuli in the form of chemical scent from larvae other ants intruders food and build up of waste and leaves behind a chemical trail which in turn provides a stimulus to other ants here each ant is an autonomous unit that reacts depending only on its local environment and the genetically encoded rules for its variety of ant despite the lack of centralized decision making ant colonies exhibit complex behavior and have even been able to demonstrate the ability to solve geometric problems for example colonies routinely find the maximum distance from all colony entrances to dispose of dead bodies organization of life a broader example of emergent properties in biology is viewed in the biological organisation of life ranging from the subatomic level to the entire biosphere for example individual atoms can be combined to form molecules such as polypeptide chains which in turn fold and refold to form proteins which in turn create even more complex structures these proteins assuming their functional status from their spatial conformation interact together and with other molecules to achieve higher biological functions and eventually create an organism another example is how cascade phenotype reactions as detailed in chaos theory arise from individual genes mutating respective positioning at the highest level all the biological communities in the world form the biosphere where its human participants form societies and the complex interactions of meta social systems such as the stock market emergence in humanity spontaneous order groups of human beings left free to each regulate themselves tend to produce spontaneous order rather than the meaningless chaos often feared this has been observed in society at least since chuang tzu in ancient china a classic traffic roundabout is a good example with cars moving in and out with such effective organization that some modern cities have begun replacing stoplights at problem intersections with traffic circles http www terrain org articles siegman htm and getting better results emergent processes or behaviours can be seen in many places such as traffic patterns cities political systems of governance cabal and market dominant minority phenomena in politics and economics organizational phenomena in computer simulations and cellular automata whenever you have a multitude of individuals interacting with one another there often comes a moment when disorder gives way to order and something new emerges a pattern a decision a structure or a change in direction miller economics the stock market or any market for that matter is an example of emergence on a grand scale as a whole it precisely regulates the relative security prices of companies across the world yet it has no leader there is no one entity which controls the workings of the entire market agents or investors have knowledge of only a limited number of companies within their portfolio and must follow the regulatory rules of the market and analyse the transactions individually or in large groupings trends and patterns emerge which are studied intensively by technical analysts money money insofar as being a medium of exchange and of deferred payment is also an example of an emergent phenomenon between market participators in their strive to possess a commodity with greater marketability than their own commodity such that the possession of these more marketable commodities money facilitate the search for commodities that participators want e g consumables world wide web and the internet the world wide web is a popular example of a decentralized system exhibiting emergent properties there is no central organization rationing the number of links yet the number of links pointing to each page follows a power law in which a few pages are linked to many times and most pages are seldom linked to a related property of the network of links in the world wide web is that almost any pair of pages can be connected to each other through a relatively short chain of links although relatively well known now this property was initially unexpected in an unregulated network it is shared with many other types of networks called small world networks internet traffic can also exhibit some seemingly emergent properties in the congestion control mechanism tcp flows can become globally synchronized at bottlenecks simultaneously increasing and then decreasing throughput in coordination congestion widely regarded as a nuisance is possibly an emergent property of the spreading of bottlenecks across a network in high traffic flows which can be considered as a phase transition see review of related research in another important example of emergence in web based systems is social bookmarking also called collaborative tagging in social bookmarking systems users assign tags to resources shared with other users which gives rise to a type of information organisation that emerges from this crowdsourcing process recent research which analyzes empirically the complex dynamics of such systems has shown that consensus on stable distributions and a simple form of shared vocabularies does indeed emerge even in the absence of a central controlled vocabulary some believe that this could be because users who contribute tags all use the same language and they share similar semantic structures underlying the choice of words the convergence in social tags may therefore be interpreted as the emergence of structures as people who have similar semantic interpretation collaboratively index online information a process called semantic imitation architecture and cities emergent structures appear at many different levels of organization or as spontaneous order emergent self organization appears frequently in cities where no planning or zoning entity predetermines the layout of the city the interdisciplinary study of emergent behaviors is not generally considered a field but divided across its application or problem domains architects and landscape architects may not design all the pathways of a complex of buildings instead they might let usage patterns emerge and then place pavement where pathways have become worn in the on course action and vehicle progression of the urban challenge could possibly be regarded as an example of cybernetic emergence patterns of road use indeterministic obstacle clearance times etc will work together to form a complex emergent pattern that can not be deterministically planned in advance architecture firms that work specifically with the concept of emergence as it relates to the built environment include emergent architecture founded by tom wiscombe in the architectural school of christopher alexander takes a deeper approach to emergence attempting to rewrite the process of urban growth itself in order to affect form establishing a new methodology of planning and design tied to traditional practices an emergent urbanism urban emergence has also been linked to theories of urban complexity and urban evolution building ecology is a conceptual framework for understanding architecture and the built environment as the interface between the dynamically interdependent elements of buildings their occupants and the larger environment rather than viewing buildings as inanimate or static objects building ecologist hal levin views them as interfaces or intersecting domains of living and non living systems the microbial ecology of the indoor environment is strongly dependent on the building materials occupants contents environmental context and the indoor and outdoor climate the strong relationship between atmospheric chemistry and indoor air quality and the chemical reactions occurring indoors the chemicals may be nutrients neutral or biocides for the microbial organisms the microbes produce chemicals that affect the building materials and occupant health and well being humans manipulate the temperature and humidity to achieve comfort with the concomitant effects on the microbes that populate and evolve eric bonabeau s attempt to define emergent phenomena is through traffic traffic jams are actually very complicated and mysterious on an individual level each driver is trying to get somewhere and is following or breaking certain rules some legal the speed limit and others societal or personal slow down to let another driver change into your lane but a traffic jam is a separate and distinct entity that emerges from those individual behaviors gridlock on a highway for example can travel backward for no apparent reason even as the cars are moving forward he has also likened emergent phenomena to the analysis of market trends and employee behavior computer ai some artificially intelligent computer applications utilize emergent behavior for animation one example is boids which mimics the swarming behavior of birds language it has been argued that language or at least language change is an emergence phenomenon while each speaker merely tries to reach her or his own communicative goals she or he uses language in a particular way if enough speakers behave in that way language is changed in a wider sense the norms of a language i e the linguistic conventions of its speech society can be seen as a system emerging from long time participation in communicative problem solving in various social circumstances emergent change processes within the field of group facilitation and organization development there have been a number of new group processes that are designed to maximize emergence and self organization by offering a minimal set of effective initial conditions examples of these processes include appreciative inquiry future search the world cafe or knowledge cafe open space technology and others holman emergence in political philosophy economist and philosopher friedrich hayek wrote about emergence in the context of law politics and markets his theories set out the difference between cosmos or grown order that is emergence and taxis or made order hayek dismisses philosophies that do not adequately recognize the emergent nature of society and which describe it as the conscious creation of a rational agent be it god the sovereign or any kind of personified body politic such as hobbes s leviathan the most important social structures including the laws nomos governing the relations between individual persons are emergent according to hayek while the idea of laws and markets as emergent phenomena comes fairly naturally to an economist and was indeed present in the works of early economists such as bernard mandeville david hume and adam smith hayek traces the development of ideas based on spontaneous order throughout the history of western thought occasionally going as far back as the presocratics in this he follows karl popper who blamed the idea of the state as a made order on plato in the open society and its enemies emergentism is a rejection of the state on the grounds that it is a perversion of the emergent rules that societies form spontaneously some century classical liberals notably gustave de molinari and bastiat were known advocates of an emergent society and wrote about the concepts in detail see the production of security and the law respectively 
in mathematics particularly in dynamical systems a bifurcation diagram shows the possible long term values equilibria fixed points or periodic orbits of a system as a function of a bifurcation parameter in the system it is usual to represent stable solutions with a solid line and unstable solutions with a dotted line bifurcations in discrete dynamical systems logistic map the bifurcation parameter r is shown on the horizontal axis of the plot and the vertical axis shows the possible long term population values of the logistic function only the stable solutions are shown here there are many other unstable solutions which are not shown in this diagram the bifurcation diagram nicely shows the forking of the possible periods of stable orbits from to to to etc each of these bifurcation points is a period doubling bifurcation the ratio of the lengths of successive intervals between values of r for which bifurcation occurs converges to the first feigenbaum constant real quadratic map the map is formula symmetry breaking in bifurcation sets in a dynamical system such as which is structurally stable when formula if a bifurcation diagram is plotted treating formula as the bifurcation parameter but for different values of formula the case formula is the symmetric pitchfork bifurcation when formula we say we have a pitchfork with broken symmetry this is illustrated in the animation on the right 
in fluid dynamics turbulence or turbulent flow is a flow regime characterized by chaotic and stochastic property changes this includes low momentum diffusion high momentum convection and rapid variation of pressure and velocity in space and time nobel laureate richard feynman described turbulence as the most important unsolved problem of classical physics flow in which the kinetic energy dies out due to the action of fluid molecular viscosity is called laminar flow while there is no theorem relating the non dimensional reynolds number re to turbulence flows at reynolds numbers larger than are typically but not necessarily turbulent while those at low reynolds numbers usually remain laminar in poiseuille flow for example turbulence can first be sustained if the reynolds number is larger than a critical value of about moreover the turbulence is generally interspersed with laminar flow until a larger reynolds number of about in turbulent flow unsteady vortices appear on many scales and interact with each other drag due to boundary layer skin friction increases the structure and location of boundary layer separation often changes sometimes resulting in a reduction of overall drag although laminar turbulent transition is not governed by reynolds number the same transition occurs if the size of the object is gradually increased or the viscosity of the fluid is decreased or if the density of the fluid is increased features turbulent diffusion is usually described by a turbulent diffusion coefficient this turbulent diffusion coefficient is defined in a phenomenological sense by analogy with the molecular diffusivities but it does not have a true physical meaning being dependent on the flow conditions and not a property of the fluid itself in addition the turbulent diffusivity concept assumes a constitutive relation between a turbulent flux and the gradient of a mean variable similar to the relation between flux and gradient that exists for molecular transport in the best case this assumption is only an approximation nevertheless the turbulent diffusivity is the simplest approach for quantitative analysis of turbulent flows and many models have been postulated to calculate it for instance in large bodies of water like oceans this coefficient can be found using richardson s four third power law and is governed by the random walk principle in rivers and large ocean currents the diffusion coefficient is given by variations of elder s formula turbulence causes the formation of eddies of many different length scales most of the kinetic energy of the turbulent motion is contained in the large scale structures the energy cascades from these large scale structures to smaller scale structures by an inertial and essentially inviscid mechanism this process continues creating smaller and smaller structures which produces a hierarchy of eddies eventually this process creates structures that are small enough that molecular diffusion becomes important and viscous dissipation of energy finally takes place the scale at which this happens is the kolmogorov length scale although it is possible to find some particular solutions of the navier stokes equations governing fluid motion all such solutions are unstable at large reynolds numbers sensitive dependence on the initial and boundary conditions makes fluid flow irregular both in time and in space so that a statistical description is needed russian mathematician andrey kolmogorov proposed the first statistical theory of turbulence based on the aforementioned notion of the energy cascade an idea originally introduced by richardson and the concept of self similarity as a result the kolmogorov microscales were named after him it is now known that the self similarity is broken so the statistical description is presently modified still a complete description of turbulence remains one of the unsolved problems in physics according to an apocryphal story werner heisenberg was asked what he would ask god given the opportunity his reply was when i meet god i am going to ask him two questions why relativity and why turbulence i really believe he will have an answer for the first a similar witticism has been attributed to horace lamb who had published a noted text book on hydrodynamics choice being quantum electrodynamics instead of relativity and turbulence lamb was quoted as saying in a speech to the british association for the advancement of science i am an old man now and when i die and go to heaven there are two matters on which i hope for enlightenment one is quantum electrodynamics and the other is the turbulent motion of fluids and about the former i am rather optimistic a more detailed presentation of turbulence with emphasis on high reynolds number flow intended for a general readership of physicists and applied mathematicians is found in the scholarpedia articles by r benzi and u frisch and by g falkovich heat and momentum transfer when flow is turbulent particles exhibit additional transverse motion which enhances the rate of energy and momentum exchange between them thus increasing the heat transfer and the friction coefficient formula and similarly for temperature formula and pressure formula where the primed quantities denote fluctuations superposed to the mean this decomposition of a flow variable into a mean value and a turbulent fluctuation was originally proposed by osborne reynolds in and is considered to be the beginning of the systematic mathematical analysis of turbulent flow as a sub field of fluid dynamics while the mean values are taken as predictable variables determined by dynamics laws the turbulent fluctuations are regarded as stochastic variables the heat flux and momentum transfer represented by the shear stress formula in the direction normal to the flow for a given time are formula where formula is the heat capacity at constant pressure formula is the density of the fluid formula is the coefficient of turbulent viscosity and formula is the turbulent thermal conductivity kolmogorov s theory of richardson s notion of turbulence was that a turbulent flow is composed by eddies of different sizes the sizes define a characteristic length scale for the eddies which are also characterized by velocity scales and time scales turnover time dependent on the length scale the large eddies are unstable and eventually break up originating smaller eddies and the kinetic energy of the initial large eddy is divided into the smaller eddies that stemmed from it these smaller eddies undergo the same process giving rise to even smaller eddies which inherit the energy of their predecessor eddy and so on in this way the energy is passed down from the large scales of the motion to smaller scales until reaching a sufficiently small length scale such that the viscosity of the fluid can effectively dissipate the kinetic energy into internal energy in his original theory of kolmogorov postulated that for very high reynolds number the small scale turbulent motions are statistically isotropic i e no preferential spatial direction could be discerned in general the large scales of a flow are not isotropic since they are determined by the particular geometrical features of the boundaries the size characterizing the large scales will be denoted as l kolmogorov s idea was that in the richardson s energy cascade this geometrical and directional information is lost while the scale is reduced so that the statistics of the small scales has a universal character they are the same for all turbulent flows when the reynolds number is sufficiently high thus kolmogorov introduced a second hypothesis for very high reynolds numbers the statistics of small scales are universally and uniquely determined by the viscosity formula and the rate of energy dissipation formula with only these two parameters the unique length that can be formed by dimensional analysis is this is today known as the kolmogorov length scale see kolmogorov microscales a turbulent flow is characterized by a hierarchy of scales through which the energy cascade takes place dissipation of kinetic energy takes place at scales of the order of kolmogorov length formula while the input of energy into the cascade comes from the decay of the large scales of order l these two scales at the extremes of the cascade can differ by several orders of magnitude at high reynolds numbers in between there is a range of scales each one with its own characteristic length r that has formed at the expense of the energy of the large ones these scales are very large compared with the kolmogorov length but still very small compared with the large scale of the flow i e formula since eddies in this range are much larger than the dissipative eddies that exist at kolmogorov scales kinetic energy is essentially not dissipated in this range and it is merely transferred to smaller scales until viscous effects become important as the order of the kolmogorov scale is approached within this range inertial effects are still much larger than viscous effects and it is possible to assume that viscosity does not play a role in their internal dynamics for this reason this range is called inertial range hence a third hypothesis of kolmogorov was that at very high reynolds number the statistics of scales in the range formula are universally and uniquely determined by the scale r and the rate of energy dissipation formula where k is the fourier transform of the velocity field thus e k d k represents the contribution to the kinetic energy from all the fourier modes with k k k d k and therefore where formula is the mean turbulent kinetic energy of the flow the wavenumber k corresponding to length scale r is formula therefore by dimensional analysis the only possible form for the energy spectrum function according with the third kolmogorov s hypothesis is where c would be a universal constant this is one of the most famous results of kolmogorov theory and considerable experimental evidence has accumulated that supports it that is the difference in velocity between points separated by a vector r since the turbulence is assumed isotropic the velocity increment depends only on the modulus of r velocity increments are useful because they emphasize the effects of scales of the order of the separation r when statistics are computed the statistical scale invariance implies that the scaling of velocity increments should occur with a unique scaling exponent formula so that when r is scaled by a factor formula should have the same statistical distribution as with formula independent of the scale r from this fact and other results of kolmogorov theory it follows that the statistical moments of the velocity increments known as structure functions in turbulence should scale as where the brackets denote the statistical average and the formula would be universal constants there is considerable evidence that turbulent flows deviate from this behavior the scaling exponents deviate from the n value predicted by the theory becoming a non linear function of the order n of the structure function the universality of the constants have also been questioned for low orders the discrepancy with the kolmogorov n value is very small which explain the success of kolmogorov theory in regards to low order statistical moments in particular it can be shown that when the energy spectrum follows a power law with formula the second order structure function has also a power law with the form since the experimental values obtained for the second order structure function only deviate slightly from the value predicted by kolmogorov theory the value for p is very near to differences are about thus the kolmogorov spectrum is generally observed in turbulence however for high order structure functions the difference with the kolmogorov scaling is significant and the breakdown of the statistical self similarity is clear this behavior and the lack of universality of the formula constants are related with the phenomenon of intermittency in turbulence this is an important area of research in this field and a major goal of the modern theory of turbulence is to understand what is really universal in the inertial range 
a fractal dimension is a ratio providing a statistical index of complexity comparing how detail in a pattern strictly speaking a fractal pattern changes with the scale at which it is measured it has also been characterized as a measure of the space filling capacity of a pattern that tells how a fractal scales differently than the space it is embedded in a fractal dimension is greater than the dimension of the space containing it and does not have to be an integer the essential idea of fractured dimensions has a long history in mathematics but the term itself was brought to the fore by mandelbrot based on his paper on self similarity in which he discussed fractional dimensions in that paper mandelbrot cited previous work by lewis fry richardson describing the counter intuitive notion that a coastline s measured length changes with the length of the measuring stick used see fig in terms of that notion the fractal dimension of a coastline quantifies how the number of scaled measuring sticks required to measure the coastline changes with the scale applied to the stick there are several formal mathematical definitions of fractal dimension that build on this basic concept of change in detail with change in scale one non trivial example is the fractal dimension of a koch snowflake it has a topological dimension of but it is by no means a rectifiable curve the length of the curve between any two points on the koch snowflake is infinite no small piece of it is line like but rather is composed of an infinite number of segments joined at different angles the fractal dimension of a curve can be explained intuitively thinking of a fractal line as an object too detailed to be one dimensional but too simple to be two dimensional therefore its dimension might best be described not by its usual topological dimension of but by its fractal dimension which in this case is a number between one and two introduction image segment fractal jpg thumb right figure a segment quadric fractal scaled and viewed through boxes of different sizes the pattern illustrates self similarity the theoretical fractal dimension for this fractal is its empirical fractal dimension from box counting analysis is using fractal analysis software a fractal dimension is an index for characterizing fractal patterns or sets by quantifying their complexity as a ratio of the change in detail to the change in scale several types of fractal dimension can be measured theoretically and empirically see fig fractal dimensions are used to characterize a broad spectrum of objects ranging from the abstract to practical phenomena including turbulence river networks urban growth human physiology medicine and market trends the essential idea of fractional or fractal dimensions has a long history in mathematics that can be traced back to the but the terms fractal and fractal dimension were coined by mathematician mandelbrot in fractal dimensions were first applied as an index characterizing complex geometric forms for which the details seemed more important than the gross picture for sets describing ordinary geometric shapes the theoretical fractal dimension equals the set s familiar euclidean or topological dimension thus it is for sets describing points dimensional sets for sets describing lines dimensional sets having length only for sets describing surfaces dimensional sets having length and width and for sets describing volumes dimensional sets having length width and height but this changes for fractal sets if the theoretical fractal dimension of a set exceeds its topological dimension the set is considered to have fractal geometry unlike topological dimensions the fractal index can take non integer values indicating that a set fills its space qualitatively and quantitatively differently than an ordinary geometrical set does for instance a curve with fractal dimension very near to say behaves quite like an ordinary line but a curve with fractal dimension winds convolutedly through space very nearly like a surface similarly a surface with fractal dimension of fills space very much like an ordinary surface but one with a fractal dimension of folds and flows to fill space rather nearly like a volume this general relationship can be seen in the two images of fractal curves in fig and fig the segment contour in fig convoluted and space filling has a fractal dimension of compared to the perceptibly less complex koch curve in fig which has a fractal dimension of the relationship of an increasing fractal dimension with space filling might be taken to mean fractal dimensions measure density but that is not so the two are not strictly correlated instead a fractal dimension measures complexity a concept related to certain key features of fractals self similarity and detail or irregularity these features are evident in the two examples of fractal curves both are curves with topological dimension of so one might hope to be able to measure their length or slope as with ordinary lines but we cannot do either of these things because fractal curves have complexity in the form of self similarity and detail that ordinary lines lack the self similarity lies in the infinite scaling and the detail in the defining elements of each set the length between any two points on these curves is undefined because the curves are theoretical constructs that never stop repeating themselves every smaller piece is composed of an infinite number of scaled segments that look exactly like the first iteration these are not rectifiable curves meaning they cannot be measured by being broken down into many segments approximating their respective lengths they cannot be characterized by finding their lengths or slopes however their fractal dimensions can be determined which shows that both fill space more than ordinary lines but less than surfaces and allows them to be compared in this regard note that the two fractal curves described above show a type of self similarity that is exact with a repeating unit of detail that is readily visualized this sort of structure can be extended to other spaces e g a fractal that extends the koch curve into d space has a theoretical d however such neatly countable complexity is only one example of the self similarity and detail that are present in fractals the example of the coast line of britain for instance exhibits self similarity of an approximate pattern with approximate scaling overall fractals show several types and degrees of self similarity and detail that may not be easily visualized these include as examples strange attractors for which the detail has been described as in essence smooth portions piling up the julia set which can be seen to be complex swirls upon swirls and heart rates which are patterns of rough spikes repeated and scaled in time fractal complexity may not always be resolvable into easily grasped units of detail and scale without complex analytic methods but it is still quantifiable through fractal dimensions history the terms fractal dimension and fractal were coined by mandelbrot in about a decade after he published his paper on self similarity in the coastline of britain various historical authorities credit him with also synthesizing centuries of complicated theoretical mathematics and engineering work and applying them in a new way to study complex geometries that defied description in usual linear terms the earliest roots of what mandelbrot synthesized as the fractal dimension have been traced clearly back to writings about undifferentiable infinitely self similar functions which are important in the mathematical definition of fractals around the time that calculus was discovered in the mid there was a lull in the published work on such functions for a time after that then a renewal starting in the late with the publishing of mathematical functions and sets that are today called canonical fractals such as the eponymous works of von koch sierpinski and julia but at the time of their formulation were often considered antithetical mathematical monsters these works were accompanied by perhaps the most pivotal point in the development of the concept of a fractal dimension through the work of hausdorff in the early who defined a fractional dimension that has come to be named after him and is frequently invoked in defining modern fractals see fractal history for more information role of scaling this scaling rule typifies conventional rules about geometry and dimension for lines it quantifies that because formula when formula as in the example above formula and for squares because formula when formula formula that is for a fractal described by formula when formula formula a non integer dimension that suggests the fractal has a dimension not equal to the space it resides in the scaling used in this example is the same scaling of the koch curve and snowflake of note these images themselves are not true fractals because the scaling described by the value of formula cannot continue infinitely for the simple reason that the images only exist to the point of their smallest component a pixel the theoretical pattern that the digital images represent however has no discrete pixel like pieces but rather is composed of an infinite number of infinitely scaled segments joined at different angles and does indeed have a fractal dimension of d is not a unique descriptor as is the case with dimensions determined for lines squares and cubes fractal dimensions are general descriptors that do not uniquely define patterns the value of d for the koch fractal discussed above for instance quantifies the pattern s inherent scaling but does not uniquely describe nor provide enough information to reconstruct it many fractal structures or patterns could be constructed that have the same scaling relationship but are dramatically different from the koch curve as is illustrated in figure for examples of how fractal patterns can be constructed see fractal sierpinski triangle mandelbrot set diffusion limited aggregation examples the concept of fractal dimension described in this article is a basic view of a complicated construct the examples discussed here were chosen for clarity and the scaling unit and ratios were known ahead of time in practise however fractal dimensions can be determined using techniques that approximate scaling and detail from limits estimated from regression lines over log vs log plots of size vs scale several formal mathematical definitions of different types of fractal dimension are listed below although for some classic fractals all these dimensions coincide in general they are not equivalent estimating from real world data the fractal dimension measures described in this article are for formally defined fractals however many real world phenomena also exhibit limited or statistical fractal properties and fractal dimensions have been estimated for sampled data from many such phenomena using computer based fractal analysis techniques practical dimension estimates are affected by various methodological issues and are sensitive to numerical or experimental noise and limitations in the amount of data nonetheless the field is rapidly growing and as evidenced by searching databases such as pubmed the past decade has seen methods develop from being largely theoretical to the point where estimated fractal dimensions for statistically self similar phenomena have many practical applications in multifarious fields including diagnostic imaging physiology neuroscience medicine physics image analysis acoustics riemann zeta zeros and electrochemical processes 
history during the first half of the twentieth century chaotic behavior in mechanics was recognized as in the three body problem in celestial mechanics but not well understood the foundations of modern quantum mechanics were laid in that period essentially leaving aside the issue of the quantum classical correspondence in systems whose classical limit exhibit chaos approaches questions related to the correspondence principle arise in many different branches of physics ranging from nuclear to atomic molecular and solid state physics and even to acoustics microwaves and optics important observations often associated with classically chaotic quantum systems are spectral level repulsion dynamical localization in time evolution e g ionization rates of atoms and enhanced stationary wave intensities in regions of space where classical dynamics exhibits only unstable trajectories as in scattering in the semiclassical approach of quantum chaos phenomena are identified in spectroscopy by analyzing the statistical distribution of spectral lines and by connecting spectral periodicities with classical orbits other phenomena show up in the time evolution of a quantum system or in its response to various types of external forces in some contexts such as acoustics or microwaves wave patterns are directly observable and exhibit irregular amplitude distributions quantum chaos typically deals with systems whose properties need to be calculated using either numerical techniques or approximation schemes see e g dyson series simple and exact solutions are precluded by the fact that the system s constituents either influence each other in a complex way or depend on temporally varying external forces quantum mechanics in non perturbative regimes for conservative systems the goal of quantum mechanics in non perturbative regimes is to find the eigenvalues and eigenvectors of a hamiltonian of the form where formula is separable in some coordinate system formula is non separable in the coordinate system in which formula is separated and formula is a parameter which cannot be considered small physicists have historically approached problems of this nature by trying to find the coordinate system in which the non separable hamiltonian is smallest and then treating the non separable hamiltonian as a perturbation finding constants of motion so that this separation can be performed can be a difficult sometimes impossible analytical task solving the classical problem can give valuable insight into solving the quantum problem if there are regular classical solutions of the same hamiltonian then there are at least approximate constants of motion and by solving the classical problem we gain clues how to find them other approaches have been developed in recent years one is to express the hamiltonian in different coordinate systems in different regions of space minimizing the non separable part of the hamiltonian in each region wavefunctions are obtained in these regions and eigenvalues are obtained by matching boundary conditions another approach is numerical matrix diagonalization if the hamiltonian matrix is computed in any complete basis eigenvalues and eigenvectors are obtained by diagonalizing the matrix however all complete basis sets are infinite and we need to truncate the basis and still obtain accurate results these techniques boil down to choosing a truncated basis from which accurate wavefunctions can be constructed the computational time required to diagonalize a matrix scales as formula where formula is the dimension of the matrix so it is important to choose the smallest basis possible from which the relevant wavefunctions can be constructed it is also convenient to choose a basis in which the matrix is sparse and or the matrix elements are given by simple algebraic expressions because computing matrix elements can also be a computational burden a given hamiltonian shares the same constants of motion for both classical and quantum dynamics quantum systems can also have additional quantum numbers corresponding to discrete symmetries such as parity conservation from reflection symmetry however if we merely find quantum solutions of a hamiltonian which is not approachable by perturbation theory we may learn a great deal about quantum solutions but we have learned little about quantum chaos nevertheless learning how to solve such quantum problems is an important part of answering the question of quantum chaos correlating statistical descriptions of quantum mechanics with classical behavior statistical measures of quantum chaos were born out of a desire to quantify spectral features of complex systems random matrix theory was developed in an attempt to characterize spectra of complex nuclei the remarkable result is that the statistical properties of many systems with unknown hamiltonians can be predicted using random matrices of the proper symmetry class furthermore random matrix theory also correctly predicts statistical properties of the eigenvalues of many chaotic systems with known hamiltonians this makes it useful as a tool for characterizing spectra which require large numerical efforts to compute a number of statistical measures are available for quantifying spectral features in a simple way it is of great interest whether or not there are universal statistical behaviors of classically chaotic systems the statistical tests mentioned here are universal at least to systems with few degrees of freedom berry and tabor have put forward strong arguments for a poisson distribution in the case of regular motion and heusler et al present a semiclassical explanation of the so called bohigas giannoni schmit conjecture which asserts universality of spectral fluctuations in chaotic dynamics the nearest neighbor distribution nnd of energy levels is relatively simple to interpret and it has been widely used to describe quantum chaos qualitative observations of level repulsions can be quantified and related to the classical dynamics in addition systems which display chaotic classical motion are expected to be characterized by the statistics of random matrix eigenvalue ensembles for systems invariant under time reversal the energy level statistics of a number of chaotic systems have been shown to be in good agreement with the predictions of the gaussian orthogonal ensemble goe of random matrices and it has been suggested that this phenomenon is generic for all chaotic systems with this symmetry if the normalized spacing between two energy levels is formula the normalized distribution of spacings is well approximated by which is the wigner distribution many hamiltonian systems which are classically integrable non chaotic have been found to have quantum solutions that yield nearest neighbor distributions which follow the poisson distributions similarly many systems which exhibit classical chaos have been found with quantum solutions yielding a wigner distribution thus supporting the ideas above one notable exception is diamagnetic lithium which though exhibiting classical chaos demonstrates wigner chaotic statistics for the even parity energy levels and nearly poisson regular statistics for the odd parity energy level distribution semiclassical methods periodic orbit theory periodic orbit theory gives a recipe for computing spectra from the periodic orbits of a system in contrast to the einstein brillouin keller method of action quantization which applies only to integrable or near integrable systems and computes individual eigenvalues from each trajectory periodic orbit theory is applicable to both integrable and non integrable systems and asserts that each periodic orbit produces a sinusoidal fluctuation in the density of states the principal result of this development is an expression for the density of states which is the trace of the semiclassical green s function and is given by the gutzwiller trace formula formula the index formula distinguishes the primitive periodic orbits the shortest period orbits of a given set of initial conditions formula is the period of the primitive periodic orbit and formula is its classical action each primitive orbit retraces itself leading to a new orbit with action formula and a period which is an integral multiple formula of the primitive period hence every repetition of a periodic orbit is another periodic orbit these repetitions are separately classified by the intermediate sum over the indices formula formula is the orbit s maslov index the amplitude factor formula represents the square root of the density of neighboring orbits neighboring trajectories of an unstable periodic orbit diverge exponentially in time from the periodic orbit the quantity formula characterizes the instability of the orbit a stable orbit moves on a torus in phase space and neighboring trajectories wind around it for stable orbits formula becomes formula where formula is the winding number of the periodic orbit formula where formula is the number of times that neighboring orbits intersect the periodic orbit in one period this presents a difficulty because formula at a classical bifurcation this causes that orbit s contribution to the energy density to diverge this also occurs in the context of photo absorption spectrum using the trace formula to compute a spectrum requires summing over all of the periodic orbits of a system this presents several difficulties for chaotic systems the number of periodic orbits proliferates exponentially as a function of action there are an infinite number of periodic orbits and the convergence properties of periodic orbit theory are unknown this difficulty is also present when applying periodic orbit theory to regular systems long period orbits are difficult to compute because most trajectories are unstable and sensitive to roundoff errors and details of the numerical integration gutzwiller applied the trace formula to approach the anisotropic kepler problem a single particle in a formula potential with an anisotropic mass tensor semiclassically he found agreement with quantum computations for low lying up to formula states for small anisotropies by using only a small set of easily computed periodic orbits but the agreement was poor for large anisotropies the figures above use an inverted approach to testing periodic orbit theory the trace formula asserts that each periodic orbit contributes a sinusoidal term to the spectrum rather than dealing with the computational difficulties surrounding long period orbits to try and find the density of states energy levels one can use standard quantum mechanical perturbation theory to compute eigenvalues energy levels and use the fourier transform to look for the periodic modulations of the spectrum which are the signature of periodic orbits interpreting the spectrum then amounts to finding the orbits which correspond to peaks in the fourier transform closed orbit theory closed orbit theory was developed by j b delos m l du j gao and j shaw it is similar to periodic orbit theory except that closed orbit theory is applicable only to atomic and molecular spectra and yields the oscillator strength density observable photo absorption spectrum from a specified initial state whereas periodic orbit theory yields the density of states only orbits that begin and end at the nucleus are important in closed orbit theory physically these are associated with the outgoing waves that are generated when a tightly bound electron is excited to a high lying state for rydberg atoms and molecules every orbit which is closed at the nucleus is also a periodic orbit whose period is equal to either the closure time or twice the closure time according to closed orbit theory the average oscillator strength density at constant formula is given by a smooth background plus an oscillatory sum of the form formula formula is a phase that depends on the maslov index and other details of the orbits formula is the recurrence amplitude of a closed orbit for a given initial state labeled formula it contains information about the stability of the orbit its initial and final directions and the matrix element of the dipole operator between the initial state and a zero energy coulomb wave for scaling systems such as rydberg atoms in strong fields the fourier transform of an oscillator strength spectrum computed at fixed formula as a function of formula is called a recurrence spectrum because it gives peaks which correspond to the scaled action of closed orbits and whose heights correspond to formula closed orbit theory has found broad agreement with a number of chaotic systems including diamagnetic hydrogen hydrogen in parallel electric and magnetic fields diamagnetic lithium lithium in an electric field the formula ion in crossed and parallel electric and magnetic fields barium in an electric field and helium in an electric field recent directions in quantum chaos the traditional topics in quantum chaos concerns spectral statistics universal and non universal features and the study of eigenfunctions quantum ergodicity scars of various chaotic hamiltonian formula further studies concern the parametric formula dependence of the hamiltonian as reflected in e g the statistics of avoided crossings and the associated mixing as reflected in the parametric local density of states ldos there is vast literature on wavepacket dynamics including the study of fluctuations recurrences quantum irreversibility issues etc special place is reserved to the study of the dynamics of quantized maps the standard map and the kicked rotator are considered to be prototype problems recent works are also focused in the study of driven chaotic systems where the hamiltonian formula is time dependent in particular in the adiabatic and in the linear response regimes conjecture in berry and tabor made a still open generic mathematical conjecture which stated roughly is in the generic case for the quantum dynamics of a geodesic flow on a compact riemann surface the quantum energy eigenvalues behave like a sequence of independent random variables provided that the underlying classical dynamics is completely integrable 
the phrase edge of chaos was coined by mathematician doyne farmer to describe the transition phenomenon discovered by computer scientist christopher langton the phrase originally refers to an area in the range of a variable lambda which was varied while examining the behavior of a cellular automaton ca as varied the behavior of the ca went through a phase transition of behaviors langton found a small area conducive to produce cas capable of universal computation at around the same time physicist james p crutchfield and others used the phrase onset of chaos to describe more or less the same concept in the sciences in general the phrase has come to refer to a metaphor that some physical biological economic and social systems operate in a region between order and either complete randomness or chaos where the complexity is maximal the generality and significance of the idea however has since been called into question by melanie mitchell and others the phrase has also been borrowed by the business community and is sometimes used inappropriately and in contexts that are far from the original scope of the meaning of the term stuart kauffman has studied mathematical models of evolving systems in which the rate of evolution is maximized near the edge of chaos 
in descriptive statistics and chaos theory a recurrence plot rp is a plot showing for a given moment in time the times at which a phase space trajectory visits roughly the same area in the phase space in other words it is a graph of showing formula on a horizontal axis and formula on a vertical axis where formula is a phase space trajectory background natural processes can have a distinct recurrent behaviour e g periodicities as seasonal or milankovich cycles but also irregular cyclicities as el southern oscillation moreover the recurrence of states in the meaning that states are again arbitrarily close after some time of divergence is a fundamental property of deterministic dynamical systems and is typical for nonlinear or chaotic systems cf recurrence theorem the recurrence of states in nature has been known for a long time and has also been discussed in early work e g henri detailed description eckmann et al introduced recurrence plots which can visualize the recurrence of states in a phase space usually a phase space does not have a low enough dimension two or three to be pictured higher dimensional phase spaces can only be visualized by projection into the two or three dimensional sub spaces however eckmann s tool enables us to investigate the m dimensional phase space trajectory through a two dimensional representation of its recurrences such recurrence of a state at time i and a different time j is pictured within a two dimensional squared matrix with black and white dots where black dots mark a recurrence and both axes are time axes this representation is called recurrence plot such a recurrence plot can be mathematically expressed as where u i is the time series m the embedding dimension and formula the time delay caused by characteristic behaviour of the phase space trajectory a recurrence plot contains typical small scale structures as single dots diagonal lines and vertical horizontal lines or a mixture of the latter which combines to extended clusters the large scale structure also called texture can be visually characterised by homogenous periodic drift or disrupted the visual appearance of an rp gives hints about the dynamics of the system the small scale structures in rps are used by the recurrence quantification analysis zbilut webber marwan et al this quantification allows to describe the rps in a quantitative way and to study transitions or nonlinear parameters of the system in contrast to the heuristic approach of the recurrence quantification analysis which depends on the choice of the embedding parameters some dynamical invariants as correlation dimension entropy or mutual information which are independent on the embedding can also be derived from recurrence plots the base for these dynamical invariants are the recurrence rate and the distribution of the lengths of the diagonal lines close returns plots are similar to recurrence plots the difference is that the relative time between recurrences is used for the formula axis instead of absolute time the main advantage of recurrence plots is that they provide useful information even for short and non stationary data where other methods fail extensions multivariate extensions of recurrence plots were developed as cross recurrence plots and joint recurrence plots the dimension of both systems must be the same but the number of considered states i e data length can be different cross recurrence plots compare the simultaneous occurrences of similar states of two systems they can be used in order to analyse the similarity of the dynamical evolution between two different systems to look for similar matching patterns in two systems or to study the time relationship of two similar systems whose time scale differ marwan kurths joint recurrence plots are the hadamard product of the recurrence plots of the considered sub systems romano et al e g for two systems formula and formula the joint recurrence plot is in contrast to cross recurrence plots joint recurrence plots compare the simultaneous occurrence of recurrences in two or more systems moreover the dimension of the considered phase spaces can be different but the number of the considered states has to be the same for all the sub systems joint recurrence plots can be used in order to detect phase synchronisation 
in physics self organized criticality soc is a property of classes of dynamical systems which have a critical point as an attractor their macroscopic behaviour thus displays the spatial and or temporal scale invariance characteristic of the critical point of a phase transition but without the need to tune control parameters to precise values the concept was put forward by per bak chao tang and kurt wiesenfeld btw in a paper published in in physical review letters and is considered to be one of the mechanisms by which complexity arises in nature its concepts have been enthusiastically applied across fields as diverse as geophysics physical cosmology evolutionary biology and ecology bio inspired computing and optimization mathematics economics quantum gravity sociology solar physics plasma physics neurobiology and others soc is typically observed in slowly driven non equilibrium systems with extended degrees of freedom and a high level of nonlinearity many individual examples have been identified since btw s original paper but to date there is no known set of general characteristics that guarantee a system will display soc toc overview self organized criticality is one of a number of important discoveries made in statistical physics and related fields over the latter half of the century discoveries which relate particularly to the study of complexity in nature for example the study of cellular automata from the early discoveries of stanislaw ulam and john von neumann through to john conway s game of life and the extensive work of stephen wolfram made it clear that complexity could be generated as an emergent feature of extended systems with simple local interactions over a similar period of time mandelbrot s large body of work on fractals showed that much complexity in nature could be described by certain ubiquitous mathematical laws while the extensive study of phase transitions carried out in the and showed how scale invariant phenomena such as fractals and power laws emerged at the critical point between phases bak tang and wiesenfeld s paper linked together these factors a simple cellular automaton was shown to produce several characteristic features observed in natural complexity fractal geometry f noise and power laws in a way that could be linked to critical point phenomena crucially however the paper demonstrated that the complexity observed emerged in a robust manner that did not depend on finely tuned details of the system variable parameters in the model could be changed widely without affecting the emergence of critical behaviour hence self organized criticality thus the key result of btw s paper was its discovery of a mechanism by which the emergence of complexity from simple local interactions could be spontaneous and therefore plausible as a source of natural complexity rather than something that was only possible in the lab or lab computer where it was possible to tune control parameters to precise values the publication of this research sparked considerable interest from both theoreticians and experimentalists and important papers on the subject are among the most cited papers in the scientific literature due to btw s metaphorical visualization of their model as a sandpile on which new sand grains were being slowly sprinkled to cause avalanches much of the initial experimental work tended to focus on examining real avalanches in granular matter the most famous and extensive such study probably being the oslo ricepile experiment other experiments include those carried out on magnetic domain patterns the barkhausen effect and vortices in superconductors early theoretical work included the development of a variety of alternative soc generating dynamics distinct from the btw model attempts to prove model properties analytically including calculating the critical exponents and examination of the necessary conditions for soc to emerge one of the important issues for the latter investigation was whether conservation of energy was required in the local dynamical exchanges of models the answer in general is no but with minor reservations as some exchange dynamics such as those of btw do require local conservation at least on average in the long term key theoretical issues yet to be resolved include the calculation of the possible universality classes of soc behaviour and the question of whether it is possible to derive a general rule for determining if an arbitrary algorithm displays soc alongside these largely lab based approaches many other investigations have centred around large scale natural or social systems that are known or suspected to display scale invariant behaviour although these approaches were not always welcomed at least initially by specialists in the subjects examined soc has nevertheless become established as a strong candidate for explaining a number of natural phenomena including earthquakes which long before soc was discovered were known as a source of scale invariant behaviour such as the gutenberg richter law describing the statistical distribution of earthquake sizes and the omori law describing the frequency of aftershocks solar flares fluctuations in economic systems such as financial markets references to soc are common in econophysics landscape formation forest fires landslides epidemics and biological evolution where soc has been invoked for example as the dynamical mechanism behind the theory of punctuated equilibria put forward by niles eldredge and stephen jay gould worryingly given the implications of a scale free distribution of event sizes some researchers have suggested that another phenomenon that should be considered an example of soc is the occurrence of wars these applied investigations of soc have included both attempts at modelling either developing new models or adapting existing ones to the specifics of a given natural system and extensive data analysis to determine the existence and or characteristics of natural scaling laws the recent excitement generated by scale free networks has raised some interesting new questions for soc related research a number of different soc models have been shown to generate such networks as an emergent phenomenon as opposed to the simpler models proposed by network researchers where the network tends to be assumed to exist independently of any physical space or dynamics realistic applications of soc rely on exponents derived from assumptions that are often oversimplified a promising exception to this limitation is the discovery by marcelo moret and gilney zebende that the globular compaction of protein chains by hydrophobic interactions is describable in non euclidean terms with fractal critical exponents inferred from the evolution of solvent accessible surface areas in chain segments of increasing length protein amino acid chains are conventionally represented by strings of letters and their similarities quantified by counting letter identities in protein words segments replacing the alpha letters with alphanumeric exponents enables non euclidean contextual analysis of universal water mediated protein interactions membrane viral fusion proteins are ideal targets for soc analysis as they exhibit up to hundreds of mutations in public data bases 
in mathematics the transfer operator encodes information about an iterated map and is frequently used to study the behavior of dynamical systems statistical mechanics quantum chaos and fractals the transfer operator is sometimes called the ruelle operator after david ruelle or the operator in reference to the applicability of the theorem to the determination of the eigenvalues of the operator the iterated function to be studied is a map formula for an arbitrary set formula the transfer operator is defined as an operator formula acting on the space of functions formula as where formula is an auxiliary valuation function when formula has a jacobian determinant then formula is usually taken to be formula the above definition of the transfer operator can be shown to be the point set limit of the measure theoretic pushforward of g in essence the transfer operator is the direct image functor in the category of measurable spaces the left adjoint of the operator is the koopman operator or composition operator applications whereas the iteration of a function formula naturally leads to a study of the orbits of points of x under iteration the study of point dynamics the transfer operator defines how smooth maps evolve under iteration thus transfer operators typically appear in physics problems such as quantum chaos and statistical mechanics where attention is focused on the time evolution of smooth functions in turn this has medical applications to rational drug design through the field of molecular dynamics it is often the case that the transfer operator is positive has discrete positive real valued eigenvalues with the largest eigenvalue being equal to one for this reason the transfer operator is sometimes called the operator the eigenfunctions of the transfer operator are usually fractals when the logarithm of the transfer operator corresponds to a quantum hamiltonian the eigenvalues will typically be very closely spaced and thus even a very narrow and carefully selected ensemble of quantum states will encompass a large number of very different fractal eigenstates with non zero support over the entire volume this can be used to explain many results from classical statistical mechanics including the irreversibility of time and the increase of entropy the transfer operator of the bernoulli map formula is exactly solvable and is a classic example of deterministic chaos the discrete eigenvalues correspond to the bernoulli polynomials this operator also has a continuous spectrum consisting of the hurwitz zeta function the transfer operator of the gauss map formula is called the gkw operator and due to its extraordinary difficulty has not been fully solved the theory of the gkw dates back to a hypothesis by gauss on continued fractions and is closely related to the riemann zeta function 
in physics an oscillon is a soliton like phenomenon that occurs in granular and other dissipative media oscillons in granular media result from vertically vibrating a plate with a layer of uniform particles placed freely on top when the sinusoidal vibrations are of the correct amplitude and frequency and the layer of sufficient thickness a localized wave referred to as an oscillon can be formed by locally disturbing the particles this meta stable state will remain for a long time many hundreds of thousands of oscillations in the absence of further perturbation an oscillon changes form with each collision of the grain layer and the plate switching between a peak that projects above the grain layer to a crater like depression with a small rim this self sustaining state was named by analogy with the soliton which is a localized wave that maintains its integrity as it moves whereas solitons occur as travelling waves in a fluid or as electromagnetic waves in a waveguide oscillons may be stationary astonishingly oscillons of opposite phase will attract over short distances and form bonded pairs oscillons of like phase repel oscillons have been observed forming molecule like structures and long chains in comparison solitons do not form bound states stable interacting localized waves with subharmonic response were discovered and named oscillons at the university of texas at austin solitary bursts had been reported earlier in a quasi two dimensional grain layer at the university of paris but these transient events were unstable and no bonding interaction or subharmonic response was reported the cause of this phenomenon is currently under debate the most likely connection is with the mathematical theory of chaos and may give insights into the way patterns in sand form the experimental procedure is similar to that used to form chladni figures of sand on a vibrating plate researchers realized that these figures say more about the vibrational modes of the plate than the response of the sand and created an experimental set up that minimized outside effects using a shallow layer of brass balls in a vacuum and a rigid plate when they vibrated the plate at critical amplitude they found that the balls formed a localized vibrating structure when perturbed which lasted indefinitely oscillons have also been experimentally observed in thin parametrically vibrated layers of viscous fluid and colloidal suspensions oscillons have been associated with faraday waves because they require similar resonance conditions nonlinear electrostatic oscillations on a plasma boundary can also appear in the form of oscillons this was discovered in 
in chaos theory the correlation dimension denoted by is a measure of the dimensionality of the space occupied by a set of random points often referred to as a type of fractal dimension for example if we have a set of random points on the real number line between and the correlation dimension will be while if they are distributed on say a triangle embedded in three dimensional space or m dimensional space the correlation dimension will be this is what we would intuitively expect from a measure of dimension the real utility of the correlation dimension is in determining the possibly fractional dimensions of fractal objects there are other methods of measuring dimension e g the hausdorff dimension the box counting dimension and the information dimension but the correlation dimension has the advantage of being straightforwardly and quickly calculated of being less noisy when only a small number of points is available and is often in agreement with other calculations of dimension for any set of n points in an m dimensional space if the number of points is sufficiently large and evenly distributed a log log graph of the correlation integral versus will yield an estimate this idea can be qualitatively understood by realizing that for higher dimensional objects there will be more ways for points to be close to each other and so the number of pairs close to each other will rise more rapidly for higher dimensions grassberger and procaccia introduced the technique in the article gives the results of such estimates for a number of fractal objects as well as comparing the values to other measures of fractal dimension the technique can be used to distinguish between deterministic chaotic and truly random behavior although it may not be good at detecting deterministic behavior if the deterministic generating mechanism is very complex as an example in the sun in time article the method was used to show that the number of sunspots on the sun after accounting for the known cycles such as the daily and year cycles is very likely not random noise but rather chaotic noise with a low dimensional fractal attractor 
recurrence quantification analysis rqa is a method of nonlinear data analysis cf chaos theory for the investigation of dynamical systems it quantifies the number and duration of recurrences of a dynamical system presented by its phase space trajectory background the recurrence quantification analysis was developed in order to quantify differently appearing recurrence plots rps based on the small scale structures therein recurrence plots are tools which visualise the recurrence behaviour of the phase space trajectory of dynamical systems they mostly contain single dots and lines which are parallel to the mean diagonal line of identity loi or which are vertical horizontal lines parallel to the loi are referred to as diagonal lines and the vertical structures as vertical lines because an rp is usually symmetric horizontal and vertical lines correspond to each other and hence only vertical lines are considered the lines correspond to a typical behaviour of the phase space trajectory whereas the diagonal lines represent such segments of the phase space trajectory which run parallel for some time the vertical lines represent segments which remain in the same phase space region for some time where formula is the time series formula the embedding dimension and formula the time delay the rqa quantifies the small scale structures of recurrence plots which present the number and duration of the recurrences of a dynamical system the measures introduced for the rqa were developed heuristically between and zbilut webber webber zbilut marwan et al they are actually measures of complexity the main advantage of the recurrence quantification analysis is that it can provide useful information even for short and non stationary data where other methods fail rqa can be applied to almost every kind of data it is widely used in physiology but was also successfully applied on problems from engineering chemistry earth sciences etc rqa measures the recurrence rate corresponds with the probability that a specific state will recur it is almost equal with the definition of the correlation sum where the loi is excluded from the computation where formula is the frequency distribution of the lengths formula of the diagonal lines this measures is called determinism and is related with the predictability of the dynamical system because white noise has a recurrence plot with almost only single dots and very few diagonal lines whereas a deterministic process has a recurrence plot with very few single dots but many long diagonal lines where formula is the frequency distribution of the lengths formula of the vertical lines which have at least a length of formula this measure is called laminarity and related with the amount of laminar phases in the system intermittency the lengths of the diagonal and vertical lines can be measured as well the averaged diagonal line length is related with the predictability time of the dynamical system and the trapping time measuring the average length of the vertical lines is related with the laminarity time of the dynamical system i e how long the system remains in a specific state because the length of the diagonal lines is related on the time how long segments of the phase space trajectory run parallel i e on the divergence behaviour of the trajectories it was sometimes stated that the reciprocal of the maximal length of the diagonal lines without loi would be an estimator for the positive maximal lyapunov exponent of the dynamical system therefore the maximal diagonal line length formula or the divergence are also measures of the rqa however the relationship between these measures with the positive maximal lyapunov exponent is not as easy as stated but even more complex to calculate the lyapunov exponent from an rp the whole frequency distribution of the diagonal lines has to be considered the divergence can have the trend of the positive maximal lyapunov exponent but not more moreover also rps of white noise processes can have a really long diagonal line although very seldom just by a finite probability it is obvious that therefore the divergence cannot reflect the maximal lyapunov exponent the probability formula that a diagonal line has exactly length formula can be estimated from the frequency distribution formula with formula the shannon entropy of this probability reflects the complexity of the deterministic structure in the system however this entropy depends sensitively on the bin number and thus may differ for different realisations of the same process as well as for different data preparations then the trend is defined by with formula as the average value and formula this latter relation should ensure to avoid the edge effects of too low recurrence point densities in the edges of the recurrence plot the measure trend provides information about the stationarity of the system similar to the diagonal wise defined recurrence rate the other measures based on the diagonal lines det l entr can be defined diagonal wise these definitions are useful to study interrelations or synchronisation between different systems using recurrence plots or cross recurrence plots time dependent rqa instead of computing the rqa measures of the entire recurrence plot they can be computed in small windows moving over the recurrence plot along the loi this provides time dependent rqa measures which are able to detect e g chaos chaos transitions marwan et al note the choice of the size of the window can strongly influence the measure trend 
in chaos theory a complexor is mathematically equivalent to a chaotic attractor the word was coined by marcial losada losada heaphy derived from the words complex order a complexor is generated by a set of deterministic nonlinear equations that has at least one positive lyapunov exponent a complexor is not a disordered structure on the contrary its configuration reveals a complex order that is manifested by its fractal nature the dynamics of a complexor in phase space reveal trajectories that never repeat themselves losada found that these trajectories reflect the creativity and innovation that characterize high performance teams losada heaphy low performance teams have trajectories in phase space that approach the limiting dynamics of point attractors in laymans terms a complexor is an unusual equation in which the graph forms an ever changing but highly ordered structure which repeats in a slightly different form over the range of the equation it was discovered during analysis of highly creative groups which much like the equation itself have a particular style of operation but continually renew and reinvent themselves losada 
in public transport bus bunching clumping or platooning refers to a group of two or more transit vehicles along the same route such as buses or trains which are scheduled to be evenly spaced running in the same location at the same time this occurs when at least one of the vehicles is unable to keep to its schedule and therefore ends up in the same location as one or more other vehicles of the same route at the same time the end result can be longer wait times for some passengers on routes that have shorter scheduled intervals classical theory a classical theory for a causal model for irregular intervals builds on the premise that a late bus tends to get later and later as it completes its run while the bus following it tends to get earlier and earlier eventually these buses form a pair one right after another and the service breaks down as the headway degrades from its nominal value the buses that are stuck together are called a bus bunch or banana bus and may involve more than two buses it is often theorized to be the primary cause of reliability problems on bus and metro systems hypothesized causes clumping can be caused by heavy usage on any particular vehicle resulting in it falling behind schedule the vehicle then ends up in the location of a later scheduled vehicle when that one should be there in some cases the later scheduled vehicle gets ahead of its schedule and they meet in a location in between the scheduled times of the two vehicles in some cases a later scheduled vehicle may actually get ahead of an earlier scheduled one when buses clump one can pass another on the street or road when trains clump this is often not possible due to the limitations of the single track abnormal passenger loads the time taken for a bus to complete its duties is related to the number of people attempting to board or alight at stops the bus that is already late tends to attract a higher number of riders due to the longer headway between it and the previous bus the higher number of riders boarding the bus results in delaying it further at the same time the following bus tends to collect fewer passengers because its headway is shorter due to the delay of its predecessor and hence it spends less than expected time on stops which further shortens its headway unless it deliberately idles on stops or slows down speed of individual drivers another cause is that some drivers are faster than others this results in catching up on long or high frequency routes deliberate acts according to the article progress has passed metrobus by lyndsey layton washington post december bus bunching may be deliberately caused by bus drivers so that the bus ahead of them picks up more passengers and decreases their own workload chaos theory the classical bus bunching theory is an example of chaos theory the orderly procession of buses is presumed inherently unstable and buses are presumed to tend towards bunches if left unchecked according to proponents of this theory it is impossible to predict from the outset which buses will be bunched and which buses will proceed on schedule to the destination because bunching is presumed to be caused by random conditions such as traffic stoplights and the number of passengers at a stop the study of the actual practice of bus operations however has been shown to be quite different practice the existence of classical pairwise bunching has not been borne out by vehicle tracking systems data studies into metro operations have broadly debunked the theory of pairwise bunching as a major cause of irregular intervals on metro lines and have tied irregularity largely to problems in other key scheduling and operational processes recent research has demonstrated that simulation models of bus routes based on the classical theory of bus bunching have failed to replicate actual conditions of bus service intervals as captured in bus location tracking databases even when random external events are incorporated into the model however simulation studies have successfully demonstrated the extent of possible factors influencing bus bunching and they may equally well be used to understand the impact of actions taken to overcome negative effects of bunching prevention adding more vehicles to the schedule has been proven not to be a solution to the problem 
in mathematics the term chaos game as coined by michael barnsley originally referred to a method of creating a fractal using a polygon and an initial point selected at random inside it the fractal is created by iteratively creating a sequence of points starting with the initial random point in which each point in the sequence is a given fraction of the distance between the previous point and one of the vertices of the polygon the vertex is chosen at random in each iteration repeating this iterative process a large number of times selecting the vertex at random on each iteration and throwing out the first few points in the sequence will often but not always produce a fractal shape using a regular triangle and the factor will result in the sierpinski triangle while creating the proper arrangement with four points and a factor will create a display of a sierpinski tetrahedron the three dimensional analogue of the sierpinski triangle as the number of points is increased to a number n the arrangement forms a corresponding n dimensional sierpinski simplex the term has been generalized to refer to a method of generating the attractor or the fixed point of any iterated function system ifs starting with any point successive iterations are formed as xk fr xk where fr is a member of the given ifs randomly selected for each iteration the iterations converge to the fixed point of the ifs whenever belongs to the attractor of the ifs all iterations xk stay inside the attractor and with probability form a dense set in the latter the chaos game method plots points in random order all over the attractor this is in contrast to other methods of drawing fractals which test each pixel on the screen to see whether it belongs to the fractal the general shape of a fractal can be plotted quickly with the chaos game method but it may be difficult to plot some areas of the fractal in detail the chaos game method is mentioned in tom stoppard s play arcadia by the aid of chaos game a new fractal can be made and while making the new fractal some parameters can be obtained this parameters are useful for applications of fractal theory such as classification and identification new fractal is self similar to original in some important features such as fractal dimension 
functional equation the functional equation arises in the study of one dimensional maps that as a function of a parameter go through a period doubling cascade the functional equation is the mathematical expression of the universality of period doubling the equation is used to specify a function g and a parameter by the relation with the boundary conditions for a particular form of solution with a quadratic dependence of the solution near x the inverse is one of the feigenbaum constants scaling function the feigenbaum scaling function provides a complete description of the attractor of the logistic map at the end of the period doubling cascade the attractor is a cantor set set and just as the middle third cantor set it can be covered by a finite set of segments all bigger than a minimal size dn for a fixed dn the set of segments forms a cover of the attractor the ratio of segments from two consecutive covers and can be arranged to approximate a function the feigenbaum scaling function 
where formula is the time series formula the embedding dimension and formula the time delay the correlation sum is used to estimate the correlation dimension 
where formula is the time series formula the embedding dimension and formula the time delay the correlation integral is used to estimate the correlation dimension 
a stable attractor in chaos theory or biology is an equilibrium state into which a system settles until disrupted by a change in the environment the system then settles to a new attractor in cellular automata a transition from a chaotic phase to a stable attractor is called a solution some mathematical functions may never converge to a solution but may cycle endlessly in a stable way leibnitz et al have devised a network routing scheme on a stable attractor model developed to account for the response of escherichia coli bacteria to variations in nutrient availability information about data paths bandwidth transit time is used to determine a stable attractor and to find a new attractor if the network falters the system is stable in noisy environments 
in chaos theory control of chaos is based on the fact that any chaotic attractor contains an infinite number of unstable periodic orbits chaotic dynamics then consists of a motion where the system state moves in the neighborhood of one of these orbits for a while then falls close to a different unstable periodic orbit where it remains for a limited time and so forth this results in a complicated and unpredictable wandering over longer periods of time control of chaos is the stabilization by means of small system perturbations of one of these unstable periodic orbits the result is to render an otherwise chaotic motion more stable and predictable which is often an advantage the perturbation must be tiny to avoid significant modification of the system s natural dynamics several techniques have been devised for chaos control but most are developments of two basic approaches the ogy ott grebogi and yorke method and pyragas continuous control both methods require a previous determination of the unstable periodic orbits of the chaotic system before the controlling algorithm can be designed in the ogy method small wisely chosen swift kicks are applied to the system once per cycle to maintain it near the desired unstable periodic orbit in the pyragas method an appropriate continuous controlling signal is injected into the system whose intensity is practically zero as the system evolves close to the desired periodic orbit but increases when it drifts away from the desired orbit experimental control of chaos by one or both of these methods has been achieved in a variety of systems including turbulent fluids oscillating chemical reactions magneto mechanical oscillators and cardiac tissues sarnobat et al attempt the control of chaotic bubbling with the ogy method and using electrostatic potential as the primary control variable the number of publications devoted to control of chaos is huge see e g chaos control bibliography forcing two systems into the same state is not the only way to achieve synchronization of chaos both control of chaos and synchronization constitute parts of cybernetical physics cybernetical physics is a research area on the border between physics and control theory books eckehard and heinz georg schuster eds handbook of chaos control wiley vch revision enlarged edition weinheim miranda j m synchronization and control of chaos an introduction for scientists and engineers london imperial college press fradkov a l pogromsky a yu introduction to control of oscillations and chaos singapore world scientific publishers 
chaos communications is an application of chaos theory which is aimed to provide security in the transmission of information performed through telecommunications technologies by secure communications one has to understand that the contents of the message transmitted are inaccessible to possible eavesdroppers in chaos communications security i e privacy is based on the complex dynamic behaviors provided by chaotic systems some properties of chaotic dynamics such as complex behaviour noise like dynamics pseudorandom noise and spread spectrum are used to encode data on the other hand being chaos a deterministic phenomenon it is possible to decode data using this determinism in practice implementations of chaos communications devices resort to one of two chaotic phenomena synchronization of chaos or control of chaos to implement chaos communications using such properties of chaos two chaotic oscillators are required as a transmitter or master and receiver or slave at the transmitter a message is added on to a chaotic signal and then the message is masked in the chaotic signal as it carries the information the chaotic signal is also called chaotic carrier synchronizing of these oscillators is similar to synchronizing random neural nets in neural cryptography when chaos synchronization is used a basic scheme of a communications device cuomo and oppenheim is made by two identical chaotic oscillators one of them is used as the transmitter and the other as the receiver they are connected in a configuration where the transmitter drives the receiver in such a way that identical synchronization of chaos between the two oscillators is achieved for the purpose of transmission of information at the transmitter a message is added as a small perturbation to the chaotic signal that drives the receiver in this way the message transmitted is masked by the chaotic signal when the receiver synchronizes to the transmitter the message is decoded by a subtraction between the signal sent by transmitter and its copy generated at the receiver by means of the synchronization of chaos mechanism this works because whilst the transmitter output contains the chaotic carrier plus the message the receiver output is made only by a copy of the chaotic carrier without the message optical chaos communications chaos communications has been a success in optical communications 
synchronization of chaos is a phenomenon that may occur when two or more chaotic oscillators are coupled or when a chaotic oscillator drives another chaotic oscillator because of the butterfly effect which causes the exponential divergence of the trajectories of two identical chaotic system started with nearly the same initial conditions having two chaotic systems evolving in synchrony might appear quite surprising however synchronization of coupled or driven chaotic oscillators is a phenomenon well established experimentally and reasonably well understood theoretically all these forms of synchronization share the property of asymptotic stability this means that once the synchronized state has been reached the effect of a small perturbation that destroys synchronization is rapidly damped and synchronization is recovered again mathematically asymptotic stability is characterized by a positive lyapunov exponent of the system composed of the two oscillators which becomes negative when chaotic synchronization is achieved some chaotic systems allow even stronger control of chaos both synchronization of chaos and control of chaos constitute parts of cybernetical physics 
interconnectivity is a concept that is used in numerous fields such as cybernetics biology ecology network theory and non linear dynamics the concept can be summarized as that all parts of a system interact with and rely on one another simply by the fact that they occupy the same system and that a system is difficult or sometimes impossible to analyze through its individual parts considered alone the concept is closely linked to the observer effect and the butterfly effect it is often linked to the concepts of interconnectedness which is used to refer to the spiritual and interdependence which refers to the moral rather than physical or scientific differentiation from the butterfly effect the key difference between interconnectivity and the butterfly effect is that while the butterfly effect deals with chain reactions and events interconnectivity deals with systems in dynamic equilibrium such as ecosystems economies societies etc while the two are often substituted incorrectly for one another they are two similar but separate concepts 
the chirikov criterion or chirikov resonance overlap criterion was established by the russian physicist boris chirikov back in he published a seminal article atom energ where he introduced the very first physical criterion for the onset of chaotic motion in deterministic hamiltonian systems he then applied such a criterion to explain puzzling experimental results on plasma confinement in magnetic bottles obtained by rodionov at the kurchatov institute as in an old oriental tale boris chirikov opened such a bottle and freed the genie of chaos which spread the world over description according to this criterion a deterministic trajectory will begin to move between two nonlinear resonances in a chaotic and unpredictable manner in the parameter range here k math is the perturbation parameter while s delta omega r delta d math is the resonance overlap parameter given by the ratio of the unperturbed resonance width in frequency delta omega r math often computed in the pendulum approximation and proportional to the square root of perturbation and the frequency difference delta d math between two unperturbed resonances since its introduction the chirikov criterion has become an important analytical tool for the determination of the chaos border 
a nonlinear dynamical system exhibits chaotic hysteresis if it simultaneously exhibits chaotic dynamics chaos theory and hysteresis as the latter involves the persistence of a state such as magnetization after the causal or exogenous force or factor is removed it involves multiple equilibria for given sets of control conditions such systems generally exhibit sudden jumps from one equilibrium state to another sometimes amenable to analysis using catastrophe theory if chaotic dynamics appear either prior to or just after such jumps or are persistent throughout each of the various equilibrium states then the system is said to exhibit chaotic hysteresis chaotic dynamics are irregular and bounded and subject to sensitive dependence on initial conditions background and applications the term was introduced initially by ralph abraham and christopher shaw but was modeled conceptually earlier and has been applied to a wide variety of systems in many disciplines the first model of such a phenomenon was due to otto in which he viewed as applying to major brain dynamics and arising from three dimensional chaotic systems in it was applied to electric oscillators by newcomb and el leithy perhaps the most widely used application since see also pecora and carroll the first to use the term for a specific application was j barkley rosser jr in who suggested that it could be applied to explaining the process of systemic economic transition with poirot following up on this in regard to the russian financial crisis of empirical analysis of the phenomenon in the russian economic transition was done by rosser rosser guastello and bond while he did not use the term puu presented a multiplier accelerator business cycle model with a cubic accelerator function that exhibited the phenomenon other conscious applications of the concept have included to rayleigh convection rolls hysteretic scaling for ferromagnetism and a pendulum on a rotating table berglund and kunz to induction motors and nagy to combinatorial optimization in integer programming wataru and eitaro to isotropic magnetization hauser to bursting oscillations in beta cells in the pancreas and population dynamics and piquet to thermal convection vadasz and to neural networks liu and xiu 
many dynamical processes that generate bubbles are nonlinear many exhibiting patterns consistent with mathematically chaotic dynamics chaos theory in such cases chaotic bubbles can be said to occur in most systems they arise out of a forcing pressure that encounters some kind of resistance or shear factor but the details vary depending on the particular context the most widely used application has been in studying bubbles in various forms of liquid media although there may have been an earlier use of the term it was used in specifically in connection with a model of the motion of a single bubble in a fluid subject to periodically driven pressure oscillations smereka birnir and banerjee for an overview of models of single bubble dynamics see feng and leal there has been a long literature on nonlinear analysis of dynamics of bubbles in liquids with an important figure in this being werner lauterborn lauterborn and cramer would also play an important role in applying chaos theory to acoustics in which bubble dynamics play a crucial part this includes analysis of chaotic dynamics in an acoustic cavitation bubble field in a liquid lauterborn holzfuss and bilio the study of the role of shear stresses in non newtonian fluids has been done by li mouline choplin and midoux a somewhat related interest has been the study of controlling such chaotic bubble dynamics control of chaos by converting them to periodic oscillations with an important application to in fluidized bed reactors also applicable to the ammoxidation of propylene to acrylonitril kaart schouten and van den bleek sarnobat et al study the behavior of electrostatic fields on chaotic bubbling in attempt to control the chaos into a lower order periodicity an early attempted application that led to failure was in alan h chaotic inflation theory of the early period of the universe while he did not precisely use the term bubbles his model involved in the original cosmic foam that collided chaotically the model has since been modified due to the inability to find in the real universe some of the phenomena predicted by it with improvements involving quantum fluctuations provided by andrei d linde in economics the bubbles in question have been those due to speculation in asset markets economic bubble the first to apply the term in this context was j barkley rosser jr in while not using the term richard h day and weihong huang showed that the interaction of fundamentalist and trend chasing traders could lead to chaotic dynamics in the price path of a speculative bubble de grauwe dewachter and embrechts applied such a model to foreign exchange rate dynamics 
for want of a nail is a proverbial rhyme showing that small actions can result in large consequences analysis this proverb has been around in many variations for centuries see historical references below and describes a situation where permitting some small undesirable situation will allow gradual and inexorable worsening the rhyme is thus a good illustration of the butterfly effect and ideas presented in chaos theory involving sensitive dependence on initial conditions the initial condition being the presence or absence of the horseshoe nail at a more literal level it summarizes the importance of military logistics throughout the history of human warfare an important thing to note is that these chains of causality are only seen in hindsight nobody ever lamented upon seeing his unshod horse that the kingdom would eventually fall because of it a somewhat similar idea is referred to in the metaphor known as the camel s nose 
the stability of the solar system is a subject of much inquiry in astronomy though the planets have been stable historically and will be in the short term their weak gravitational effects on one another can add up in unpredictable ways for this reason among others the solar system is chaotic and even the most precise long term models for the orbital motion of the solar system are not valid over more than a few tens of millions of years the solar system is stable in human terms in that none of the planets will collide with each other or be ejected from the system in the next few billion years and the earth s orbit will be relatively stable since newton s law of gravitation mathematicians and astronomers such as laplace lagrange gauss kolmogorov vladimir arnold and moser have searched for evidence for the stability of the planetary motions and this quest led to many mathematical developments and several successive proofs of stability for the solar system overview and challenges the orbits of the planets are open to long term variations and modeling the solar system is subject to the n body problem resonance resonance happens when any two periods have a simple numerical ratio the most fundamental period for an object in the solar system is its orbital period and orbital resonances pervade the solar system in the american astronomer daniel kirkwood noticed that asteroids in the asteroid belt are not randomly distributed there were distinct gaps in the belt and at locations that corresponded to resonances with jupiter for example there were no asteroids at the resonance a distance of astronomical units or at the resonance at astronomical units another common form of resonance in the solar system is spin orbit resonance where the period of spin the time it takes the orbit to rotate once about its axis has a simple numerical relationship with its orbital period an example is our own moon which is in a spin orbit resonance that keeps the far side of the moon away from the earth predictability the planets orbits are chaotic over longer timescales such that the whole solar system possesses a lyapunov time in the range of years in all cases this means that the position of a planet along its orbit ultimately becomes impossible to predict with any certainty so for example the timing of winter and summer become uncertain but in some cases the orbits themselves may change dramatically such chaos manifests most strongly as changes in eccentricity with some planets orbits becoming significantly in calculation the unknowns include asteroids the solar quadrupole moment mass loss from the sun through radiation and solar wind and drag of solar wind on planetary magnetospheres galactic tidal forces the fractional effect and effects from passing stars furthermore the equations of motion describe a process that is inherently serial so there is little gain from massively parallel computers scenarios resonance the system lies in a orbital resonance c j cohen and e c hubbard at the naval surface warfare center dahlgren division discovered this in although the resonance itself will remain stable in the short term it becomes impossible to predict the position of pluto with any degree of accuracy more than years the lyapunov time into the future jovian moon resonance jupiter s moon io has an orbital period of days nearly half that of the next satellite europa days they are said to be in a orbit orbit resonance this particular resonance has important consequences because europa s gravity perturbs the orbit of io as io moves closer to jupiter and then further away in the course of a orbit it experiences significant tidal stresses resulting in active volcanoes which voyager observed europa is also in a resonance with the next satellite ganymede resonance the planet mercury is especially susceptible to jupiter s influence because of a small celestial coincidence mercury s perihelion the point where it gets closest to the sun moves around at a rate of about degrees every years and jupiter s perihelion moves around only a little slower one day the two may fall into sync at which time jupiter s constant gravitational tugs could accumulate and pull mercury off course this could eject it from the solar system altogether or send it on a collision course with venus or earth chaos from geological processes another example is earth s axial tilt which thanks to friction raised within earth s mantle by tidal interactions with the moon see below will be rendered chaotic at some point between and billion years from now studies longstop project longstop long term gravitational study of the outer planets was a international consortium of solar system dynamicists led by archie roy it involved creation of a model on a supercomputer integrating the orbits of only the outer planets its results revealed several curious exchanges of energy between the outer planets but no signs of gross instability digital orrery another project involved constructing the digital orrery by gerry sussman and his mit group in the group used a supercomputer to integrate the orbits of the outer planets over million years some per cent of the age of the solar system in sussman and wisdom found data using the orrery which revealed that pluto s orbit shows signs of chaos due in part to its peculiar resonance with neptune if pluto s orbit is chaotic then technically the whole solar system is chaotic because each planet even one as small as pluto affects the others to some extent through gravitational interactions laskar in jacques laskar of the bureau des longitudes in paris published the results of his numerical integration of the solar system over million years these were not the full equations of motion but rather averaged equations along the lines of those used by laplace laskar s work showed that the earth s orbit as well as the orbits of all the inner planets is chaotic and that an error as small as metres in measuring the position of the earth today would make it impossible to predict where the earth would be in its orbit in just over million years time laskar gastineau jacques laskar and his colleague gastineau in took a more thorough approach by directly simulating possible futures each of the cases has slightly different initial conditions mercury s position varies by about metre between one simulation and the next in cases mercury goes into a dangerous orbit and often ends up colliding with venus or plunging into the sun moving in such a warped orbit mercury s gravity is more likely to shake other planets out of their settled paths in one simulated case its perturbations send mars heading towards earth 
fractal analysis is assessing fractal characterisics of data it consists of several methods to assign a fractal dimension and other fractal characteristics to a dataset which may be a theoretical dataset or a pattern or signal extracted from phenomena including natural geometric objects sound market fluctuations heart rates digital images molecular motion networks etc fractal analysis is now widely used in all areas of science an important limitation of fractal analysis is that arriving at an empirically determined fractal dimension does not necessarily prove that a pattern is fractal rather other essential characteristics have to be considered types of fractal analysis several types of fractal analysis are done including box counting lacunarity analysis mass methods and multifractal analysis a common feature of all types of fractal analysis is the need for benchmark patterns against which to assess outputs these can be acquired with various types of fractal generating software capable of generating benchmark patterns suitable for this purpose which generally differ from software designed to render fractal art 
a plot named after henri is used to quantify self similarity in processes usually periodic functions it is also known as a return map given a time series of the form a return map in its simplest form first plots x x then plots x x then x x and so on rr tachograph the rr tachograph is a picture of the rr interval which is the interval between r waves of the tachogram usually felt as heartbeats 
the process of a laminar flow becoming turbulent is known as laminar turbulent transition this process is an extraordinarily complicated process which at present is not fully understood however as the result of many decades of intensive research certain features have become gradually clear and it is known that the process proceeds through a series of stages while the process applies to any fluid flow it is most often used in the context boundary layers due to their ubiquity in real fluid flows and their importance in many fluid dynamic processes history osborne reynolds demonstrated the transition to turbulent flow in a classical experiment in which he examined an outlet from a large water tank through a small tube at the end of the tank there was a stopcock used to vary the water speed inside the tube the junction of the tube with the tank was nicely rounded a filament of coloured fluid was introduced at the mouth when the water was slow the filament remained distinct through the entire length of the tube when the speed was increased the filament broke up at a given point and diffused throughout the cross section reynolds identified the governing parameter the dimensionless reynolds number the point at which the colour diffuses throughout the tube is the transition point from laminar to turbulent reynolds found that the transition occurred between and depending on the smoothness of the entry conditions when extreme care is taken the transition can even happen with re as high as on the other hand appears to be about the lowest value obtained at a rough entrance transition stages in a boundary layer a boundary layer can transition to turbulence through a number of paths which path is realized physically depends on the initial conditions such as initial disturbance amplitude and surface roughness the level of understanding of each phase varies greatly from near complete understanding of primary mode growth to a near complete lack of understanding of bypass mechanisms receptivity the initial stage of the natural transition process is known as the receptivity phase and consists of the transformation of environmental disturbances both acoustic sound and vortical turbulence into small perturbations within the boundary layer the mechanisms by which these disturbances are varied and include freestream sound and or turbulence interacting with surface curvature shape discontinuities and surface roughness these initial conditions are small often unmeasurable perturbations to the basic state flow from here the growth or decay of these disturbances depends on the nature of the disturbance and the nature of the basic state acoustic disturbances tend to influence the growth of two dimensional instabilities such as waves t s waves while vortical disturbances tend to lead to the growth of three dimensional phenomena such as the crossflow instability numerous experiments in recent decades have revealed that the extent of the amplification region and hence the location of the transition point on the body surface is strongly dependent not only upon the amplitude and or the spectrum of external disturbances but also on their physical nature some of the disturbances easily penetrate into the boundary layer whilst others do not consequently the concept of boundary layer transition is a complex one and still lacks a complete theoretical exposition primary mode growth if the initial environmentally generated disturbance is small enough the next stage of the transition process is that of primary mode growth in this stage the initial disturbances grow or decay in a manner described by linear stability theory the specific instabilities that are exhibited in reality depend on the geometry of the problem and the nature and amplitude of initial disturbances across a range of reynolds numbers in a given flow configuration the most amplified modes can and often do vary there are several major types of instability which commonly occur in boundary layers in subsonic and early supersonic flows the dominant two dimensional instabilities are t s waves for flows in which a three dimensional boundary layer develops such as a swept wing the crossflow instability becomes important for flows navigating concave surface curvature vortices may become the dominant instability each instability has its own physical origins and its own set of control strategies some of which are contraindicated by other instabilities adding to the difficulty in controlling laminar turbulent transition secondary instabilities the primary modes themselves don t actually lead directly to breakdown but instead lead to the formation of secondary instability mechanisms as the primary modes grow and distort the mean flow they begin to exhibit nonlinearities and linear theory no longer applies complicating the matter is the growing distortion of the mean flow which can lead to inflection points in the velocity profile a situation shown by lord rayleigh to indicate absolute instability in a boundary layer these secondary instabilities lead rapidly to breakdown these secondary instabilities are often much higher in frequency than their linear precursors 
in quantum chaos a branch of mathematical physics quantum ergodicity is a property of the quantization of classical mechanical systems that are chaotic in the sense of exponential sensitivity to initial conditions quantum ergodicity states roughly that in the high energy limit the probability distributions associated to energy eigenstates of a quantized ergodic hamiltonian tend to a uniform distribution in the classical phase space this is consistent with the intuition that the flows of ergodic systems are equidistributed in phase space by contrast classical completely integrable systems generally have periodic orbits in phase space and this is exhibited in a variety of ways in the high energy limit of the eigenstates typically that some form of concentration or scarring occurs in the limit the model case of a hamiltonian is the geodesic hamiltonian on the cotangent bundle of a compact riemannian manifold the quantization of the geodesic flow is given by the fundamental solution of the equation where formula is the square root of the laplace beltrami operator the quantum ergodicity theorem of shnirelman yves colin de and zelditch states that a compact riemannian manifold whose unit tangent bundle is ergodic under the geodesic flow is also ergodic in the sense that the probability density associated to the n th eigenfunction of the laplacian tends weakly to the uniform distribution on the unit cotangent bundle as n 
nonlinear dynamics is a peer reviewed scientific journal it has the subtitle of an international journal of nonlinear dynamics and chaos in engineering systems and is published by springer nonlinear dynamics provides a forum for the rapid publication of original research in the field the scope encompasses all nonlinear dynamic phenomena associated with mechanical structural civil aeronautical ocean electrical and control systems review articles and original contributions are based on analytical computational and experimental methods subjects covered in journal the journal examines such topics as perturbation and computational methods symbolic manipulation dynamic stability local and global methods bifurcations chaos and deterministic and random vibrations the journal also investigates lie groups multibody dynamics robotics fluid solid interactions system modeling and identification friction and damping models signal analysis and measurement techniques the journal is abstracted indexed in chemical abstracts service cas current contents life sciences pubmed medline science citation index academic onefile compendex csa proquest current abstracts current contents engineering computing and technology digital mathematics registry ei page one gale google scholar journal citation reports science edition mathematical reviews oclc science citation index science citation index expanded scisearch scopus summon by serial solutions the shock and vibration digest toc premier vinity russian academy of science zentralblatt math editor in chief the editor in chief of the journal is ali h nayfeh of virginia polytechnic institute in blacksburg va 
lagrangian coherent structures are structures which separate dynamically distinct regions in time varying systems such as turbulent flows in fluid mechanics they can be defined in terms of finite time lyapunov exponents based on a frame independent description of the system in terms of lagrangian mechanics 
in chaos theory and fluid dynamics chaotic mixing is a process by which flow tracers develop into complex fractals under the action of a fluid flow the flow is characterized by an exponential growth of fluid filaments even very simple flows such as the blinking vortex or finitely resolved wind fields can generate exceptionally complex patterns from initially simple tracer fields the phenomenon is still not well understood and is the subject of much current research context of chaotic advection fluid flows two basic mechanisms are responsible for fluid mixing diffusion and advection in liquids molecular diffusion alone is hardly efficient for mixing advection that is the transport of matter by a flow is required for better mixing the fluid flow obeys fundamental equations of fluid dynamics such as the conservation of mass and the conservation of momentum called equations these equations are written for the eulerian velocity field rather than for the lagrangian position of fluid particles lagrangian trajectories are then obtained by integrating the flow studying the effect of advection on mixing amounts to describing how different lagrangian particles explore the domain and separate from each other conditions for chaotic advection where formula are the components of the velocity field which are assumed to be known from the solution of the equations governing fluid flow such as the navier stokes equations and formula is the physical position if the dynamical system governing trajectories is chaotic the integration of a trajectory is extremely sensitive to initial conditions and neighboring points separate exponentially with time this phenomenon is called chaotic advection dynamical systems and chaos theory state that at least degrees of freedom are necessary for a dynamic system to be chaotic three dimensional flows have three degrees of freedom corresponding to the three coordinates and usually result in chaotic advection except when the flow has symmetries that reduce the number of degrees of freedom in flows with less than degrees of freedom lagrangian trajectories are confined to closed tubes and shear induced mixing can only proceed within these tubes this is the case for d stationary flows in which there are only two degrees of freedom formula and formula for stationary time independent flows lagrangian trajectories of fluid particles coincide with the streamlines of the flow that are isolines of the stream function in d streamlines are concentric closed curves that cross only at stagnation points thus a spot of dyed fluid to be mixed can only explore the region bounded by the most external and internal streamline on which it is lying at the initial time regarding practical applications this configuration is not very satisfying for d unstationary time dependent flows instantaneous closed streamlines and lagrangian trajectories do not coincide any more hence lagrangian trajectories explore a larger volume of the volume resulting in better mixing chaotic advection is observed for most d unstationary flows a famous example is the blinking vortex flow introduced by aref where two fixed rod like agitators are alternately rotated inside the fluid switching periodically the active rotating agitator introduces a time dependency in the flow which results in chaotic advection lagrangian trajectories can therefore escape from closed streamlines and visit a large fraction of the fluid domain shear a flow promotes mixing by separating neighboring fluid particles this separation occurs because of velocity gradients a phenomenon called shearing let formula and formula be two neighboring fluid particles separated by formula at time t when the particles are advected by a flow formula at time formula the approximate separation between the particles can be found through taylor expansion hence and the rate of growth of the separation is therefore given by the gradient of the velocity field in the direction of the separation the plane shear is a simple example of large scale stationary flow that deforms fluid elements because of a uniform shear characterization of chaotic advection lyapunov exponents if the flow is chaotic then small initial errors formula in a trajectory will diverge exponentially we are interested in calculating the e how fast do nearby trajectories diverge the jacobi matrix of the velocity field formula provides information about the local rate of divergence of nearby trajectories or the local rate of stretching of lagrangian space the finite time lyapunov exponents are defined as the time average of the logarithms where formula is the i th lyapunov exponent of the system while formula is the i th principal component of the matrix h if we start with a set of orthonormal initial error vectors formula then the matrix h will map them to a set of final orthogonal error vectors of length formula the action of the system maps an infinitesimal sphere of inititial points to an ellipsoid whose major axis is given by the formula while the minor axis is given by formula where n is the number of dimensions this definition of lyapunov exponents is both more elegant and more appropriate to real world continuous time dynamical systems than the more usual definition based on discrete function maps chaos is defined as the existence of at least one positive lyapunov exponent in a chaotic system we call the lyapunov exponent the asymptotic value of the greatest eigenvalue of h the lyapunov exponent of a flow is a unique quantity that characterizes the asymptotic separation of fluid particles in a given flow it is often used as a measure of the the equivalence of the two methods is due to the ergodicity of the chaotic system filament growth versus evolution of the tracer gradient in parallel with the definition of the lyapunov exponent we define the matrix if we define formula as the squared lengths of the principal components of the tracer where the formula s are arranged as before from largest to smallest therefore growth in the error vector will cause a corresponding decrease in the tracer gradient and vice versa this can be understood very simply and intuitively by considering two nearby points since the difference in tracer concentration will be fixed the only source of variation in the gradients between them will be their separation contour advection contour advection is another useful method for characterizing chaotic mixing in chaotic flows advected contours will grow exponentially over time the figure above shows the frame by frame evolution of a contour advected over several days the figure to the right shows the length of this contour as a function of time the link between exponential contour growth and positive lyapunov exponents is where formula is the path and the integral is performed over the length of the contour sections in chaotic advection a fluid particle travels within a large region and encounters other particles that were initially far from it one can then consider that a particle is mixed with particles that travel within the same region however the region covered by a trajectory does not always span the whole fluid domain sections are used to distinguish regions of good and bad mixing the map is defined as the transformation formula transforms a point like particle into the position of the particle after a time interval t especially for a time periodic flow with period t applying the map several times to a particle gives the successive positions of the particle period after period a section is built by starting from a few different initial conditions and plotting the corresponding iterates this comes down to plotting the trajectories stroboscoped every t avoiding non chaotic islands requires understanding the physical origin of these regions generally speaking changing the geometry of the flow can modify the presence or absence of islands in the figure eight flow for instance for a very thin rod the influence of the rod is not felt far from its location and almost circular trajectories exist within the loops of the figure eight with a larger rod right part of the figure particles can escape from these loops and islands do not exist any more resulting in better mixing with a section the mixing quality of a flow can be analyzed by distinguishing between chaotic and elliptic regions this is a crude measure of the mixing process however since the stretching properties cannot be inferred from this mapping method nevertheless this technique is very useful for studying the mixing of periodic flows and can be extended to a d domain fractal dimension through a continual process of stretching and folding much like in a baker s map tracers advected in chaotic flows will develop into complex fractals the fractal dimension of a single contour will be between and exponential growth ensures that the contour in the limit of very long time integration becomes fractal fractals composed of a single curve are infinitely long and when formed iteratively have an exponential growth rate just like an advected contour the koch snowflake for instance grows at a rate of per iteration the figure below shows the fractal dimension of an advected contour as a function of time measured in four different ways a good method of measuring the fractal dimension of an advected contour is the uncertainty exponent evolution of tracer concentration fields in chaotic advection in fluid mixing one often wishes to homogenize a species that can be characterized by its concentration field q often the species can be considered as a passive tracer that does not modify the flow the species can be for example a dye to be mixed compared to the simple diffusion equation the term proportional to the velocity field formula represents the effect of advection when mixing a spot of tracer the advection term dominates the evolution of the concentration field at the beginning of the mixing process chaotic advection transforms the spot into a bundle of thin filaments the width of a dye filament decreases exponentially with time until an equilibrium scale is reached at which the effect of diffusion starts to be significant this scale is called the batchelor scale it is defined as the square root of the ratio between the diffusion coefficient and the lyapunov exponent where formula is the lyapunov exponent and d is the diffusion coefficient this scale measures the balance between stretching and diffusion on the evolution of the concentration field stretching tends to decrease the width of a filament while diffusion tends to increase it the batchelor scale is the smallest lengthscale that can be observed in the concentration field since diffusion smears out quickly any finer detail when most dye filaments reach the batchelor scale diffusion begins to decrease significantly the contrast of concentration between the filament and the surrounding domain the time at which a filament reaches the batchelor scale is therefore called its mixing time the resolution of the equation shows that after the mixing time of a filament the decrease of the concentration fluctuation due to diffusion is exponential resulting in fast homogenization with the surrounding fluid history of chaotic advection the birth of the theory of chaotic advection is usually traced back to a paper by hassan aref in this work aref studied the mixing induced by two vortices switched alternately in and off inside an inviscid fluid this seminal work had been made possible by earlier developments in the fields of dynamical systems and fluid mechanics in the previous decades vladimir arnold and michel had already noticed that the trajectories advected by area preserving three dimensional flows could be chaotic however the practical interest of chaotic advection for fluid mixing applications remained unnoticed until the work of aref in the s since then the whole toolkit of dynamical systems and chaos theory has been used to characterize fluid mixing by chaotic advection recent work has for example employed topological methods to characterize the stretching of fluid particles other recent directions of research concern the study of chaotic advection in complex flows such as granular flows 
in mathematics the uncertainty exponent is a method of measuring the fractal dimension of a basin boundary in a chaotic scattering system the invariant set of the system is usually not directly accessible because it is non attracting and typically of measure zero therefore the only way to infer the presence of members and to measure the properties of the invariant set is through the basins of attraction note that in a scattering system basins of attraction are not limit cycles therefore do not constitute members of the invariant set suppose we start with a random trajectory and perturb it by a small amount formula in a random direction if the new trajectory ends up in a different basin from the old one then it is called epsilon uncertain if we take a large number of such trajectories then the fraction of them that are epsilon uncertain is the uncertainty fraction formula and we expect it to scale exponentially the uncertainty exponent can be shown to approximate the box counting dimension where n is the embedding dimension please refer to the article on chaotic mixing for an example of numerical computation of the uncertainty dimension compared with that of a box counting dimension 
in the theory of dynamical systems or turbulent flow the scenario is the transition to chaos turbulence due to intermittency 
chaotic scattering is a branch of chaos theory dealing with scattering systems displaying a strong sensitivity to initial conditions in a classical scattering system there will be one or more impact parameters b in which a particle is sent into the scatterer this gives rise to one or more exit parameters y as the particle exits towards infinity while the particle is traversing the system there may also be a delay time t time it takes for the particle to exit the addition to the distance travelled s which in certain systems i e billiard like systems in which the particle undergoes lossless collisions with hard fixed objects the two will be below in a chaotic scattering system a minute change in the impact parameter may give rise to a very large change in the exit parameters system an excellent example system is the gr scattering system known simply as the three disc embodies many of the important concepts in chaotic scattering while being simple and easy to understand and simulate the concept is very simple we have three hard discs arranged in some triangular formation a point particle is sent in and undergoes perfect elastic collisions until it exits towards infinity in this discussion we will only consider gr systems having equally sized discs equally spaced around the points of an equilateral triangle figure illustrates this system while figure shows two example trajectories note first that the trajectories bounce around the system for some time before finally exiting note also that if we consider the impact parameters to be the start of the two perfectly horizontal lines at left the system is completely reversible the exit point could also be the entry point the two trajectories are initially so close as to be almost identical by the time they exit they are completely different thus illustrating the strong sensitivity to initial conditions this system will be used as an example throughout the article decay rate if we introduce a large number of particles with uniformly distributed impact parameters the rate at which they exit the system is known as the decay rate we can calculate the decay rate by simulating the system over many trials and forming a histogram of the delay time t for the gr system it is easy to see that the delay time and the length of the particle trajectory are equivalent but for a multiplication coefficient a typical choice for the impact parameter is the y coordinate while the trajectory angle is kept constant at zero meanwhile we say that the particle has exited the system once it passes a border some arbitrary but sufficiently large distance from the centre of the system where n is the total number of particles figure shows a plot of the path length versus the number of particles for a simulation of one million particles started with random impact parameter b a fitted straight line of negative slope formula is overlaid the path length s is equivalent to the decay time t provided we scale the constant speed appropriately note that an exponential decay rate is a property specifically of hyperbolic chaotic scattering non hyperbolic scatterers may have an arithmetic decay rate an experimental system and the stable manifold figure shows an experimental realization of the system using a laser instead of a point particle as anyone who s actually tried this knows this is not a very effective method of testing the laser beam gets scattered in every direction as shown by sweet ott and yorke a more effective method is to direct coloured light through the gaps between the discs or in this case tape coloured strips of paper across pairs of cylinders and view the reflections through an open gap the result is a complex pattern of stripes of alternating colour as shown below seen more clearly in the simulated version below that figures and show the basins of attraction for each impact parameter b that is for a given value of b through which gap does the particle exit the basin boundaries form a cantor set and represent members of the stable manifold trajectories that once started never exit the system the invariant set and the symbolic dynamics so long as it is symmetric we can easily think of the system as an iterated function map a common method of representing a chaotic dynamical system figure shows one possible representation of the variables with the first variable formula representing the angle around the disc at rebound and the second formula representing the impact rebound angle relative to the disc a subset of these two variables called the invariant set will map onto themselves this set four members of which are shown in figures and will be fractal totally non attracting and of measure zero this is an interesting inversion of the more normally discussed chaotic systems in which the fractal invariant set is attracting and in fact comprises the basin of attraction note that the totally non attracting nature of the invariant set is another property of a hyperbolic chaotic scatterer each member of the invariant set can be modelled using symbolic dynamics the trajectory is labelled based on each of the discs off of which it rebounds the set of all such sequences form an uncountable set pre members of the stable manifold may be likewise represented except each sequence will have a starting point when you consider that a member of the invariant set must fit in the boundaries between two basins of attraction it is apparent that if perturbed the trajectory may exit anywhere along the sequence thus it should also be apparent that an infinite number of alternating basins of all three colours will exist between any given boundary because of their unstable nature it is difficult to access members of the invariant set or the stable manifold directly the uncertainty exponent is ideally tailored to measure the fractal dimension of this type of system once again using the single impact parameter b we perform multiple trials with random impact parameters perturbing them by a minute amount formula and counting how frequently the number of rebounds off the discs changes that is the uncertainty fraction note that even though the system is two dimensional a single impact parameter is sufficient to measure the fractal dimension of the stable manifold this is demonstrated in figure which shows the basins of attraction plotted as a function of a dual impact parameter formula and formula the stable manifold which can be seen in the boundaries between the basins is fractal along only one dimension figure plots the uncertainty fraction f as a function of the uncertainty formula for a simulated system the slope of the fitted curve returns the uncertainty exponent formula thus the box counting dimension of the stable manifold is formula the invariant set is the intersection of the stable and unstable manifolds since the system is the same whether run forwards or backwards the unstable manifold is simply the mirror image of the stable manifold and their fractal dimensions will be equal on this basis we can calculate the fractal dimension of the invariant set where d s and d u are the fractal dimensions of the stable and unstable manifolds respectively and n is the dimensionality of the system the fractal dimension of the invariant set is d relationship between the fractal dimension decay rate and lyapunov exponents where d is the information dimension and and are the small and large lyapunov exponents respectively for an attractor formula and it reduces to the conjecture 
the double scroll attractor sometimes known as chua s attractor is a strange attractor observed from a physical electronic chaotic circuit generally chua s circuit with a single nonlinear resistor see chua s diode the double scroll system is often described by a system of three nonlinear ordinary differential equations and a segment piecewise linear equation see chua s equations this makes the system easily simulated numerically and easily manifested physically due to chua s circuits simple design using a chua s circuit this shape is viewed on an oscilloscope using the x y and z output signals of the circuit this chaotic attractor is known as the double scroll because of its shape in three dimensional space which is similar to two saturn like rings connected by swirling lines the attractor was first observed in simulations then realized physically after leon chua invented the autonomous chaotic circuit which became known as chua s circuit the double scroll attractor from the chua circuit was rigorously proven to be chaotic through a number of return maps of the attractor explicitly derived by way of compositions of the eigenvectors of the dimensional state space numerical analysis of the double scroll attractor has shown that its geometrical structure is made up of an infinite number of fractal like layers each cross section appears to be a fractal at all scales recently there has also been reported the discovery of hidden attractors within the double scroll variations on the double scroll comprise the family of so called n scroll attractors that is multiple scrolls in a single attractor 
structural biology is a branch of molecular biology biochemistry and biophysics concerned with the molecular structure of biological macromolecules especially proteins and nucleic acids how they acquire the structures they have and how alterations in their structures affect their function this subject is of great interest to biologists because macromolecules carry out most of the functions of cells and because it is only by coiling into specific three dimensional shapes that they are able to perform these functions this architecture the tertiary structure of molecules depends in a complicated way on the molecules basic composition or primary structures most often researchers use them to study the native states of macromolecules but variations on these methods are also used to watch nascent or denatured molecules assume or reassume their native states see protein folding a third approach that structural biologists take to understanding structure is bioinformatics to look for patterns among the diverse sequences that give rise to particular shapes researchers often can deduce aspects of the structure of integral membrane proteins based on the membrane topology predicted by hydrophobicity analysis see protein structure prediction in the past few years it has become possible for highly accurate physical molecular models to complement the in silico study of biological structures 
in molecular biology and genetics a transcription factor sometimes called a sequence specific dna binding factor is a protein that binds to specific dna sequences thereby controlling the flow or transcription of genetic information from dna to mrna transcription factors perform this function alone or with other proteins in a complex by promoting as an activator or blocking as a repressor the recruitment of rna polymerase the enzyme that performs the transcription of genetic information from dna to rna to specific genes a defining feature of transcription factors is that they contain one or more dna binding domains dbds which attach to specific sequences of dna adjacent to the genes that they regulate additional proteins such as coactivators chromatin remodelers histone acetylases deacetylases kinases and methylases while also playing crucial roles in gene regulation lack dna binding domains and therefore are not classified as transcription factors conservation in different organisms transcription factors are essential for the regulation of gene expression and are as a consequence found in all living organisms the number of transcription factors found within an organism increases with genome size and larger genomes tend to have more transcription factors per gene there are approximately proteins in the human genome that contain dna binding domains and most of these are presumed to function as transcription factors therefore approximately of genes in the genome code for transcription factors which makes this family the single largest family of human proteins furthermore genes are often flanked by several binding sites for distinct transcription factors and efficient expression of each of these genes requires the cooperative action of several different transcription factors see for example hepatocyte nuclear factors hence the combinatorial use of a subset of the approximately human transcription factors easily accounts for the unique regulation of each gene in the human genome during development function basal transcription regulation in eukaryotes an important class of transcription factors called general transcription factors gtfs are necessary for transcription to occur many of these gtfs don t actually bind dna but are part of the large transcription preinitiation complex that interacts with rna polymerase directly the most common gtfs are tfiia tfiib tfiid see also tata binding protein tfiie tfiif and tfiih the preinitiation complex binds to promoter regions of dna upstream to the gene that they regulate differential enhancement of transcription other transcription factors differentially regulate the expression of various genes by binding to enhancer regions of dna adjacent to regulated genes these transcription factors are critical to making sure that genes are expressed in the right cell at the right time and in the right amount depending on the changing requirements of the organism development many transcription factors in multicellular organisms are involved in development responding to cues stimuli these transcription factors turn on off the transcription of the appropriate genes which in turn allows for changes in cell morphology or activities needed for cell fate determination and cellular differentiation the hox transcription factor family for example is important for proper body pattern formation in organisms as diverse as fruit flies to humans another example is the transcription factor encoded by the sex determining region y sry gene which plays a major role in determining gender in humans response to intercellular signals cells can communicate with each other by releasing molecules that produce signaling cascades within another receptive cell if the signal requires upregulation or downregulation of genes in the recipient cell often transcription factors will be downstream in the signaling cascade estrogen signaling is an example of a fairly short signaling cascade that involves the estrogen receptor transcription factor estrogen is secreted by tissues such as the ovaries and placenta crosses the cell membrane of the recipient cell and is bound by the estrogen receptor in the cell s cytoplasm the estrogen receptor then goes to the cell s nucleus and binds to its dna binding sites changing the transcriptional regulation of the associated genes response to environment not only do transcription factors act downstream of signaling cascades related to biological stimuli but they can also be downstream of signaling cascades involved in environmental stimuli examples include heat shock factor hsf which upregulates genes necessary for survival at higher temperatures hypoxia inducible factor hif which upregulates genes necessary for cell survival in low oxygen environments and sterol regulatory element binding protein srebp which helps maintain proper lipid levels in the cell cell cycle control many transcription factors especially some that are proto oncogenes or tumor suppressors help regulate the cell cycle and as such determine how large a cell will get and when it can divide into two daughter cells one example is the myc oncogene which has important roles in cell growth and apoptosis pathogenesis transcription factors can also be used to alter gene expression in a host cell to promote pathogenesis a well studied example of this are the transcription activator like effectors tal effectors secreted by xanthomonas bacteria when injected into plants these proteins can enter the nucleus of the plant cell bind plant promoter sequences and activate transcription of plant genes that aid in bacterial infection tal effectors contain a central repeat region in which there is a simple relationship between the identity of two critical residues in sequential repeats and sequential dna bases in the tal target site this property likely makes it easier for these proteins to evolve in order to better compete with the defense mechanisms of the host cell regulation synthesis transcription factors like all proteins are transcribed from a gene on a chromosome into rna and then the rna is translated into protein any of these steps can be regulated to affect the production and thus activity of a transcription factor one interesting implication of this is that transcription factors can regulate themselves for example in a negative feedback loop the transcription factor acts as its own repressor if the transcription factor protein binds the dna of its own gene it will down regulate the production of more of itself this is one mechanism to maintain low levels of a transcription factor in a cell nuclear localization in eukaryotes transcription factors like most proteins are transcribed in the nucleus but are then translated in the cell s cytoplasm many proteins that are active in the nucleus contain nuclear localization signals that direct them to the nucleus but for many transcription factors this is a key point in their regulation important classes of transcription factors such as some nuclear receptors must first bind a ligand while in the cytoplasm before they can relocate to the nucleus accessibility of dna binding site in eukaryotes the dna is organized with the help of histones in the compact particles the nucleosomes where about dna base pairs make two turns around the histone protein octamer dna within nucleosomes is inaccessible to many transcription factors some transcription factors so called pioneering factors are still able to bind their dna binding sites on the nucleosomal dna for most of other transcription factors the nucleosome should be actively removed by molecular motors such as chromatin remodelers alternatively the nucleosome can be partially unwrapped by thermal fluctuations allowing temporary access to the transcription factor binding site in many cases a transcription factor needs to compete for binding to its dna binding site with other transcription factors and histones or non histone chromatin proteins pairs of transcription factors and other proteins can play antagonistic roles activator versus repressor in the regulation of the same gene availability of other cofactors transcription factors most transcription factors do not work alone often for gene transcription to occur a number of transcription factors must bind to dna regulatory sequences this collection of transcription factors in turn recruit intermediary proteins such as cofactors that allow efficient recruitment of the preinitiation complex and rna polymerase thus for a single transcription factor to initiate transcription all of these other proteins must also be present and the transcription factor must be in a state where it can bind to them if necessary structure trans activating domain trans activating domains tads are named after their amino acid composition these amino acids are either essential for the activity or simply the most abundant in the tad transactivation by the transcription factor is mediated by acidic amino acids whereas hydrophobic residues in play a similar role hence the tads in and are referred to as acidic or hydrophobic activation domains respectively nine amino acid transactivation domain defines a novel domain common to a large superfamily of eukaryotic transcription factors represented by in yeast and by nfat nf and in mammals martin piskacek nature precedings http precedings nature com documents version martin piskacek nature precedings http precedings nature com documents version martin piskacek nature precedings http precedings nature com documents version ref prediction for tads for both acidic and hydrophilic transactivation domains is available online from expasy and embnet spain transcription factors mll nf and nf interact directly with the general coactivators and cbp interact with and with multiple domains of cbp kix and ibid kix domain of general coactivators interacts with transcription factors and interactions of and with were reported is a common transactivation domain recruits multiple general coactivators cbp and response elements the dna sequence that a transcription factor binds to is called a transcription factor binding site or response element transcription factors interact with their binding sites using a combination of electrostatic of which hydrogen bonds are a special case and van der waals forces due to the nature of these chemical interactions most transcription factors bind dna in a sequence specific manner however not all bases in the transcription factor binding site may actually interact with the transcription factor in addition some of these interactions may be weaker than others thus transcription factors do not bind just one sequence but are capable of binding a subset of closely related sequences each with a different strength of interaction for example although the consensus binding site for the tata binding protein tbp is tataaaa the tbp transcription factor can also bind similar sequences such as tatatat or tatataa because transcription factors can bind a set of related sequences and these sequences tend to be short potential transcription factor binding sites can occur by chance if the dna sequence is long enough it is unlikely however that a transcription factor binds all compatible sequences in the genome of the cell other constraints such as dna accessibility in the cell or availability of cofactors may also help dictate where a transcription factor will actually bind thus given the genome sequence it is still difficult to predict where a transcription factor will actually bind in a living cell additional recognition specificity however may be obtained through the use of more than one dna binding domain for example tandem dbds in the same transcription factor or through dimerization of two transcription factors that bind to two or more adjacent sequences of dna clinical significance transcription factors are of clinical significance for at least two reasons mutations can be associated with specific diseases and they can be targets of medications disorders due to their important roles in development intercellular signaling and cell cycle some human diseases have been associated with mutations in transcription factors many transcription factors are either tumor suppressors or oncogenes and thus mutations or aberrant regulation of them is associated with cancer three groups of transcription factors are known to be important in human cancer the nf kappab and ap families the stat family and the steroid receptors potential drug targets approximately of currently prescribed drugs directly target the nuclear receptor class of transcription factors examples include tamoxifen and bicalutamide for the treatment of breast and prostate cancer respectively and various types of anti inflammatory and anabolic steroids in addition transcription factors are often indirectly modulated by drugs through signaling cascades it might be possible to directly target other less explored transcription factors such as nf with drugs transcription factors outside the nuclear receptor family are thought to be more difficult to target with small molecule therapeutics since it is not clear that they are drugable but progress has been made on the notch pathway analysis there are different technologies available to analyze transcription factors on the genomic level dna sequencing and database research are commonly used the protein version of the transcription factor is detectable by using specific antibodies the sample is detected on a western blot by using electrophoretic mobility shift assay emsa the activation profile of transcription factors can be detected a multiplex approach for activation profiling is a tf chip system where several of different transcription factors can be detected in parallel this technology is based on dna microarrays providing the specific dna binding sequence for the transcription factor protein on the array surface classes as described in more detail below transcription factors may be classified by their mechanism of action regulatory function or sequence homology and hence structural similarity in their dna binding domains 
biophysics is an interdisciplinary science that uses the methods of physical science to study biological systems studies included under the branches of biophysics span all levels of biological organization from the molecular scale to whole organisms and ecosystems biophysical research shares significant overlap with biochemistry nanotechnology bioengineering agrophysics and systems biology molecular biophysics typically addresses biological questions that are similar to those in biochemistry and molecular biology but the questions are approached quantitatively scientists in this field conduct research concerned with understanding the interactions between the various systems of a cell including the interactions between dna rna and protein biosynthesis as well as how these interactions are regulated a great variety of techniques are used to answer these questions fluorescent imaging techniques as well as electron microscopy x ray crystallography nmr spectroscopy and atomic force microscopy afm are often used to visualize structures of biological significance conformational change in structure can be measured using techniques such as dual polarisation interferometry and circular dichroism direct manipulation of molecules using optical tweezers or afm can also be used to monitor biological events where forces and distances are at the nanoscale molecular biophysicists often consider complex biological events as systems of interacting units which can be understood through statistical mechanics thermodynamics and chemical kinetics by drawing knowledge and experimental techniques from a wide variety of disciplines biophysicists are often able to directly observe model or even manipulate the structures and interactions of individual molecules or complexes of molecules in addition to traditional i e molecular and cellular biophysical topics like structural biology or enzyme kinetics modern biophysics encompasses an extraordinarily broad range of research from bioelectronics to quantum biology involving both experimental and theoretical tools it is becoming increasingly common for biophysicists to apply the models and experimental techniques derived from physics as well as mathematics and statistics see biomathematics to larger systems such as tissues organs populations and ecosystems focus as a subfield biophysics often does not have university level departments of its own but has presence as groups across departments within the fields of molecular biology biochemistry chemistry computer science mathematics medicine pharmacology physiology physics and neuroscience what follows is a list of examples of how each department applies its efforts toward the study of biophysics this list is hardly all inclusive nor does each subject of study belong exclusively to any particular department each academic institution makes its own rules and there is much overlap between departments many biophysical techniques are unique to this field research efforts in biophysics are often initiated by scientists who were traditional physicists chemists and biologists by training 
differential scanning calorimetry or dsc is a thermoanalytical technique in which the difference in the amount of heat required to increase the temperature of a sample and reference is measured as a function of temperature both the sample and reference are maintained at nearly the same temperature throughout the experiment generally the temperature program for a dsc analysis is designed such that the sample holder temperature increases linearly as a function of time the reference sample should have a well defined heat capacity over the range of temperatures to be scanned the technique was developed by e s watson and m j o neill in and introduced commercially at the pittsburgh conference on analytical chemistry and applied spectroscopy the first adiabatic differential scanning calorimeter that could be used in biochemistry was developed by p l privalov and d r monaselidze in the term dsc was coined to describe this instrument which measures energy directly and allows precise measurements of heat capacity detection of phase transitions the basic principle underlying this technique is that when the sample undergoes a physical transformation such as phase transitions more or less heat will need to flow to it than the reference to maintain both at the same temperature whether less or more heat must flow to the sample depends on whether the process is exothermic or endothermic for example as a solid sample melts to a liquid it will require more heat flowing to the sample to increase its temperature at the same rate as the reference this is due to the absorption of heat by the sample as it undergoes the endothermic phase transition from solid to liquid likewise as the sample undergoes exothermic processes such as crystallization less heat is required to raise the sample temperature by observing the difference in heat flow between the sample and reference differential scanning calorimeters are able to measure the amount of heat absorbed or released during such transitions dsc may also be used to observe more subtle phase changes such as glass transitions it is widely used in industrial settings as a quality control instrument due to its applicability in evaluating sample purity and for studying polymer curing dta an alternative technique which shares much in common with dsc is differential thermal analysis dta in this technique it is the heat flow to the sample and reference that remains the same rather than the temperature when the sample and reference are heated identically phase changes and other thermal processes cause a difference in temperature between the sample and reference both dsc and dta provide similar information dsc measures the energy required to keep both the reference and the sample at the same temperature whereas dta measures the difference in temperature between the sample and the reference when they are both put under the same heat dsc curves formula where formula is the enthalpy of transition formula is the calorimetric constant and formula is the area under the curve the calorimetric constant will vary from instrument to instrument and can be determined by analyzing a well characterized sample with known enthalpies of transition applications differential scanning calorimetry can be used to measure a number of characteristic properties of a sample using this technique it is possible to observe fusion and crystallization events as well as glass transition temperatures tg dsc can also be used to study oxidation as well as other chemical reactions glass transitions may occur as the temperature of an amorphous solid is increased these transitions appear as a step in the baseline of the recorded dsc signal this is due to the sample undergoing a change in heat capacity no formal phase change occurs as the temperature increases an amorphous solid will become less viscous at some point the molecules may obtain enough freedom of motion to spontaneously arrange themselves into a crystalline form this is known as the crystallization temperature tc this transition from amorphous solid to crystalline solid is an exothermic process and results in a peak in the dsc signal as the temperature increases the sample eventually reaches its melting temperature tm the melting process results in an endothermic peak in the dsc curve the ability to determine transition temperatures and enthalpies makes dsc a valuable tool in producing phase diagrams for various chemical systems examples the technique is widely used across a range of applications both as a routine quality test and as a research tool the equipment is easy to calibrate using low melting indium at for example and is a rapid and reliable method of thermal analysis polymers dsc is used widely for examining polymers to check their composition melting points and glass transition temperatures for most polymers are available from standard compilations and the method can show possible polymer degradation by the lowering of the expected melting point tm for example tm depends on the molecular weight of the polymer so lower grades will have lower melting points than expected the percentage crystallinity of a polymer can be found from the crystallization peak of the dsc graph since the heat of fusion can be calculated from the area under an absorption peak dsc can also be used to study thermal degradation of polymers impurities in polymers can be determined by examining thermograms for anomalous peaks and plasticisers can be detected at their characteristic boiling points liquid crystals dsc is used in the study of liquid crystals as some forms of matter go from solid to liquid they go through a third state which displays properties of both phases this anisotropic liquid is known as a liquid crystalline or mesomorphous state using dsc it is possible to observe the small energy changes that occur as matter transitions from a solid to a liquid crystal and from a liquid crystal to an isotropic liquid oxidative stability using differential scanning calorimetry to study the stability to oxidation of samples generally requires an airtight sample chamber usually such tests are done isothermally at constant temperature by changing the atmosphere of the sample first the sample is brought to the desired test temperature under an inert atmosphere usually nitrogen then oxygen is added to the system any oxidation that occurs is observed as a deviation in the baseline such analysis can be used to determine the stability and optimum storage conditions for a material or compound safety screening dsc makes a reasonable initial safety screening tool in this mode the sample will be housed in a non reactive crucible often gold or gold plated steel and which will be able to withstand pressure typically up to bar the presence of an exothermic event can then be used to assess the stability of a substance to heat however due to a combination of relatively poor sensitivity slower than normal scan rates typically min due to much heavier crucible and unknown activation energy it is necessary to deduct about from the initial start of the observed exotherm to suggest a maximum temperature for the material a much more accurate data set can be obtained from an adiabatic calorimeter but such a test may take days from ambient at a rate of a increment per half hour drug analysis dsc is widely used in the pharmaceutical and polymer industries for the polymer chemist dsc is a handy tool for studying curing processes which allows the fine tuning of polymer properties the cross linking of polymer molecules that occurs in the curing process is exothermic resulting in a positive peak in the dsc curve that usually appears soon after the glass transition in the pharmaceutical industry it is necessary to have well characterized drug compounds in order to define processing parameters for instance if it is necessary to deliver a drug in the amorphous form it is desirable to process the drug at temperatures below those at which crystallization can occur general chemical analysis freezing point depression can be used as a purity analysis tool when analysed by differential scanning calorimetry this is possible because the temperature range over which a mixture of compounds melts is dependent on their relative amounts consequently less pure compounds will exhibit a broadened melting peak that begins at lower temperature than a pure compound 
bioelectromagnetism sometimes equated with bioelectricity refers to the electrical magnetic or electromagnetic fields produced by living cells tissues or organisms examples include the cell membrane potential and the electric currents that flow in nerves and muscles as a result of action potentials bioelectromagnetism is somewhat similar to bioelectromagnetics which deals with the effect on life from external electromagnetism yet such an effect also falls under the definition of bioelectromagnetism description biological cells use bioelectricity to store metabolic energy to do work or trigger internal changes and to signal one another bioelectromagnetism is the electric current produced by action potentials along with the magnetic fields they generate through the phenomenon of electromagnetism bioelectromagnetism is studied primarily through the techniques of electrophysiology in the late eighteenth century the italian physician and physicist luigi galvani first recorded the phenomenon while dissecting a frog at a table where he had been conducting experiments with static electricity galvani coined the term animal electricity to describe the phenomenon while contemporaries labeled it galvanism galvani and contemporaries regarded muscle activation as resulting from an electrical fluid or substance in the nerves bioelectromagnetism is an aspect of all living things including all plants and animals some animals have acute bioelectric sensors and others such as migratory birds are believed to navigate in part by orienteering with respect to the earth s magnetic field also sharks are more sensitive to local interaction in electromagnetic fields than most humans other animals such as the electric eel are able to generate large electric fields outside their bodies in the life sciences biomedical engineering uses concepts of circuit theory molecular biology pharmacology and bioelectricity bioelectromagnetism is associated with biorhythms and chronobiology biofeedback is used in physiology and psychology to monitor rhythmic cycles of physical mental and emotional characteristics and as a technique for teaching the control of bioelectric functions bioelectromagnetism involves the interaction of ions there are multiple categories of bioelectromagnetism such as brainwaves myoelectricity e g heart muscle phenomena and other related subdivisions of the same general bioelectromagnetic phenomena one such phenomenon is a brainwave which neurophysiology studies where bioelectromagnetic fluctuations of voltage between parts of the cerebral cortex are detectable with an electroencephalograph this is primarily studied in the brain by way of electroencephalograms 
optical tweezers originally called single beam gradient force trap are scientific instruments that use a highly focused laser beam to provide an attractive or repulsive force typically on the order of piconewtons depending on the refractive index mismatch to physically hold and move microscopic dielectric objects optical tweezers have been particularly successful in studying a variety of biological systems in recent years history and development the detection of optical scattering and gradient forces on micrometer sized particles was first reported in by arthur ashkin a scientist working at bell labs years later ashkin and colleagues reported the first observation of what is now commonly referred to as an optical tweezers a tightly focused beam of light capable of holding microscopic particles stable in three dimensions one of the authors of this seminal paper united states secretary of energy steven chu would go on to use optical tweezing in his work on cooling and trapping neutral atoms this research earned chu the nobel prize in physics along with claude cohen tannoudji and william d phillips interview conducted for internal newsletter at bell labs contains confirmation of ashkin as the inventor of optical trapping and provides information on the nobel prize in physics ref in an interview steven chu described how ashkin had first envisioned optical tweezing as a method for trapping atoms ashkin was able to trap larger particles to nanometers in diameter but it fell to chu to extend these techniques to the trapping of neutral atoms nanometers in diameter utilizing resonant laser light and a magnetic gradient trap cf magneto optical trap in the late arthur ashkin and joseph m dziedzic demonstrated the first application of the technology to the biological sciences using it to trap an individual tobacco mosaic virus and escherichia coli bacterium throughout the and afterwards researchers like carlos bustamante james spudich and steven block pioneered the use of optical trap force spectroscopy to characterize molecular scale biological motors these molecular motors are ubiquitous in biology and are responsible for locomotion and mechanical action within the cell optical traps allowed these biophysicists to observe the forces and dynamics of nanoscale motors at the single molecule level optical trap force spectroscopy has since led to greater understanding of the stochastic nature of these force generating molecules optical tweezers have proven useful in other areas of biology as well for instance in the techniques of optical tweezers were applied in the field of cell sorting by creating a large optical intensity pattern over the sample area cells can be sorted by their intrinsic optical characteristics optical tweezers have also been used to probe the cytoskeleton measure the visco elastic properties of biopolymers and study cell motility researchers have also worked to convert optical tweezers from large complex instruments to smaller simpler ones for use by those with smaller research budgets physics of optical tweezers general description optical tweezers are capable of manipulating nanometer and micrometer sized dielectric particles by exerting extremely small forces via a highly focused laser beam the beam is typically focused by sending it through a microscope objective the narrowest point of the focused beam known as the beam waist contains a very strong electric field gradient it turns out that dielectric particles are attracted along the gradient to the region of strongest electric field which is the center of the beam the laser light also tends to apply a force on particles in the beam along the direction of beam propagation it is easy to understand why if one considers conservation of momentum photons that are absorbed or scattered by the tiny dielectric particle in its path impart momentum to the dielectric particle this is known as the scattering force and results in the particle being displaced slightly downstream from the exact position of the beam waist as seen in the figure optical traps are very sensitive instruments and are capable of the manipulation and detection of sub nanometer displacements for sub micrometre dielectric particles for this reason they are often used to manipulate and study single molecules by interacting with a bead that has been attached to that molecule dna and the proteins and enzymes that interact with it are commonly studied in this way for quantitative scientific measurements most optical traps are operated in such a way that the dielectric particle rarely moves far from the trap center the reason for this is that the force applied to the particle is linear with respect to its displacement from the center of the trap as long as the displacement is small in this way an optical trap can be compared to a simple spring which follows hooke s law detailed view of optical tweezers proper explanation of optical trapping behavior depends upon the size of the trapped particle relative to the wavelength of light used to trap it in cases where the dimensions of the particle are much greater than the wavelength a simple ray optics treatment is sufficient if the wavelength of light far exceeds the particle dimensions the particles can be treated as electric dipoles in an electric field for optical trapping of dielectric objects of dimensions within an order of magnitude of the trapping beam wavelength the only accurate models involve the treatment of either time dependent or time harmonic maxwell equations using appropriate boundary conditions the ray optics approach in cases where the diameter of a trapped particle is significantly greater than the wavelength of light the trapping phenomenon can be explained using ray optics as shown in the figure individual rays of light emitted from the laser will be refracted as it enters and exits the dielectric bead as a result the ray will exit in a direction different from which it originated since light has a momentum associated with it this change in direction indicates that its momentum has changed due to newton s third law there should be an equal and opposite momentum change on the particle most optical traps operate with a gaussian beam mode profile intensity in this case if the particle is displaced from the center of the beam as in the right part of the figure the particle has a net force returning it to the center of the trap because more intense beams impart a larger momentum change towards the center of the trap than less intense beams which impart a smaller momentum change away from the trap center the net momentum change or force returns the particle to the trap center if the particle is located at the center of the beam then individual rays of light are refracting through the particle symmetrically resulting in no net lateral force the net force in this case is along the axial direction of the trap which cancels out the scattering force of the laser light the cancellation of this axial gradient force with the scattering force is what causes the bead to be stably trapped slightly downstream of the beam waist the standard tweezers works with the trapping laser propagated in the direction of gravity and the inverted tweezers works against gravity the electric dipole approximation in cases where the diameter of a trapped particle is significantly smaller than the wavelength of light the conditions for rayleigh scattering are satisfied and the particle can be treated as a point dipole in an inhomogenous electromagnetic field the force applied on a single charge in an electromagnetic field is known as the lorentz force the force on the dipole can be calculated by substituting two terms for the electric field in the equation above one for each charge the polarization of a dipole is formula where formula is the distance between the two charges for a point dipole the distance is infinitesimal formula taking into account that the two charges have opposite signs the force takes the form notice that the formula cancel out multiplying through by the charge formula converts position formula into polarization formula where in the second equality it has been assumed that the dielectric particle is linear i e formula in the final steps two equalities will be used a vector analysis equality one of maxwell s equations first the vector equality will be inserted for the first term in the force equation above maxwell s equation will be substituted in for the second term in the vector equality then the two terms which contain time derivatives can be combined into a single term the second term in the last equality is the time derivative of a quantity that is related through a multiplicative constant to the poynting vector which describes the power per unit area passing through a surface since the power of the laser is constant when sampling over frequencies much shorter than the frequency of the laser s light hz the derivative of this term averages to zero and the force can be written as the square of the magnitude of the electric field is equal to the intensity of the beam as a function of position therefore the result indicates that the force on the dielectric particle when treated as a point dipole is proportional to the gradient along the intensity of the beam in other words the gradient force described here tends to attract the particle to the region of highest intensity in reality the scattering force of the light works against the gradient force in the axial direction of the trap resulting in an equilibrium position that is displaced slightly downstream of the intensity maximum under the rayleigh approximation we can write the scattering force as since the scattering is isotropic the net momentum is transferred in the forward direction on the quantum level we picture this as incident photons all traveling in the forward direction and being scattered isotropically by conservation of momentum the sphere must accumulate the photons original momenta causing a forward force experimental design construction and operation the most basic optical tweezer setup will likely include the following components a laser usually a beam expander some optics used to steer the beam location in the sample plane a microscope objective and condenser to create the trap in the sample plane a position detector e g quadrant photodiode to measure beam displacements and a microscope illumination source coupled to a ccd camera an nd yag laser wavelength is a common choice of laser for working with biological specimens this is because such specimens being mostly water have a low absorption coefficient at this wavelength a low absorption is advisable so as to minimise damage to the biological material sometimes referred to as opticution perhaps the most important consideration in optical tweezer design is the choice of the objective a stable trap requires that the gradient force which is dependent upon the numerical aperture na of the objective be greater than the scattering force suitable objectives typically have an na between and while alternatives are available perhaps the simplest method for position detection involves imaging the trapping laser exiting the sample chamber onto a quadrant photodiode lateral deflections of the beam are measured similarly to how it is done using atomic force microscopy afm expanding the beam emitted from the laser to fill the aperture of the objective will result in a tighter diffraction limited spot while lateral translation of the trap relative to the sample can be accomplished by translation of the microscope slide most tweezer setups have additional optics designed to translate the beam to give an extra degree of translational freedom this can be done by translating the first of the two lenses labelled as beam steering in the figure for example translation of that lens in the lateral plane will result in a laterally deflected beam from what is drawn in the figure if the distance between the beam steering lenses and the objective are chosen properly this will correspond to a similar deflection before entering the objective and a resulting lateral translation in the sample plane the position of the beam waist that is the focus of the optical trap can be adjusted by an axial displacement of the initial lens such an axial displacement causes the beam to diverge or converge slightly the end result of which is an axially displaced position of the beam waist in the sample chamber visualization of the sample plane is usually accomplished through illumination via a separate light source coupled into the optical path in the opposite direction using dichroic mirrors this light is incident on a ccd camera and can be viewed on an external monitor or used for tracking the trapped particle position via video tracking descriptions of various optical tweezer setups optical tweezers based on alternate laser beam modes the majority of optical tweezers make use of conventional gaussian beams however a number of other beam types have been used to trap particles including high order laser beams i e hermite gaussian beam temxy laguerre gaussian lg beams templ and bessel beams optical tweezers based on laguerre gaussian beams have the unique capability of trapping particles that are optically reflective and absorptive laguerre gaussian beams also possess a well defined orbital angular momentum that can rotate particles this is accomplished without external mechanical or electrical steering of the beam both zero and higher order bessel beams also possess a unique tweezing ability they can trap and rotate multiple particles that are millimeters apart and even around obstacles micromachines can be driven by these unique optical beams due to their intrinsic rotating mechanism due to the spin and orbital angular momentum of light multiplexed optical tweezers a typical setup uses one laser to create one or two traps commonly two traps are generated by splitting the laser beam into two orthogonally polarized beams optical tweezing operations with more than two traps can be realized either by time sharing a single laser beam among several optical tweezers or by diffractively splitting the beam into multiple traps with acousto optic deflectors or galvanometer driven mirrors a single laser beam can be shared among hundreds of optical tweezers in the focal plane or else spread into an extended one dimensional trap specially designed diffractive optical elements can divide a single input beam into hundreds of continuously illuminated traps in arbitrary three dimensional configurations the trap forming hologram also can specify the mode structure of each trap individually thereby creating arrays of optical vortices optical tweezers and holographic line traps for example when implemented with a spatial light modulator such holographic optical traps also can move objects in three dimensions optical traps based on single mode optical fibers the standard fiber optical trap relies on the same principle as the optical trapping but with the gaussian laser beam delivered through an optical fiber if one end of the optical fiber is moulded into a lens like facet the nearly gaussian beam carried by a single mode standard fiber will be focussed at some distance from the fiber tip the effective numerical aperture of such assembly is usually not enough to allow for a full optical trap but only for a trap optical trapping and manipulation of objects will be possible only when e g they are in contact with a surface a true optical trapping based on a single fiber with a trapping point which is not in nearly contact with the fiber tip has been realized based on a not standard annular core fiber arrangement and a total internal reflection geometry on the other hand if the ends of the fiber are not moulded the laser exiting the fiber will be diverging and thus a stable optical trap can only be realised by balancing the gradient and the scattering force from two opposing ends of the fiber the gradient force will trap the particles the transverse direction while the axial optical force comes from the scattering force of the two counter propagating beams emerging from the two fibers the equilibrium z position of such a trapped bead is where the two scattering forces equal each other this work was pioneered by a constable et al opt lett and followed by j guck et al phys rev lett who made use of this technique to stretch microparticles by manipulating the input power into the two ends of the fiber there will be an increase of a optical stretching that can be used to measure viscoelastic properties of cells with sensitivity sufficient to distinguish between different individual cytoskeletal phenotypes i e human erythrocytes and mouse fibroblasts a recent test has seen great success in differentiating cancerous cells from non cancerous ones from the two opposed non focused laser beams multimode fiber based traps for advanced manipulation while earlier version of fiber based laser traps exclusively used single mode beams m kreysing and colleagues recently showed that the careful excitation of further optical modes in a short piece of optical fiber allows the realization of non trivial trapping geometries by this the researchers were able to orient various human cell types individual cells and clusters on a microscope the main advantage of the so called optical cell rotator technology over standard optical tweezers is the decoupling of trapping from imaging optics this its modular design and the high compatibility of divergent laser traps with biological material indicates the great potential of this new generation of laser traps in medical research and life science optical tweezers in a landscape cell sorting one of the more common cell sorting systems makes use of flow cytometry through fluorescent imaging in this method a suspension of biologic cells is sorted into two or more containers based upon specific fluorescent characteristics of each cell during an assisted flow by using an electrical charge that the cell is trapped in the cells are then sorted based on the fluorescence intensity measurements the sorting process is undertaken by an electrostatic deflection system that diverts cells into containers based upon their charge in the optically actuated sorting process the cells are flowed through into an optical landscape i e or optical lattices without any induced electrical charge the cells would sort based on their intrinsic refractive index properties and can be re configurability for dynamic sorting an optical lattice can be created using diffractive optics and optical elements on the other hand k ladavac et al used a spatial light modulator to project an intensity pattern to enable the optical sorting process k xiao and d g grier applied holographic video microscopy to demonstrate that this technique can sort colloidal spheres with part per thousand resolution for size and refractive index the main mechanism for sorting is the arrangement of the optical lattice points as the cell flow through the optical lattice there are forces due to the particles drag force that is competing directly with the optical gradient force see physics of optical tweezers from the optical lattice point by shifting the arrangement of the optical lattice point there is a preferred optical path where the optical forces are dominant and biased with the aid of the flow of the cells there is a resultant force that is directed along that preferred optical path hence there is a relationship of the flow rate with the optical gradient force by adjusting the two forces one will be able to obtain a good optical sorting efficiency competition of the forces in the sorting environment need fine tuning to succeed in high efficient optical sorting the need is mainly with regards to the balance of the forces drag force due to fluid flow and optical gradient force due to arrangement of intensity spot scientists at the university of st andrews have received considerable funding from the uk engineering and physical sciences research council epsrc for an optical sorting machine this new technology could rival the conventional fluorescence activated cell sorting optical tweezers based on evanescent fields an evanescent field is a residue optical field that leaks during total internal reflection this leaking of light fades off at an exponential rate the evanescent field has found a number of applications in nanometer resolution imaging microscopy optical micromanipulation optical tweezers are becoming ever more relevant in research in optical tweezers a continuous evanescent field can be created when light is propagating through an optical waveguide multiple total internal reflection the resulting evanescent field has a directional sense and will propel microparticles along its propagating path this work was first pioneered by s kawata and t sugiura in who showed that the field can be coupled to the particles in proximity on the order of nanometers this direct coupling of the field is treated as a type of photon tunnelling across the gap from prism to microparticles the result is a directional optical propelling force a recent updated version of the evanescent field optical tweezers makes use of extended optical landscape patterns to simultaneously guide a large number of particles into a preferred direction without using a waveguide it is termed as lensless optical trapping the orderly movement of the particles is aided by the introduction of ronchi ruling that creates well defined optical potential wells replacing the waveguide this means that particles are propelled by the evanescent field while being trapped by the linear bright fringes at the moment there are scientists working on focused evanescent fields as well another approach that has been recently proposed makes use of surface plasmons which is an enhanced evanescent wave localized at a metal dielectric interface the enhanced force field experienced by colloidal particles exposed to surface plasmons at a flat metal dielectric interface has been for the first time measured using a photonic force microscope the total force magnitude being found times stronger compared to a normal evanescent wave by patterning the surface with gold microscopic islands it is possible to have selective and parallel trapping in these islands the forces of the latter optical tweezers lie in the femtonewton range optical tweezers an indirect approach ming wu a uc berkeley professor of electrical engineering and computer sciences invented the new optoelectronic tweezers wu transformed the optical energy from low powered light emitting diodes led into electrical energy via a photoconductive surface the idea is to allow the led to switch on and off the photoconductive material via its fine projection as the optical pattern can be easily transformable through optical projection this method allows a high flexibility of switching different optical landscapes the manipulation tweezing process is done by the variations between the electric field actuated by the light pattern the particles will be either attracted or repelled from the actuated point due to the its induced electrical dipole particles suspended in a liquid will be susceptible to the electrical field gradient this is known as dielectrophoresis one clear advantage is that the electrical conductivity is different between different kinds of cells living cells have a lower conductive medium while the dead ones have minimum or no conductive medium the system may be able to manipulate roughly cells or particles at the same time see comments by professor kishan dholakia on this new technique k dholakia nature materials aug news and views the system was able to move live e coli bacteria and micrometre wide particles using an optical power output of less than microwatts this is one hundred thousandth of the power needed for optical tweezers optical binding when a cluster of microparticles are trapped within a monochromatic laser beam the organization of the microparticles within the optical trapping is heavily dependent on the redistributing of the optical trapping forces amongst the microparticles this redistribution of light forces amongst the cluster of microparticles provides a new force equilibrium on the cluster as a whole as such we can say that the cluster of microparticles are somewhat bound together by light one of the first evidence of optical binding was reported by michael m burns jean marc fournier and jene a golovchenko 
fluorescence recovery after photobleaching frap denotes an optical technique capable of quantifying the two dimensional lateral diffusion of a molecularly thin film containing fluorescently labeled probes or to examine single cells this technique is very useful in biological studies of cell membrane diffusion and protein binding in addition surface deposition of a fluorescing phospholipid bilayer or monolayer allows the characterization of hydrophilic or hydrophobic surfaces in terms of surface structure and free energy similar though less well known techniques have been developed to investigate the dimensional diffusion and binding of molecules inside the cell they are also referred to as frap experimental setup where w is the radius of the beam and is the time required for the bleach spot to recover half of its initial integrated intensity applications supported lipid bilayers originally the frap technique was intended for use as a mean to characterize the mobility of individual lipid molecules within a cell membrane while providing great utility in this role current research leans more toward investigation of artificial lipid membranes supported by hydrophilic or hydrophobic substrates to produce lipid bilayers or monolayers respectively and incorporating membrane proteins these biomimetic structures are potentially useful as analytical devices for determining the identity of unknown substances understanding cellular transduction and identifying ligand binding sites protein binding this technique is commonly used in conjunction with green fluorescent protein gfp fusion proteins where the studied protein is fused to a gfp when excited by a specific wavelength of light the protein will fluoresce when the protein that is being studied is produced with the gfp then the fluorescence can be tracked photodestroying the gfp and then watching the repopulation into the bleached area can reveal information about protein interaction partners organelle continuity and protein trafficking if after some time the fluorescence doesn t reach the initial level anymore then some part of the fluorescence is caused by an immobile fraction that cannot be replenished by diffusion similarly if the fluorescent proteins bind to static cell receptors the rate of recovery will be retarded by a factor related to the association and disassociation coefficients of binding this observation has most recently been exploited to investigate protein binding applications outside the membrane frap can also be used to monitor proteins outside the membrane after the protein of interest is made fluorescent generally by expression as a gfp fusion protein a confocal microscope is used to photobleach and monitor a region of the cytoplasm mitotic spindle nucleus or another cellular structure the mean fluorescence in the region can then be plotted versus time since the photobleaching and the resulting curve can yield kinetic coefficients such as those for the protein s binding reactions and or the protein s diffusion coefficient in the medium where it is being monitored often the only dynamics considered are diffusion and binding unbinding interactions however in principle proteins can also move via flow i e undergo directed motion and this was recognized very early by axelrod et al this could be due to flow of the cytoplasm or nucleoplasm or transport along filaments in the cell such as microtubules by molecular motors the analysis is most simple when the fluorescence recovery is limited by either the rate of diffusion into the bleached area or by rate at which bleached proteins unbind from their binding sites within the bleached area and are replaced by fluorescent protein let us look at these two limits for the common case of bleaching a gfp fusion protein in a living cell diffusion limited fluorescence recovery for a circular bleach spot of radius w and diffusion dominated recovery the fluorescence is described by an equation derived by soumpasis which involves modified bessel functions formula and formula with formula the characteristic timescale for diffusion and t is the time f t is the normalized fluorescence goes to as t goes to infinity the diffusion timescale for a bleached spot of radius w is formula with d the diffusion coefficient note that this is for an instantaneous bleach with a step function profile i e the fraction of protein assumed to be bleached instantaneously at time t formula is formula for r the distance from the centre of the bleached area it is also assumes that the recovery can be modelled by diffusion in two dimensions that is also both uniform and isotropic in other words that diffusion is occurring in a uniform medium so the effective diffusion constant d is the same everywhere and that the diffusion is isotropic i e occurs at the same rate along all axes in the plane in practice in a cell none of these assumptions will be strictly true thus the equation of soumpasis is just a useful approximation that can be used when the assumptions listed above are good approximations to the true situation and when the recovery of fluorescence is indeed limited by the timescale of diffusion formula note that just because the soumpasis can be fitted adequately to data does not necessarily imply that the assumptions are true and that diffusion dominates recovery reaction limited recovery the equation describing the fluorescence as a function of time is particularly simple in another limit if a large number of proteins bind to sites in a small volume such that there the fluorescence signal is dominated by the signal from bound proteins and if this binding is all in a single state with an off rate koff then the fluorescence as a function of time is given by note that the recovery depends on the rate constant for unbinding off only it does not depend on the on rate for binding although it does depend on a number of assumptions if all these assumptions are satisfied then fitting an exponential to the recovery curve will give the off rate constant off however other dynamics can give recovery curves similar to exponentials so fitting an exponential does not necessarily imply that recovery is dominated by a simple bimolecular reaction one way to distinguish between recovery with a rate determined by unbinding and recovery that is limited by diffusion is to note that the recovery rate for unbinding limited recovery is independent of the size of the bleached area r while it scales as formula for diffusion limited recovery thus if a small and a large area are bleached if recovery is limited by unbinding then the recovery rates will be the same for the two sizes of bleached area whereas if recovery is limited by diffusion then it will be much slower for the larger bleached area diffusion and reaction in general the recovery of fluorescence will not be dominated by either simple isotropic diffusion or by a single simple unbinding rate there will be both diffusion and binding and indeed the diffusion constant may not be uniform in space and there may be more than one type of binding sites and these sites may also have a non uniform distribution in space flow processes may also be important this more complex behavior implies that a model with several parameters is required to describe the data models with only either a single diffusion constant d or a single off rate constant off are inadequate there are models with both diffusion and reaction unfortunately a single frap curve may provide insufficient evidence to reliably and uniquely fit possibly noisy experimental data sadegh zadeh et al have shown that frap curves can be fitted by different pairs of values of the diffusion constant and the on rate constant or in other words that fits to the frap are not unique this is in three parameter on rate constant off rate constant and diffusion constant fits fits that are not unique are not generally useful thus for models with a number of parameters a single frap experiment may be insufficient to estimate all the model parameters then more data is required e g by bleaching areas of different sizes determining some model parameters independently etc 
wave mechanics quasinormal modes qnm are the modes of energy dissipation of a perturbed object or field i e they describe perturbations of a field that decay in time a familiar example is the perturbation gentle tap of a wine glass with a knife the glass begins to ring it rings with a set or superposition of its natural frequencies its modes of sonic energy dissipation one could call these modes normal if the glass went on ringing forever here the amplitude of oscillation decays in time so we call its modes quasi normal to a very high degree of accuracy quasinormal ringing can be approximated by where formula is the amplitude of oscillation formula is the frequency and formula is the decay rate the quasinormal frequency is described by two numbers or more compactly where formula stands for the real part here formula is what is commonly referred to as the quasinormal mode frequency it is a complex number with two pieces of information real part is the temporal oscillation imaginary part is the temporal exponential decay in certain cases the amplitude of the wave decays quickly to follow the decay for a longer time one may plot formula mathematical physics in theoretical physics a quasinormal mode is a formal solution of linearized differential equations such as the linearized equations of general relativity constraining perturbations around a black hole solution with a complex eigenvalue frequency black holes have many quasinormal modes also ringing modes that describe the exponential decrease of asymmetry of the black hole in time as it evolves towards the perfect spherical shape recently the properties of quasinormal modes have been tested in the context of the ads cft correspondence also the asymptotic behavior of quasinormal modes was proposed to be related to the immirzi parameter in loop quantum gravity but convincing arguments have not been found yet biophysics in computational biophysics quasinormal modes also called quasiharmonic modes are derived from diagonalizing the matrix of equal time correlations of atomic fluctuations references quasinormal modes in the context of the ads cft correspondence 
in biophysics transduction is the conveyance of energy from one electron a donor to another a receptor at the same time that the class of energy changes in photosynthesis when the electrons of the chlorophyll pair receive the photon energy from the collecting associated pigments the photonic energy is destined to link one molecule of phosphate to one of nad the resulting nadp in turn will use the stored energy in the generation of atp which is the end point of the light induced photosynthetic process is quite wrong nadp is reduced electrons are added to nadph in the light reactions of linear electron transport no atp involved directly phosphate is linked to adp forming atp but this does not involve nadp h it involves the proton gradient and atp synthase generated by photosynthetic electron transport processes this means that the photon s energy ends up its circuit by being transduced to an electron that takes part in the formation of a molecular link of energy rich phosphate in the pathway of this end point transduction the energy is transferred along a number of molecules cytochromes in a downward way so that energy is partially dissipated at each step the liberated heat energy serves the homeostasis of the plant and at the end of the chain the remaining energy is perhaps exactly the one that is needed to build nadp this process is committed i e there is no return path homeostasis theoretically might save the day only at the beginning before the luminic energy transferred to the chlorophyl pair is conveyed to the first element of the cytochrome chain there is a gap in the process when the energy is carried as a series of excitons these are now called resonant energy transferring molecules of the chlorophyll class which transfer what is considered electromagnetic energy from one to its neighbor with no participation of electrons nor enzymes at this stage if the first pigment has received an excess of light the exciton perhaps might dissipate the energy as heat 
stochastic resonance sr is a phenomenon that occurs in a threshold measurement system e g a man made instrument or device a natural cell organ or organism when an appropriate measure of information transfer signal to noise ratio mutual information coherence d etc is maximized in the presence of a specific non zero level of stochastic input noise thereby lowering the response threshold the system resonates at a particular noise level definition stochastic resonance is observed when noise added to a system changes the system s behaviour in some fashion more technically sr occurs if the signal to noise ratio of a nonlinear system or device increases for moderate values of noise intensity it often occurs in bistable systems or in systems with a sensory threshold and when the input signal to the system is sub threshold for lower noise intensities the signal does not cause the device to cross threshold so little signal is passed through it for large noise intensities the output is dominated by the noise also leading to a low signal to noise ratio for moderate intensities the noise allows the signal to reach threshold but the noise intensity is not so large as to swamp it thus a plot of signal to noise ratio as a function of noise intensity shows a shape strictly speaking stochastic resonance occurs in bistable systems when a small periodic sinusoidal force is applied together with a large wide band stochastic force noise the system response is driven by the combination of the two forces that compete cooperate to make the system switch between the two stable states the degree of order is related to the amount of periodic function that it shows in the system response when the periodic force is chosen small enough in order to not make the system response switch the presence of a non negligible noise is required for it to happen when the noise is small very few switches occur mainly at random with no significant periodicity in the system response when the noise is very strong a large number of switches occur for each period of the sinusoid and the system response does not show remarkable periodicity between these two conditions there exists an optimal value of the noise that cooperatively concurs with the periodic forcing in order to make almost exactly one switch per period a maximum in the signal to noise ratio such a favorable condition is quantitatively determined by the matching of two time scales the period of the sinusoid the deterministic time scale and the kramers rate i e the inverse of the average switch rate induced by the sole noise the stochastic time scale thus the term stochastic resonance stochastic resonance was discovered and proposed for the first time in to explain the periodic recurrence of ice ages since then the same principle has been applied in a wide variety of systems nowadays stochastic resonance is commonly invoked when noise and nonlinearity concur to determine an increase of order in the system response suprathreshold stochastic resonance suprathreshold stochastic resonance is a particular form of stochastic resonance it is the phenomenon where random fluctuations or noise provide a signal processing benefit in a nonlinear system unlike most of the nonlinear systems where stochastic resonance occurs suprathreshold stochastic resonance occurs not only when the strength of the fluctuations is small relative to that of an input signal but occurs even for the smallest amount of random noise furthermore it is not restricted to a subthreshold signal hence the qualifier suprathreshold in suprathreshold stochastic resonance neuroscience psychology and biology stochastic resonance has been observed in the neural tissue of the sensory systems of several organisms computationally neurons exhibit sr because of non linearities in their processing sr has yet to be fully explained in biological systems but neural synchrony in the brain specifically in the gamma wave frequency has been suggested as a possible neural mechanism for sr by researchers who have investigated the perception of subconscious visual sensation medicine sr based techniques has been used to create a novel class of medical devices such as vibrating insoles for enhancing sensory and motor function in the elderly patients with diabetic neuropathy and patients with stroke see the review of modern physics article for a comprehensive overview of stochastic resonance signal analysis a related phenomenon is dithering applied to analog signals before analog to digital conversion stochastic resonance can be used to measure transmittance amplitudes below an instrument s detection limit if gaussian noise is added to a subthreshold i e immeasurable signal then it can be brought into a detectable region after detection the noise is removed a fourfold improvement in the detection limit can be obtained 
dna binding proteins are proteins that are composed of dna binding domains and thus have a specific or general affinity for either single or double stranded dna sequence specific dna binding proteins generally interact with the major groove of b dna because it exposes more functional groups that identify a base pair however there are some known minor groove dna binding ligands such as netropsin distamycin hoechst pentamidine and others examples dna binding proteins include transcription factors which modulate the process of transcription various polymerases nucleases which cleave dna molecules and histones which are involved in chromosome packaging and transcription in the cell nucleus dna binding proteins can incorporate such domains as the zinc finger the helix turn helix and the leucine zipper among many others that facilitate binding to nucleic acid non specific dna protein interactions structural proteins that bind dna are well understood examples of non specific dna protein interactions within chromosomes dna is held in complexes with structural proteins these proteins organize the dna into a compact structure called chromatin in eukaryotes this structure involves dna binding to a complex of small basic proteins called histones while in prokaryotes multiple types of proteins are involved the histones form a disk shaped complex called a nucleosome which contains two complete turns of double stranded dna wrapped around its surface these non specific interactions are formed through basic residues in the histones making ionic bonds to the acidic sugar phosphate backbone of the dna and are therefore largely independent of the base sequence chemical modifications of these basic amino acid residues include methylation phosphorylation and acetylation these chemical changes alter the strength of the interaction between the dna and the histones making the dna more or less accessible to transcription factors and changing the rate of transcription other non specific dna binding proteins in chromatin include the high mobility group proteins which bind to bent or distorted dna these proteins are important in bending arrays of nucleosomes and arranging them into the larger structures that make up chromosomes dna binding proteins that specifically bind single stranded dna a distinct group of dna binding proteins are the dna binding proteins that specifically bind single stranded dna in humans replication protein a is the best understood member of this family and is used in processes where the double helix is separated including dna replication recombination and dna repair these binding proteins seem to stabilize single stranded dna and protect it from forming stem loops or being degraded by nucleases binding to particular dna sequences in contrast other proteins have evolved to bind to particular dna sequences the most intensively studied of these are the various transcription factors which are proteins that regulate transcription each transcription factor binds to one particular set of dna sequences and activates or inhibits the transcription of genes that have these sequences close to their promoters the transcription factors do this in two ways firstly they can bind the rna polymerase responsible for transcription either directly or through other mediator proteins this locates the polymerase at the promoter and allows it to begin transcription alternatively transcription factors can bind enzymes that modify the histones at the promoter this will change the accessibility of the dna template to the polymerase as these dna targets can occur throughout an organism s genome changes in the activity of one type of transcription factor can affect thousands of genes consequently these proteins are often the targets of the signal transduction processes that control responses to environmental changes or cellular differentiation and development the specificity of these transcription factors interactions with dna come from the proteins making multiple contacts to the edges of the dna bases allowing them to read the dna sequence most of these base interactions are made in the major groove where the bases are most accessible mathematical descriptions of protein dna binding taking into account sequence specificity competitive and cooperative binding of proteins of different types are usually performed with the help of the lattice models 
specific absorption rate sar is a measure of the rate at which energy is absorbed by the body when exposed to a radio frequency rf electromagnetic field although it can also refer to absorption of other forms of energy by tissue including ultrasound it is defined as the power absorbed per mass of tissue and has units of watts per kilogram w kg sar is usually averaged either over the whole body or over a small sample volume typically or of tissue the value cited is then the maximum level measured in the body part studied over the stated volume or mass calculation sar can be calculated from the electric field within the tissue as where a local sar is determined over a mass of normal operating mode partial body sar kg kg exposed patient mass patient mass first level controlled operating mode partial body sar kg kg exposed patient mass patient mass c in cases where the orbit is in the field of a small local rf transmit coil care should be taken to ensure that the temperature rise is limited to in comparison to the short term relatively intensive exposures described above for long term environmental exposure of the general public there is a limit of kg averaged over the whole body 
fluorescence resonance energy transfer fret resonance energy transfer ret or electronic energy transfer eet is a mechanism describing energy transfer between two chromophores a donor chromophore initially in its electronic excited state may transfer energy to an acceptor chromophore in proximity typically less than through nonradiative coupling this mechanism is termed resonance energy transfer and is named after the german scientist theodor when both chromophores are fluorescent the term fluorescence resonance energy transfer is often used instead although the energy is not actually transferred by fluorescence in order to avoid an erroneous interpretation of the phenomenon that is always a nonradiative transfer of energy even when occurring between two fluorescent chromophores the name resonance energy transfer is preferred to fluorescence resonance energy transfer however the latter enjoys common usage in scientific literature fret is analogous to near field communication in that the radius of interaction is much smaller than the wavelength of light emitted in the near field region the excited chromophore emits a virtual photon that is instantly absorbed by a receiving chromophore these virtual photons are undetectable since their existence violates the conservation of energy and momentum and hence fret is known as a radiationless mechanism quantum electrodynamical calculations have been used to determine that radiationless fret and radiative energy transfer are the short and long range asymptotes of a single unified mechanism theoretical basis where formula is the rate of energy transfer formula the radiative decay rate and the formula are the rate constants of any other de excitation pathway with formula being the distance of this pair of donor and acceptor i e the distance at which the energy transfer efficiency is the distance depends on the overlap integral of the donor emission spectrum with the acceptor absorption spectrum and their mutual molecular orientation as expressed by the following equation p eq note a factor has appeared by mistake in literature which for example is referred wrong in the book p eq see this page discussion tab where formula is the fluorescence quantum yield of the donor in the absence of the acceptor is the dipole orientation factor formula is the refractive index of the medium formula is avogadro s number and formula is the spectral overlap integral calculated as where formula is the normalized donor emission spectrum and formula is the acceptor molar extinction coefficient is often assumed this value is obtained when both dyes are freely rotating and can be considered to be isotropically oriented during the excited state lifetime if either dye is fixed or not free to rotate then will not be a valid assumption in most cases however even modest reorientation of the dyes results in enough orientational averaging that does not result in a large error in the estimated energy transfer distance due to the sixth power dependence of r on even when is quite different from the error can be associated with a shift in r and thus determinations of changes in relative distance for a particular system are still valid fluorescent proteins do not reorient on a timescale that is faster than their fluorescence lifetime in this case where formula and formula are the donor fluorescence lifetimes in the presence and absence of an acceptor respectively or as where formula and formula are the donor fluorescence intensities with and without an acceptor respectively experimental confirmation of the resonance energy transfer theory the inverse sixth power distance dependence of resonance energy transfer was experimentally confirmed by stryer and haugland using a donor and an acceptor separated on an oligoproline helix haugland yguerabide and stryer also experimentally demonstrated the theoretical dependence of resonance energy transfer on the overlap integral by using a fused indolosteroid as a donor and a ketone as an acceptor methods in fluorescence microscopy fluorescence confocal laser scanning microscopy as well as in molecular biology fret is a useful tool to quantify molecular dynamics in biophysics and biochemistry such as protein protein interactions interactions and protein conformational changes for monitoring the complex formation between two molecules one of them is labeled with a donor and the other with an acceptor and these fluorophore labeled molecules are mixed when they are dissociated the donor emission is detected upon the donor excitation on the other hand when the donor and acceptor are in proximity due to the interaction of the two molecules the acceptor emission is predominantly observed because of the intermolecular fret from the donor to the acceptor for monitoring protein conformational changes the target protein is labeled with a donor and an acceptor at two loci when a twist or bend of the protein brings the change in the distance or relative orientation of the donor and acceptor fret change is observed if a molecular interaction or a protein conformational change is dependent on ligand binding this fret technique is applicable to fluorescent indicators for the ligand detection fret studies are scalable the extent of energy transfer is often quantified from the milliliter scale of cuvette based experiments to the femtoliter scale of microscopy based experiments this quantification can be based directly sensitized emission method on detecting two emission channels under two different excitation conditions primarily donor and primarily acceptor however for robustness reasons fret quantification is most often based on measuring changes in fluorescence intensity or fluorescence lifetime upon changing the experimental conditions e g a microscope image of donor emission is taken with the acceptor being present the acceptor is then bleached such that it is incapable of accepting energy transfer and another donor emission image is acquired a pixel based quantification using the second equation in the theory section above is then possible an alternative way of temporarily deactivating the acceptor is based on its fluorescence saturation exploiting polarisation characteristics of light a fret quantification is also possible with only a single camera exposure cfp yfp pairs the most popular fret pair for biological use is a cyan fluorescent protein cfp yellow fluorescent protein yfp pair both are color variants of green fluorescent protein gfp while labeling with organic fluorescent dyes requires troublesome processes of purification chemical modification and intracellular injection of a host protein gfp variants can be easily attached to a host protein by genetic engineering by virtue of gfp variants the use of fret techniques for biological research is becoming more and more popular bret a limitation of fret is the requirement for external illumination to initiate the fluorescence transfer which can lead to background noise in the results from direct excitation of the acceptor or to photobleaching to avoid this drawback bioluminescence resonance energy transfer or bret has been developed this technique uses a bioluminescent luciferase typically the luciferase from renilla reniformis rather than cfp to produce an initial photon emission compatible with yfp fret and bret are also the common tools in the study of biochemical reaction kinetics and molecular motors photobleaching fret fret efficiencies can also be inferred from the photobleaching rates of the donor in the presence and absence of an acceptor this method can be performed on most fluorescence microscopes one simply shines the excitation light of a frequency that will excite the donor but not the acceptor significantly on specimens with and without the acceptor fluorophore and monitors the donor fluorescence typically separated from acceptor fluorescence using a bandpass filter over time the timescale is that of photobleaching which is seconds to minutes with fluorescence in each curve being given by formula formula where formula and formula are the photobleaching decay time constants of the donor in the presence and in the absence of the acceptor respectively notice that the fraction is the reciprocal of that used for lifetime measurements this technique was introduced by jovin in its use of an entire curve of points to extract the time constants can give it accuracy advantages over the other methods also the fact that time measurements are over seconds rather than nanoseconds makes it easier than fluorescence lifetime measurements and because photobleaching decay rates do not generally depend on donor concentration unless acceptor saturation is an issue the careful control of concentrations needed for intensity measurements is not needed it is however important to keep the illumination the same for the with and without acceptor measurements as photobleaching increases markedly with more intense incident light homo fret in general fret refers to situations where the donor and acceptor proteins or fluorophores are of two different types in many biological situations however researchers might need to examine the interactions between two or more proteins of the same indeed the same protein with itself for example if the protein folds or forms part of a polymer chain of proteins or for other questions of quantification in biological cells obviously spectral differences will not be the tool used to detect and measure fret as both the acceptor and donor protein emit light with the same wavelengths yet researchers can detect differences in the polarisation between the light which excites the fluorophores and the light which is emitted in a technique called fret anisotropy imaging the level of quantified anisotropy difference in polarisation between the excitation and emission beams then becomes an indicative guide to how many fret events have happened tr fret background time resolved fluorometry trf combined with or fluorescence resonance energy transfer fret offers a powerful tool for researchers tr fret combines the low background aspect of trf with the homogeneous assay format of fret the resulting assay provides an increase in flexibility reliability and sensitivity in addition to higher throughput and fewer false positive false negative results fret involves two fluorophores a donor figure tr fret and an acceptor figure tr fret excitation of the donor by an energy source e g flash lamp or laser produces an energy transfer to the acceptor if they are within a given proximity to each other the acceptor in turn emits light at its characteristic wavelength figure tr fret because of this energy transfer interactions between biomolecules can be assessed by coupling each partner with a fluorescent label and detecting the level of energy transfer acceptor emission as a measure of energy transfer can be detected without needing to separate bound from unbound assay components e g a filtration or wash step this homogeneous format reduces both assay time and costs fret is driven by several factors including spectral overlap and the proximity of the fluorophores involved wherein energy transfer occurs only when the distance between the donor and the acceptor is small enough in practice fret systems are characterized by the radius the distance between the fluorophores at which fret efficiency is for many fret parings lies between and depending on the acceptor used and the spatial arrangements of the fluorophores within the assay advantages biological fluids or serum contain many compounds and proteins which are naturally fluorescent therefore the use of conventional steady state fluorescence presents serious limitations in assay sensitivity long lived fluorophores such as rare earth elements called lanthanides combined with time resolved detection a delay between excitation and emission detection minimizes prompt fluorescence interference time resolved fluorometry takes advantage of the unique properties of lanthanides commonly used lanthanides in assays are samarium sm europium eu terbium tb and dysprosium dy lanthanide ion complexes are well suited for this application due to their large shifts and extremely long emission lifetimes from to msec compared to more traditional fluorophores tr fret assays also offer advantages over other common fluorescence based assays such as fluorescence polarization fp assays in fp assays background fluorescence due to library compounds is normally depolarized and background signal due to scattered light e g precipitated compounds is normally polarized depending on the assay configuration either case can lead to a false positive or false negative result however because the donor species used in a tr fret assay has a fluorescent lifetime that is many orders of magnitude longer than background fluorescence or scattered light emission signal resulting from energy transfer can be measured after any interfering signal has completely decayed tr fret assays can also be formatted to use limiting receptor and excess tracer concentrations unlike fp assays which can provide reagent cost savings to the researcher other methods a different but related mechanism is dexter electron transfer an alternative method to detecting proximity is the bimolecular fluorescence complementation bifc where two halves of a yfp are fused to a protein when these two halves meet they form a fluorophore after about s hr applications fret has been used to measure distance and detect molecular interactions in a number systems and has applications in biology and chemistry fret can be used to measure distances between domains a single protein and therefore to provide information about protein confirmation fret can also detect interaction between proteins applied in vivio in living cells fret has been used to detect the location and interactions of genes and cellular structures including intergrins and membrane proteins fret can be used to obtain information about metabolic or signaling pathways fret is also used to study lipid rafts in cell membranes 
bioenergetics is the subject of a field of biochemistry that concerns energy flow through living systems this is an active area of biological research that includes the study of thousands of different cellular processes such as cellular respiration and the many other metabolic processes that can lead to production and utilization of energy in forms such as atp molecules overview bioenergetics is the part of biochemistry concerned with the energy involved in making and breaking of chemical bonds in the molecules found in biological organisms growth development and metabolism are some of the central phenomena in the study of biological organisms the role of energy is fundamental to such biological processes the ability to harness energy from a variety of metabolic pathways is a property of all living organisms life is dependent on energy transformations living organisms survive because of exchange of energy within and without in a living organism chemical bonds are broken and made as part of the exchange and transformation of energy energy is available for work such as mechanical work or for other processes such as chemical synthesis and anabolic processes in growth when weak bonds are broken and stronger bonds are made the production of stronger bonds allows release of usable energy living organisms obtain energy from organic and inorganic materials for example lithotrophs can oxidize minerals such as nitrates or forms of sulfur such as elemental sulfur sulfites and hydrogen sulfide to produce atp in photosynthesis autotrophs can produce atp using light energy heterotrophs must consume organic compounds these are mostly carbohydrates fats and proteins the amount of energy actually obtained by the organism is lower than the amount present in the food there are losses in digestion metabolism and thermogenesis the materials are generally combined with oxygen to release energy although some can also be oxidized anaerobically by various organisms the bonds holding the molecules of nutrients together and the bonds holding molecules of free oxygen together are all relatively weak compared with the chemical bonds holding carbon dioxide and water together the utilization of these materials is a form of slow combustion that is why the energy content of food can be estimated with a bomb calorimeter the materials are oxidized slowly enough that the organisms do not actually produce fire the oxidation releases energy because stronger bonds have been formed this net energy may evolve as heat or some of which may be used by the organism for other purposes such as breaking other bonds to do chemistry living organisms produce atp from energy sources via oxidative phosphorylation the terminal phosphate bonds of atp are relatively weak compared with the stronger bonds formed when atp is broken down to adenosine monophosphate and phosphate dissolved in water here it is the energy of hydration that results in energy release this hydrolysis of atp is used as a battery to store energy in cells for intermediate metabolism utilization of chemical energy from such molecular bond rearrangement powers biological processes in every biological organism types of reactions the free energy g gained or lost in a reaction can be calculated g h t s also g g rt p r where cotransport in august robert k crane presented for the first time his discovery of the sodium glucose cotransport as the mechanism for intestinal glucose absorption crane s discovery of cotransport was the first ever proposal of flux coupling in biology and was the most important event concerning carbohydrate absorption in the century chemiosmotic theory one of the major triumphs of bioenergetics is peter d mitchell s chemiosmotic theory of how protons in aqueous solution function in the production of atp in cell organelles such as mitochondria other cellular sources of atp such as glycolysis were understood first but such processes for direct coupling of enzyme activity to atp production are not the major source of useful chemical energy in most cells chemiosmotic coupling is the major energy producing process in most cells being utilized in chloroplasts and several single celled organisms in addition to mitochondria energy balance energy balance is the biological homeostasis of energy in living systems it is measured with the following equation 
magnetoception or magnetoreception as it was first referred to in is a sense which allows an animal to detect a magnetic field to perceive direction altitude or location this sense has been proposed to explain the navigational abilities of several animal species and has been postulated as a method for animals to develop regional maps for the purpose of navigation magnetoception deals with the detection of the magnetic field magnetoception has been observed in bacteria it has also been commonly hypothesized in birds where sensing of the earth s magnetic field may be important to the navigational abilities during migration fungi insects including fruit flies and honeybees and animals such as turtles lobsters sharks and stingrays proposed mechanisms an unequivocal demonstration of the use of magnetic fields for orientation within an organism has been in a class of bacteria known as magnetotactic bacteria these bacteria demonstrate a behavioural phenomenon known as magnetotaxis in which the bacteria orients itself and migrates in the direction along the earth s magnetic field lines the bacteria contain magnetosomes which are particles of magnetite or iron sulfide enclosed within the bacteria cells each bacterium cell essentially acts as a magnetic dipole they form in chains where the moments of each magnetosome align in parallel giving the bacteria its permanent magnet characteristics these chains are formed symmetrically to preserve the crystalline structure of the cells these bacteria are said to have permanent magnetic sensitivity for animals the mechanism for magnetoception is unknown but there exist two main hypotheses to explain the phenomenon according to one model cryptochrome when exposed to blue light becomes activated to form a pair of two radicals molecules with a single unpaired electron where the spins of the two unpaired electrons are correlated the surrounding magnetic field affects the kind of this correlation parallel or anti parallel and this in turn affects the length of time cryptochrome stays in its activated state activation of cryptochrome may affect the light sensitivity of retinal neurons with the overall result that the bird can see the magnetic field the magnetic field is only gauss and so it is difficult to conceive of a mechanism by which such a field could lead to any chemical changes other than those affecting the weak magnetic fields between radical pairs cryptochromes are therefore thought to be essential for the light dependent ability of the fruit fly drosophila melanogaster to sense magnetic fields the second proposed model for magnetoreception relies on also referred to as iron ii iii oxide or magnetite a natural oxide with strong magnetism iron ii iii oxide remains permanently magnetized when its length is larger than and becomes magnetized when exposed to a magnetic field if its length is less than in both of these situations the magnetic field leads to a transducible signal via a physical effect on this magnetically sensitive oxide another less general type of magnetic sensing mechanism in animals that has been thoroughly described is the inductive sensing methods used by sharks stingrays and chimaeras cartilaginous fish these species possess a unique electroreceptive organ known as ampullae of lorenzini which can detect a slight variation in electric potential these organs are made up of mucus filled canals that connect from the skin s pores to small sacs within the animal s flesh that are also filled with mucus the ampullae of lorenzini are capable of detecting dc currents and have been proposed to be used in the sensing of the weak electric fields of prey and predators the sensing method of these organs is based on faraday s law as a conductor moves through a magnetic field an electric potential is generated in this case the conductor is the animal moving through a magnetic field and the potential induced depends on the time varying rate of flux through the conductor according to v ind frac d phi dt math these organs detect very small fluctuations in the potential difference between the pore and the base of the electroreceptor sack an increase in potential results in a decrease in the rate of nerve activity and a decrease in potential results in an increase in the rate of nerve activity this is analogous to the behavior of a current carrying conductor with a fixed channel resistance an increase in potential would decrease the amount of current detected and vice versa these receptors are located along the mouth and nose of sharks and stingrays in invertebrates the mollusc tritonia diomeda has been studied for clues as to the neural mechanism behind magnetoreception in a species some of the earliest work with tritonia showed that prior to a full moon tritonia would orient their bodies between magnetic north and east a y maze was established with a right turn equal to geomagnetic south and a left turn equal to geomagnetic east within this geomagnetic field of tritonia made a turn to the left or magnetic east however when a reversed magnetic field was applied that rotated magnetic north there was no significant preference for either turn which now corresponded with magnetic north and magnetic west these results though interesting do not conclusively establish that tritonia uses magnetic fields in magnetoreception these experiments do not include a control for the activation of the coil in the reversed magnetic field experiments therefore it is possible that heat or noise generated by the coil was responsible for the loss of choice preference future work with tritonia was unable to identify any neurons that showed rapid changes in firing as a result of magnetic fields however pedal neurons two bisymmetric neurons located within the tritonia pedal ganglion exhibited gradual changes in firing over time following minutes of magnetic stimulation provided by a coil further studies showed that pedal neurons in the pedal ganglion were inhibited when exposed to magnetic fields over the course of minutes the function of both pedal neurons and pedal neurons is currently unknown drosophila melanogaster has been another invertebrate in which it has been suggested that orientation in response to magnetic fields is possible recently precise experimental techniques such as gene knockouts have allowed a closer examination of possible magnetoreception in drosophila various drosophila strains have been trained to respond to magnetic fields in a choice test flies were loaded into an apparatus with two arms that were surrounded by electric coils current was run through each of the coils but only one would a gauss magnetic field at a time the flies in this t maze were tested on their native ability to recognize the presence of the magnetic field in an arm and on their response following training where the magnetic field was paired with a sucrose reward many of the strains of flies showed a learned preference for the magnetic field following training however when the only cryptochrome found in drosophila type cry is altered either through a missense mutation or replacement of the cry gene the flies exhibit a loss of magnetosensitivity furthermore when light is filtered to only allow wavelengths greater than through drosophila loses its trained response to magnetic fields this response to filtered light is likely linked to the action spectrum of fly cryptochrome which has a range from and plateaus from although it had previously been believed that a tryptophan triad in cryptochrome was responsible for the free radicals on which magnetic fields could act recent work with drosophila has shown that tryptophan might not be behind cryptochrome dependent magnetoreception alteration of the tryptophan protein does not result in the loss of magnetosensitivity of a fly expressing either type cry or the cryptochrome found in vertebrates type cry therefore it remains unclear exactly how cryptochrome mediates magnetoreception it is also important to note that in these experiments a gauss magnetic field is used which is times the strength of the magnetic field drosophila have not yet been shown to respond to magnetic fields with the same strength as the field in homing pigeons homing pigeons have been known to use magnetic fields as part of their complex navigation system work by william keeton showed that homing pigeons that were time shifted were unable to orient themselves correctly on a clear sunny day this was considered a result of the fact that homing pigeons who used the sun for navigation would have to compensate for its movement throughout the day and a time shifted pigeon would be incapable of doing such compensation properly however if time shifted pigeons were released on overcast day they navigated correctly this led to the hypothesis that under particular conditions homing pigeons rely on magnetic fields to orient themselves further experiments with magnets attached to the backs of homing pigeons demonstrated that disruption of the ability to sense the magnetic field leads to a loss of proper orientation behavior under overcast conditions there have been two mechanisms implicated in homing pigeon magnetoreception the visually mediated free radical pair mechanism and a magnetite based directional compass or inclination compass more recent behavioral tests have shown that pigeons are able to detect magnetic anomalies of microtesla in a choice test birds were trained to jump on to a platform on one end of a tunnel if there was no magnetic field present and to jump on to a platform on the other end of the tunnel if a magnetic field was present in this test birds were rewarded with a food prize and punished with a time penalty homing pigeons were able to make the correct choice of the time which is higher than what would be expected if the pigeons were simply guessing the ability of pigeons to detect a magnetic field is impaired by application of ligocaine an anesthetic to the olfactory mucosa furthermore sectioning the trigeminal nerve leads to an inability to detect a magnetic field while sectioning of the olfactory nerve has no effect on the magnetic sense of homing pigeons these results suggest that magnetite located in the beak of pigeons may be responsible for magnetoreception via trigeminal mediation however it has not been shown that the magnetite located in the beak of pigeons is capable of responding to a magnetic field with the strength therefore the receptor responsible for magnetosensitivity in homing pigeons has not been cemented aside from the sensory receptor for magnetic reception in homing pigeons there has been work on neural regions that are possibly involved in the processing of magnetic information within the brain areas of the brain that have shown increases in activity in response to magnetic fields with a strength of or microtesla are the posterior vestibular nuclei dorsal thalamus hippocampus and visual hyperpallium as previously mentioned pigeons provided some of the first evidence for the use of magnetoreception in navigation as a result they have been an organism of focus in magnetoreception studies the precise mechanism used by pigeons has not been established and so it is as of yet unclear whether pigeons rely solely on a cryptochrome mediated receptor or on beak magnetite in mammals work with mice mole rats and bats has shown that some mammals may be capable of magnetoception when woodmice are removed from their home area and deprived of visual and olfactory cues they seem to orient themselves correctly towards their homes until an inverted magnetic field is applied to their cage when the same mice are allowed access to visual cues however they demonstrate the ability to orient themselves towards home despite the presence of inverted magnetic fields this seems to suggest that woodmice use magnetic fields to orient themselves when displaced if there are no other cues available however studies such as this have been criticized because of the difficulty of completely removing sensory cues and the fact that while some of these studies are done the magnetic field is artificially changed before the test as opposed to during the test due to the timing of the magnetic fields activation the results of these experiments do not conclusively show that woodmice respond to magnetic fields when deprived of other cues work with the zambian mole rat a subterranean mammal has led to reports that they use magnetic fields as a polarity compass to aid in the orientation of their nests in contrast to work with woodmice zambian mole rats do not exhibit different orientation behavior when a visual cue such as the sun is present a result that has been suggested is due to their subterranean lifestyle further investigation of mole rat magnetoreception lead to the finding that exposure to magnetic fields leads to an increase in neural activity within the superior colliculus as measured by immediate early gene expression the activity level of neurons within two levels of the superior colliculus the outer sublayer of the intermediate gray layer and the deep gray layer were elevated in a non specific manner when exposed to various magnetic fields however within the inner sublayer of the intermediate gray layer ingi there were two or three clusters of responsive cells the more time the mole rats were exposed to a magnetic field the greater the immediate early gene expression within the ingi however if zambian mole rats were placed in a field with a shielded magnetic field only a few scattered cells were active therefore it has been proposed that in mammals the superior colliculus is an important neural structure in the processing of magnetic information bats also seem to utilize magnetic fields in orienting themselves while bats have been known to utilize echolocation to undergo navigation over short distances it is unclear what they use to navigate over longer distances when eptisecus fuscus are taken from their home roosts and exposed to magnetic fields degreees clockwise or counterclockwise of magnetic north they are disoriented and set off for their homes in the wrong direction therefore it seems that eptisecus fuscus is capable of magnetosensation however the exact use of magnetic fields in eptisecus fuscus is unclear as it is possible that the magnetic field is used as a map compass or compass calibrator recent work with another bat species myotis myotis has lent credence to the idea that bats use magnetic fields as a compass calibrator and their primary compass is the sun in humans magnetic bones have been found in the human nose specifically the sphenoidal ethmoid sinuses beginning in the late the group of robin baker at the university of manchester began to conduct experiments that purported to exhibit magnetoception in humans people were disoriented and then asked about certain directions their answers were more accurate if there was no magnet attached to their head other scientists have maintained they could not reproduce these results a study found some other evidence for human magnetoception has been put forward low frequency magnetic fields can produce an evoked response in the brains of human subjects magnetoception in humans has also been achieved by magnetic implants and by non permanently attached artificial sensory organs however these exercises do little to demonstrate that humans are innately capable of magnetoreception additionally a magnetosensitive protein cryptochrome has been found in the human eye given the lack of knowledge as to how cryptochrome mediates magnetosensitivity in drosophila it is unclear whether the cryptochrome found in humans functions in the same way and can be used for magnetoception issues clearly the largest issue affecting verification of an animal magnetic sense is that despite more than years of work on magnetoreception there has yet to be an identification of a sensory receptor in various organisms a cryptochrome mediated receptor has been implicated in magnetoreception at the same time a magnetite system has been found to be relevant to magnetosensation in birds furthermore it is possible that both of these mechanisms play a role in magnetic field detection in animals this dual mechanism theory has been proposed in birds and the question that arises if such a mechanism is actually responsible for magnetoception is to what degree each method is responsible for stimuli transduction regardless of the importance of either of these mechanisms an issue they must both address is how they lead to a tranducible signal given a magnetic field with the strength the precise purpose of magnetoreception in animal navigation is also unclear there is evidence that some animals may be using their magnetic sense as either a map compass or compass calibrator for example birds such as the homing pigeon are believed to use the magnetite in their beaks to detect magnetic signposts and thus the magnetic sense they gain from this pathway is a possible map yet it has also been suggested that homing pigeons and other birds use the visually mediated cryptochrome receptor as a compass the purpose of magnetoreception in birds and other animals may be varied but work to conclusively establish which aspect of navigation in particular such a sense is used for is often difficult to conduct there are also a number of experimental concerns that are difficult to disentangle from the results of much of the research an example of this difficulty has been reflected in the fact that numerous studies use magnetic fields that are larger than the field as a result of the lack of studies using ambient magnetic fields with the same strength as those of the earth the relevance of magnetoreception for animals has not been conclusively proven furthermore in many studies such as those done with tritonia electrophysiological recordings have been done with only one or two neurons the scale of these experiments is not particularly large in addition many of the studies on magnetoreception have been solely correlational future directions the discovery of a sensory receptor for magnetoreception is clearly the foremost issue in this field at the moment the establishment of drosophila as an organism of interest has allowed for genetic techniques to be used that have already led to serious results on magnetoreception as more manipulative studies involving nerve lesioning and genetic manipulation occur serious advances in our knowledge of magnetoreception should occur due to the length of time that has elapsed since magnetoreception was first suggested and today clarification of its purpose and mechanism in animals will probably require multidimensional analysis integrating the work of neurologists ethologists geneticists and physicists among other scientists 
opticution is the term given to the damaging beyond viability of biological samples that are manipulated via optical traps most commonly optical tweezers the term was coined by arthur ashkin who developed optical tweezers at bell labs in the and as different biological samples absorb different wavelengths choosing the proper laser frequency can help to minimize the danger of opticution near infrared diode lasers are a popular choice because of this and their relative inexpensiveness recent work by neumann characterization of photodamage to escherichia coli in optical traps biophysical journal nov has shown that for continuous lasers such as those employed in optical traps the damage seems to be a single photon process as opposed to the two photon process found in pulsed lasers furthermore the damage is related to the presence of oxygen and use of an oxygen scrubbing system can reduce optical damage indefinitely 
surface plasmon resonance spr can be described as the resonant collective oscillation of valence electrons in a solid stimulated by incident light the resonance condition is established when the frequency of light photons matches the natural frequency of surface electrons oscillating against the restoring force of positive nuclei spr in nanometer sized structures is called localized surface plasmon resonance spr is the basis of many standard tools for measuring adsorption of material onto planar metal typically gold and silver surfaces or onto the surface of metal nanoparticles it is the fundamental principle behind many color based biosensor applications and different lab on a chip sensors explanation surface plasmons also known as surface plasmon polaritons are surface electromagnetic waves that propagate in a direction parallel to the metal dielectric or metal vacuum interface since the wave is on the boundary of the metal and the external medium air or water for example these oscillations are very sensitive to any change of this boundary such as the adsorption of molecules to the metal surface to describe the existence and properties of surface plasmons one can choose from various models quantum theory drude model etc the simplest way to approach the problem is to treat each material as a homogeneous continuum described by a frequency dependent relative permittivity between the external medium and the surface this quantity hereafter referred to as the materials dielectric constant is complex permittivity in order for the terms which describe the electronic surface plasmons to exist the real part of the dielectric constant of the metal must be negative and its magnitude must be greater than that of the dielectric this condition is met in the ir visible wavelength region for air metal and water metal interfaces where the real dielectric constant of a metal is negative and that of air or water is positive lsprs localized sprs are collective electron charge oscillations in metallic nanoparticles that are excited by light they exhibit enhanced near field amplitude at the resonance wavelength this field is highly localized at the nanoparticle and decays rapidly away from the nanoparticle dieletric interface into the dielectric background though far field scattering by the particle is also enhanced by the resonance light intensity enhancement is a very important aspect of lsprs and localization means the lspr has very high spatial resolution subwavelength limited only by the size of nanoparticles because of the enhanced field amplitude effects that depend on the amplitude such as magneto optical effect are also enhanced by lsprs realization in order to excite surface plasmons in a resonant manner one can use an electron or light beam visible and infrared are typical the incoming beam has to match its momentum to that of the plasmon in the case of p polarized light polarization occurs parallel to the plane of incidence this is possible by passing the light through a block of glass to increase the wavenumber and the momentum and achieve the resonance at a given wavelength and angle s polarized light polarization occurs perpendicular to the plane of incidence cannot excite electronic surface plasmons where formula is the dielectric constant and formula is the magnetic permeability of the material the glass block the metal film typical metals that support surface plasmons are silver and gold but metals such as copper titanium or chromium have also been used when using light to excite sp waves there are two configurations which are well known in the otto setup the light illuminates the wall of a glass block typically a prism and is totally internally reflected a thin metal film for example gold is positioned close enough to the prism wall so that an evanescent wave can interact with the plasma waves on the surface and hence excite the plasmons in the kretschmann configuration the metal film is evaporated onto the glass block the light again illuminates the glass block and an evanescent wave penetrates through the metal film the plasmons are excited at the outer side of the film this configuration is used in most practical applications spr emission when the surface plasmon wave interacts with a local particle or irregularity such as a rough surface part of the energy can be re emitted as light this emitted light can be detected behind the metal film from various directions applications surface plasmons have been used to enhance the surface sensitivity of several spectroscopic measurements including fluorescence raman scattering and second harmonic generation however in their simplest form spr reflectivity measurements can be used to detect molecular adsorption such as polymers dna or proteins etc technically it is common that the angle of the reflection minimum absorption maximum is measured this angle changes in the order of during thin about nm thickness film adsorption see also the examples in other cases the changes in the absorption wavelength is followed the mechanism of detection is based on that the adsorbing molecules cause changes in the local index of refraction changing the resonance conditions of the surface plasmon waves if the surface is patterned with different biopolymers using adequate optics and imaging sensors i e a camera the technique can be extended to surface plasmon resonance imaging spri this method provides a high contrast of the images based on the adsorbed amount of molecules somewhat similar to brewster angle microscopy this latter is most commonly used together with a langmuir blodgett trough for nanoparticles localized surface plasmon oscillations can give rise to the intense colors of suspensions or sols containing the nanoparticles nanoparticles or nanowires of noble metals exhibit strong absorption bands in the ultraviolet visible light regime that are not present in the bulk metal this extraordinary absorption increase has been exploited to increase light absorption in photovoltaic cells by depositing metal nanoparticles on the cell surface the energy color of this absorption differs when the light is polarized along or perpendicular to the nanowire shifts in this resonance due to changes in the local index of refraction upon adsorption to the nanoparticles can also be used to detect biopolymers such as dna or proteins related complementary techniques include plasmon waveguide resonance qcm extraordinary optical transmission and dual polarization interferometry data interpretation the most common data interpretation is based on the fresnel formulas which treat the formed thin films as infinite continuous dielectric layers this interpretation may result multiple possible refractive index and thickness values however usually only one solution is within the reasonable data range metal particle plasmons are usually modeled using the mie scattering theory in many cases no detailed models are applied but the sensors are calibrated for the specific application and used with interpolation within the calibration curve examples layer by layer self assembly one of the first common applications of surface plasmon resonance spectroscopy was the measurement of the thickness and refractive index of adsorbed self assembled nanofilms on gold substrates the resonance curves shift to higher angles as the thickness of the adsorbed film increases this example is a static spr measurement when higher speed observation is desired one can select an angle right below the resonance point the angle of minimum reflectance and measure the reflectivity changes at that point this is the so called dynamic spr measurement the interpretation of the data assumes that the structure of the film does not change significantly during the measurement binding constant determination when the affinity of two ligands has to be determined the binding constant must be determined it is the equilibrium value for the product quotient this value can also be found using the dynamical spr parameters and as in any chemical reaction it is the association rate divided by the dissociation rate for this a so called bait ligand is immobilized on the dextran surface of the spr crystal through a microflow system a solution with the prey analyte is injected over the bait layer as the prey analyte binds the bait ligand an increase in spr signal expressed in response units ru is observed after desired association time a solution without the prey analyte usually the buffer is injected on the microfluidics that dissociates the bound complex between bait ligand and prey analyte now as the prey analyte dissociates from the bait ligand a decrease in spr signal expressed in resonance units ru is observed from these association on rate k a and dissociation rates off rate k d the equilibrium dissociation constant binding constant k d can be calculated the actual spr signal can be explained by the electromagnetic coupling of the incident light with the surface plasmon of the gold layer this plasmon can be influenced by the layer just a few nanometer across the gold solution interface i e the bait protein and possibly the prey protein binding makes the reflection angle change formula magnetic plasmon resonance recently there has been an interest in magnetic surface plasmons these require materials with large negative magnetic permeability a property that has only recently been made available with the construction of metamaterials 
in molecular biology the term double helix refers to the structure formed by double stranded molecules of nucleic acids such as dna and rna the double helical structure of a nucleic acid complex arises as a consequence of its secondary structure and is a fundamental component in determining its tertiary structure the term entered popular culture with the publication in of the double helix a personal account of the discovery of the structure of dna by james watson the dna double helix is a spiral polymer of nucleic acids held together by nucleotides which base pair together in b dna the most common double helical structure the double helix is right handed with about nucleotides per turn the double helix structure of dna contains a major groove and minor groove the major groove being wider than the minor groove given the difference in widths of the major groove and minor groove many proteins which bind to dna do so through the wider major groove history the double helix model of dna structure was first published in the journal nature by james d watson and francis crick in x y z coordinates in based upon the crucial x ray diffraction image of dna labeled as photo from rosalind franklin in followed by her more clarified dna image with raymond gosling maurice wilkins alexander stokes and herbert wilson as well as base pairing chemical and biochemical information by erwin chargaff the previous model was triple stranded dna the realization that the structure of dna is that of a double helix elucidated the mechanism of base pairing by which genetic information is stored and copied in living organisms is widely considered one of the most important scientific discoveries of the century crick wilkins and watson each received one third of the nobel prize in physiology or medicine for their contributions to the discovery franklin whose breakthrough x ray diffraction data was used to formulate the dna structure died in and thus was ineligible to be nominated for a nobel prize nucleic acid hybridization hybridization is the process of complementary base pairs binding to form a double helix melting is the process by which the interactions between the strands of the double helix are broken separating the two nucleic acid strands these bonds are weak easily separated by gentle heating enzymes or physical force melting occurs preferentially at certain points in the nucleic acid t and a rich sequences are more easily melted than c and g rich regions particular base steps are also susceptible to dna melting particularly t a and t g base steps these mechanical features are reflected by the use of sequences such as tataa at the start of many genes to assist rna polymerase in melting the dna for transcription strand separation by gentle heating as used in pcr is simple providing the molecules have fewer than about base pairs kilobase pairs or kbp the intertwining of the dna strands makes long segments difficult to separate the cell avoids this problem by allowing its dna melting enzymes helicases to work concurrently with topoisomerases which can chemically cleave the phosphate backbone of one of the strands so that it can swivel around the other helicases unwind the strands to facilitate the advance of sequence reading enzymes such as dna polymerase base pair geometry the geometry of a base or base pair step can be characterized by coordinates shift slide rise tilt roll and twist these values precisely define the location and orientation in space of every base or base pair in a nucleic acid molecule relative to its predecessor along the axis of the helix together they characterize the helical structure of the molecule in regions of dna or rna where the normal structure is disrupted the change in these values can be used to describe such disruption rise and twist determine the handedness and pitch of the helix the other coordinates by contrast can be zero slide and shift are typically small in b dna but are substantial in a and z dna roll and tilt make successive base pairs less parallel and are typically small a diagram of these coordinates can be found in website note that tilt has often been used differently in the scientific literature referring to the deviation of the first inter strand base pair axis from perpendicularity to the helix axis this corresponds to slide between a succession of base pairs and in helix based coordinates is properly termed inclination helix geometries at least three dna conformations are believed to be found in nature a dna b dna and z dna the b form described by james d watson and francis crick is believed to predominate in cells it is wide and extends per bp of sequence the double helix makes one complete turn about its axis every base pairs in solution this frequency of twist known as the helical pitch depends largely on stacking forces that each base exerts on its neighbours in the chain a dna and z dna differ significantly in their geometry and dimensions to b dna although still form helical structures the a form appears likely to occur only in dehydrated samples of dna such as those used in crystallographic experiments and possibly in hybrid pairings of dna and rna strands segments of dna that cells have been methylated for regulatory purposes may adopt the z geometry in which the strands turn about the helical axis the opposite way to a dna and b dna there is also evidence of protein dna complexes forming z dna structures other conformations are possible a dna b dna c dna e dna l dna the enantiomeric form of d dna p dna s dna z dna etc have been described so far in fact only the letters f q u v and y are available to describe any new dna structure that may appear in the future however most of these forms have been created synthetically and have not been observed in naturally occurring biological systems there are also triple stranded dna forms and quadruplex forms such as the g quadruplex grooves twin helical strands form the dna backbone another double helix may be found by tracing the spaces or grooves between the strands these voids are adjacent to the base pairs and may provide a binding site as the strands are not directly opposite each other the grooves are unequally sized one groove the major groove is wide and the other the minor groove is wide the narrowness of the minor groove means that the edges of the bases are more accessible in the major groove as a result proteins like transcription factors that can bind to specific sequences in double stranded dna usually make contacts to the sides of the bases exposed in the major groove this situation varies in unusual conformations of dna within the cell see below but the major and minor grooves are always named to reflect the differences in size that would be seen if the dna is twisted back into the ordinary b form non double helical forms other non double helical forms of dna have been theorized for example the side by side sbs geometry where the two strands do not wind around each other side by side models of dna were proposed early in the history of molecular biology but these were set aside in favor of the double helical model due to x ray crystallography of dna duplexes and later the nucleosome core particle as well as the discovery of topoisomerases the state of current understanding in the field was aptly outlined in an exchange of correspondence in current science in single stranded nucleic acids do not adopt a helical formation and are described by models such as the random coil or worm like chain bending dna is a relatively rigid polymer typically modelled as a worm like chain it has three significant degrees of freedom bending twisting and compression each of which cause particular limitations on what is possible with dna within a cell twisting torsional stiffness is important for the circularisation of dna and the orientation of dna bound proteins relative to each other and bending axial stiffness is important for dna wrapping and circularisation and protein interactions compression extension is relatively unimportant in the absence of high tension persistence length axial stiffness this value may be directly measured using an atomic force microscope to directly image dna molecules of various lengths in an aqueous solution the average persistence length is or base pairs the diameter of dna is although can vary significantly this makes dna a moderately stiff molecule the persistence length of a section of dna is somewhat dependent on its sequence and this can cause significant variation the variation is largely due to base stacking energies and the residues which extend into the minor and major grooves models for dna bending the entropic flexibility of dna is remarkably consistent with standard polymer physics models such as the kratky porod worm like chain model consistent with the worm like chain model is the observation that bending dna is also described by hooke s law at very small sub piconewton forces however for dna segments less than the persistence length the bending force is approximately constant and behaviour deviates from the worm like chain predictions this effect results in unusual ease in circularising small dna molecules and a higher probability of finding highly bent sections of dna bending preference dna molecules often have a preferred direction to bend i e anisotropic bending this is again due to the properties of the bases which make up the dna sequence a random sequence will have no preferred bend direction i e isotropic bending preferred dna bend direction is determined by the stability of stacking each base on top of the next if unstable base stacking steps are always found on one side of the dna helix then the dna will preferentially bend away from that direction as bend angle increases then steric hindrances and ability to roll the residues relative to each other also play a role especially in the minor groove a and t residues will be preferentially be found in the minor grooves on the inside of bends this effect is particularly seen in dna protein binding where tight dna bending is induced such as in nucleosome particles see base step distortions above g a t t c c c a a a a a t g t c a a a a a a t a g g c a a a a a a t g c c a a a a a a t c c c a a a c pre the intrinsically bent structure is induced by the propeller twist of base pairs relative to each other allowing unusual bifurcated hydrogen bonds between base steps at higher temperatures this structure and so the intrinsic bend is lost all dna which bends anisotropically has on average a longer persistence length and greater axial stiffness this increased rigidity is required to prevent random bending which would make the molecule act isotropically circularization dna circularization depends on both the axial bending stiffness and torsional rotational stiffness of the molecule for a dna molecule to successfully circularize it must be long enough to easily bend into the full circle and must have the correct number of bases so the ends are in the correct rotation to allow bonding to occur the optimum length for circularization of dna is around base pairs with an integral number of turns of the dna helix i e multiples of base pairs having a non integral number of turns presents a significant energy barrier for circularization for example a x base pair molecule will circularize hundreds of times faster than x base pair molecule stretching longer stretches of dna are entropically elastic under tension when dna is in solution it undergoes continuous structural variations due to the energy available in the thermal bath of the solvent this is due to the thermal vibration of the molecule combined with continual collisions with water molecules for entropic reasons more compact relaxed states are thermally accessible than stretched out states and so dna molecules are almost universally found in a tangled relaxed layouts for this reason a single molecule of dna will stretch under a force straightening it out using optical tweezers the entropic stretching behavior of dna has been studied and analyzed from a polymer physics perspective and it has been found that dna behaves largely like the kratky porod worm like chain model under physiologically accessible energy scales under sufficient tension and positive torque dna is thought to undergo a phase transition with the bases splaying outwards and the phosphates moving to the middle this proposed structure for overstretched dna has been called p form dna in honor of linus pauling who originally presented it as a possible structure of dna the mechanical properties dna under compression have not been characterized due to experimental difficulties in preventing the polymer from bending under the compressive force supercoiling and topology the b form of the dna helix twists per bp in the absence of torsional strain but many molecular biological processes can induce torsional strain a dna segment with excess or insufficient helical twisting is referred to respectively as positively or negatively supercoiled dna in vivo is typically negatively supercoiled which facilitates the unwinding melting of the double helix required for rna transcription within the cell most dna is topologically restricted dna is typically found in closed loops such as plasmids in prokaryotes which are topologically closed or as very long molecules whose diffusion coefficients produce effectively topologically closed domains linear sections of dna are also commonly bound to proteins or physical structures such as membranes to form closed topological loops ref any change of t in a closed topological domain must be balanced by a change in w and vice versa this results in higher order structure of dna a circular dna molecule with a writhe of will be circular if the twist of this molecule is subsequently increased or decreased by supercoiling then the writhe will be appropriately altered making the molecule undergo plectonemic or toroidal superhelical coiling when the ends of a piece of double stranded helical dna are joined so that it forms a circle the strands are topologically knotted this means the single strands cannot be separated any process that does not involve breaking a strand such as heating the task of un knotting topologically linked strands of dna falls to enzymes known as topoisomerases these enzymes are dedicated to un knotting circular dna by cleaving one or both strands so that another double or single stranded segment can pass through this un knotting is required for the replication of circular dna and various types of recombination in linear dna which have similar topological constraints the linking number paradox for many years the origin of residual supercoiling in eukaryotic genomes remained unclear this topological puzzle was referred to by some as the linking number paradox however when experimentally determined structures of the nucleosome displayed an over twisted left handed wrap of dna around the histone octamer this paradox was considered to be solved by the scientific community 
interactions occur when two or more proteins bind together often to carry out their biological function many of the most important molecular processes in the cell such as dna replication are carried out by large molecular machines that are built from a large number of protein components organised by their interactions protein interactions have been studied from the perspectives of biochemistry quantum chemistry molecular dynamics chemical biology signal transduction and other metabolic or genetic epigenetic networks indeed interactions are at the core of the entire interactomics system of any living cell interactions between proteins are important for the majority of biological functions for example signals from the exterior of a cell are mediated to the inside of that cell by interactions of the signaling molecules this process called signal transduction plays a fundamental role in many biological processes and in many diseases e g cancers proteins might interact for a long time to form part of a protein complex a protein may be carrying another protein for example from cytoplasm to nucleus or vice versa in the case of the nuclear pore importins or a protein may interact briefly with another protein just to modify it for example a protein kinase will add a phosphate to a target protein this modification of proteins can itself change interactions for example some proteins with domains only bind to other proteins when they are phosphorylated on the amino acid tyrosine while bromodomains specifically recognise acetylated lysines in conclusion interactions are of central importance for virtually every process in a living cell information about these interactions improves our understanding of diseases and can provide the basis for new therapeutic approaches methods to investigate interactions as interactions are so important there are a multitude of methods to detect them each of the approaches has its own strengths and weaknesses especially with regard to the sensitivity and specificity of the method a high sensitivity means that many of the interactions that occur in reality are detected by the screen a high specificity indicates that most of the interactions detected by the screen are also occurring in reality methods such as yeast two hybrid screening can be used to detect novel interactions visualization of networks visualization of interaction networks is a popular application of scientific visualization techniques although protein interaction diagrams are common in textbooks diagrams of whole cell protein interaction networks were not as common since the level of complexity made them difficult to generate one example of a manually produced molecular interaction map is kurt kohn s map of cell cycle control drawing on kohn s map in schwikowski uetz and fields published a paper on interactions in yeast linking together interacting proteins determined by two hybrid testing they used a layered graph drawing method to find an initial placement of the nodes and then improved the layout using a force based algorithm the cytoscape software is a widely used application to visualise protein protein interaction networks database collections methods for identifying interacting proteins have defined hundreds of thousands of interactions these interactions are collected together in specialised biological databases that allow the interactions to be assembled and studied further the first of these databases was dip the database of interacting proteins since that time a large number of further database collections have been created such as biogrid and string external links 
what is life is a non fiction science book written for the lay reader by physicist erwin the book was based on a course of public lectures delivered by in february under the auspices of the dublin institute for advanced studies at trinity college dublin the lectures attracted an audience of about who were warned that the subject matter was a difficult one and that the lectures could not be termed popular even though the most dreaded weapon mathematical deduction would hardly be utilized s lecture focused on one important question how can the events in space and time which take place within the spatial boundary of a living organism be accounted for by physics and chemistry in the book introduced the idea of an aperiodic crystal that contained genetic information in its configuration of covalent chemical bonds in the this idea stimulated enthusiasm for discovering the genetic molecule although the existence of dna had been known since its role in reproduction and its helical shape were still unknown at the time of s lecture in retrospect s aperiodic crystal can be viewed as a well reasoned theoretical prediction of what biologists should have been looking for during their search for genetic material both james d watson and independently francis crick co discoverers of the structure of dna credited s book with presenting an early theoretical description of how the storage of genetic information would work and each respectively acknowledged the book as a source of inspiration for their initial researches background the book is based on lectures delivered under the auspices of the institute at trinity college dublin in february and published in at that time dna was not yet accepted as the carrier of hereditary information which only was the case after the experiment of one of the most successful branches of physics at this time was statistical physics and quantum mechanics a theory which is also very statistical in its nature himself is one of the founding fathers of quantum mechanics max s thinking about the physical basis of life was an important influence on geneticist and nobel prize winner h j muller had in his article variation due to change in the individual gene already laid out all the basic properties of the heredity molecule that derives from first principles in what is life properties which muller refined in his article the gene as the basis of life and further clarified during the long before the publication of what is life but the role of the macromolecule dna as the genetic material was not yet suspected in rather some form of protein was expected to be the genetic material at that time content in chapter i explains that most physical laws on a large scale are due to chaos on a small scale he calls this principle order from disorder as an example he mentions diffusion which can be modeled as a highly ordered process but which is caused by random movement of atoms or molecules if the number of atoms is reduced the behaviour of a system becomes more and more random he states that life greatly depends on order and that a naive physicist may assume that the master code of a living organism has to consist of a large number of atoms in chapter ii and iii he summarizes what was known at this time about the hereditary mechanism most importantly he elaborates the important role mutations play in evolution he concludes that the carrier of hereditary information has to be both small in size and permanent in time contradicting the naive physicist s expectation this contradiction cannot be resolved by classical physics in chapter iv presents molecules which are indeed stable even if they consist of only a few atoms as the solution even though molecules were known before their stability could not be explained by classical physics but is due to the discrete nature of quantum mechanics furthermore mutations are directly linked to quantum leaps he continues to explain in chapter v that true solids which are also permanent are crystals the stability of molecules and crystals is due to the same principles and a molecule might be called the germ of a solid on the other hand an amorphous solid without crystalline structure should be regarded as a liquid with a very high viscosity believes the heredity material to be a molecule which unlike a crystal does not repeat itself he calls this an aperiodic crystal the aperiodic nature allows to encode an almost infinite number of possibilities with a small number of atoms he finally compares this picture with the known facts and finds it in accordance with them living matter while not eluding the laws of physics as established up to date is likely to involve other laws of physics hitherto unknown which however once they have been revealed will form just as integral a part of science as the former he knows that this statement is open to misconception and tries to clarify it the main principle involved with order from disorder is the second law of thermodynamics according to which entropy only increases in a closed system such as the universe explains that living matter evades the decay to thermodynamical equilibrium by homeostatically maintaining negative entropy today this quantity is called information in an open system in chapter vii he maintains that order from order is not absolutely new to physics in fact it is even simpler and more plausible but nature follows order from disorder with some exceptions as the movement of the celestial bodies and the behaviour of mechanical devices such as clocks but even those are influenced by thermal and frictional forces the degree to which a system functions mechanically or statistically depends on the temperature if heated a clock ceases to function because it melts conversely if the temperature approaches absolute zero any system behaves more and more mechanically some systems approach this mechanical behaviour rather fast with room temperature already being practically equivalent to absolute zero concludes this chapter and the book with philosophical speculations on determinism free will and the mystery of human consciousness he believes he must reconcile two premises the body fully obeys the laws of quantum mechanics where quantum indeterminacy plays no important role except to increase randomness at the quantum scale and there is incontrovertible direct experience that we freely direct our bodies can predict outcomes and take responsibility for our choice of action rejects the idea that the source of consciousness should perish with the body because he finds the idea distasteful he also rejects the idea that there are multiple immortal souls that can exist without the body because he believes that consciousness is nevertheless highly dependent on the body writes that to reconcile the two premises the only possible alternative is simply to keep to the immediate experience that consciousness is a singular of which the plural is unknown that there is only one thing and that what seems to be a plurality is merely a series of different aspects of this one thing any intuitions that consciousness is plural he says are illusions is sympathetic to the hindu concept of brahman by which each individual s consciousness is only a manifestation of a unitary consciousness pervading the universe which corresponds to the hindu concept of god concludes that i am the person if any who controls the motion of the atoms according to the laws of nature however he also qualifies the conclusion as necessarily subjective in its philosophical implications in the final paragraph he points out that what is meant by i is not the collection of experienced events but namely the canvas upon which they are collected if a hypnotist succeeds in blotting out all earlier reminiscences he writes there would be no loss of personal existence nor will there ever be s paradox in a world governed by the second law of thermodynamics all closed systems are expected to approach a state of maximum disorder since life approaches and maintains a highly ordered state some argue that this seems to violate the aforementioned second law implicating a paradox however since life is not a closed system there is no paradox the increase of order inside an organism is more than paid for by an increase in disorder outside this organism by this mechanism the second law is obeyed and life maintains a highly ordered state which it sustains by causing a net increase in disorder in the universe 
electrorotation is the circular movement of an electrically polarized particle similar to the slip of an electric motor it can arise from a phase lag between an applied rotating electric field and the respective relaxation processes and may thus be used to investigate the processes or if these are known or can be accurately described by models to determine particle properties the method is popular in cellular biophysics as it allows to measure cellular properties like conductivity and permittivity of cellular compartments and their surrounding membranes 
the worm like chain wlc model in polymer physics is used to describe the behavior of semi flexible polymers it is sometimes referred to as the kratky porod model theoretical considerations the wlc model envisions an isotropic rod that is continuously flexible this is in contrast to the freely jointed chain model that is flexible only between discrete segments the worm like chain model is particularly suited for describing stiffer polymers with successive segments displaying a sort of cooperativity all pointing in roughly the same direction at room temperature the polymer adopts a conformational ensemble that is smoothly curved at formula k the polymer adopts a rigid rod conformation for a polymer of length formula parametrize the path of the polymer as formula allow formula to be the unit tangent vector to the chain at formula and formula to be the position vector along the chain then formula stretching worm like chain polymers laboratory tools such as atomic force microscopy afm and optical tweezers have been used to characterize the force dependent stretching behavior of the polymers listed above an interpolation formula that describes the extension formula of a wlc with contour length formula and persistence length formula in response to a stretching force formula is where formula is the boltzmann constant and formula is the absolute temperature bustamante et al marko et al where a typical value for the stretch modulus of double stranded dna is around pn and nm for the persistence length wang et al 
molecular motors are biological molecular machines that are the essential agents of movement in living organisms in general terms a motor may be defined as a device that consumes energy in one form and converts it into motion or mechanical work for example many protein based molecular motors harness the chemical free energy released by the hydrolysis of atp in order to perform mechanical work in terms of energetic efficiency this type of motor can be superior to currently available man made motors one important difference between molecular motors and macroscopic motors is that molecular motors operate in the thermal bath an environment in which the fluctuations due to thermal noise are significant theoretical considerations because the motor events are stochastic molecular motors are often modeled with the fokker planck equation or with monte carlo methods these theoretical models are especially useful when treating the molecular motor as a brownian motor experimental observation many more techniques are also used as new technologies and methods are developed it is expected that knowledge of naturally occurring molecular motors will be helpful in constructing synthetic nanoscale motors non biological recently chemists and those involved in nanotechnology have begun to explore the possibility of creating molecular motors de novo these synthetic molecular motors currently suffer many limitations that confine their use to the research laboratory however many of these limitations may be overcome as our understanding of chemistry and physics at the nanoscale increases systems like the nanocars while not technically motors are illustrative of recent efforts towards synthetic nanoscale motors 
mechanotransduction refers to the many mechanisms by which cells convert mechanical stimulus into chemical activity mechanotransduction is responsible for a number of senses and physiological processes in the body including proprioception touch balance and hearing the basic mechanism of mechanotransduction involves converting mechanical signals into electrical or chemical signals in this process a mechanically gated ion channel makes it possible for sound pressure or movement to cause a change in the excitability of specialized sensory cells and sensory neurons the stimulation of a mechanoreceptor causes mechanically sensitive ion channels to open and produce a transduction current that changes the membrane potential of the cell cellular responses to mechanotransduction are variable and give rise to a variety of changes and sensations ear one such mechanism is the opening of ion channels in the hair cells of the cochlea in the inner ear air pressure changes in the ear canal cause the vibrations of the tympanic membrane and middle ear ossicles at the end of the ossicular chain movement of the stapes footplate within the oval window of the cochlea in turn generates a pressure field within the cochlear fluids imparting a pressure differential across the basilar membrane a sinusoidal pressure wave results in localized vibrations of the organ of corti near the base for high frequencies near the apex for low frequencies the cochlea thus acts as an acoustic prism distributing the energy of each fourier component of a complex sound at different locations along its longitudinal axis hair cells in the cochlea are stimulated when the basilar membrane is driven up and down by differences in the fluid pressure between the scala vestibuli and scala tympani because this motion is accompanied by a shearing motion between the tectorial membrane and the reticular lamina of the organ of corti the hair bundles that link the two are deflected which initiates mechano electrical transduction when the basilar membrane is driven upward shear between the hair cells and the tectorial membrane deflects hair bundles in the excitatory direction toward their tall edge at the midpoint of an oscillation the hair bundles resume their resting position when the basilar membrane moves downward the hair bundles are driven in the inhibitory direction basilar membrane motion causes a shearing motion between the reticular lamina and the tectorial membrane thereby activating the mechano sensory apparatus of the hair bundle which in turn generates a receptor potential in the hair cells thus the sound pressure wave is transduced to an electrical signal which can be processed as sound in higher parts of the auditory system skeletal muscle when a deformation is imposed on a muscle changes in cellular and molecular conformations link the mechanical forces with biochemical signals and the close integration of mechanical signals with electrical metabolic and hormonal signaling may disguise the aspect of the response that is specific to the mechanical forces cartilage one of the main mechanical functions of articular cartilage is to act as a low friction load bearing surface due to its unique location at joint surfaces articular cartilage experiences a range of static and dynamic forces that include shear compression and tension these mechanical loads are absorbed by the cartilage extracellular matrix ecm where they are subsequently dissipated and transmitted to chondrocytes cartilage cells chondrocytes sense and convert the mechanical signals they receive into biochemical signals which subsequently direct and mediate both anabolic matrix building and catabolic matrix degrading processes these processes include the synthesis of matrix proteins type ii collagen and proteoglycans proteases protease inhibitors transcription factors cytokines and growth factors the balance that is struck between anabolic and catabolic processes is strongly influenced by the type of loading that cartilage experiences high strain rates such as which occurs during impact loading cause tissue damage degradation decreased matrix production and apoptosis decreased mechanical loading over long periods such as during extended bed rest causes a loss of matrix production static loads have been shown to be detrimental to biosynthesis while oscillatory loads at low frequencies similar that of a normal walking gait have been shown to be beneficial in maintaining health and increasing matrix synthesis due to the complexity of in vivo loading conditions and the interplay of other mechanical and biochemical factors the question of what an optimal loading regimen may be or whether one exists remain unanswered although studies have shown that like most biological tissues cartilage is capable of mechanotransduction the precise mechanisms by which this is done remain unknown however there exist a few hypotheses which begin with the identification of mechanoreceptors in order for mechanical signals to be sensed there need to be mechanoreceptors on the surface of chondrocytes candidates for chondrocyte mechanoreceptors include stretch activated ion channels sac the hyaluronan receptor annexin v a collagen type ii receptor and integrin receptors of which there exist several types on chondrocytes using the integrin linked mechanotransduction pathway as an example being one of the better studied pathways it has been shown to mediate chondrocyte adhesion to cartilage surfaces mediate survival signaling and regulate matrix production and degradation integrin receptors have an extracellular domain that binds to the ecm proteins collagen fibronectin laminin vitronectin and osteopontin and a cytoplasmic domain that interacts with intracellular signaling molecules when an integrin receptor binds to its ecm ligand and is activated additional integrins cluster around the activated site in addition kinases e g focal adhesion kinase fak and adapter proteins e g paxillin pax talin tal and shc are recruited to this cluster which is called the focal adhesion complex fac the activation of these fac molecules in turn triggers downstream events that up regulate and or down regulate intracellular processes such as transcription factor activation and gene regulation resulting in apoptosis or differentiation in addition to binding to ecm ligands integrins are also receptive to autocrine and paracrine signals such as growth factors in the tgf beta family chondrocytes have been shown to secrete tgf b and upregulate tgf b receptors in response to mechanical stimulation this secretion may be a mechanism for autocrine signal amplification within the tissue integrin signaling is just one example of multiple pathways that are activated when cartilage is loaded some intracellular processes that have been observed to occur within these pathways include phosphorylation of mapk and sapk erk kinase sek of the jnk pathway as well as changes in camp levels actin re organization and changes in the expression of genes which regulate cartilage ecm content more recent studies have hypothesized that chondrocyte primary cilium act as a mechanoreceptor for the cell transducing forces from the extracellular matrix into the cell each chondrocyte has one cilium and it is hypothesized to transmit mechanical signals by way of bending in response to ecm loading integrins have been identified on the upper shaft of the cilium acting as anchors to the collagen matrix around it recent studies published by wann et al in faseb journal have demonstrated for the first time that primary cilia are required for chondrocyte mechanotransduction chondrocytes derived from mutant mice did not express primary cilia and did not show the characteristic mechanosensitive up regulation of proteoglycan synthesis seen in wild type cells it is important to examine the mechanotransduction pathways in chondrocytes since mechanical loading conditions which represent an excessive or injuruous response upregulates synthetic activity and increases catabolic signalling cascades involving mediators such as no and mmps in addition studies by chowdhury tt and agarwal s have shown that mechanical loading which represents physiological loading conditions will block the production of catabolic mediators inos cox no induced by inflammatory cytokines il and restore anabolic activities thus an improved understanding of the interplay of biomechanics and cell signalling will help to develop therapeutic methods for blocking catabolic components of the mechanotransduction pathway a better understanding of the optimal levels of in vivo mechanical forces are therefore necessary for maintaining the health and viability of cartilage preventative techniques may be devised for the prevention of cartilage degradation and disease 
bioelectromagnetics is the study of the interaction between electromagnetic fields and biological entities common areas of investigation include animal navigation utilizing the geomagnetic field potential effects of man made sources of electromagnetic fields like mobile phones and developing new therapies to treat various conditions the term is similar to bioelectromagnetism which deals with the ability of living cells tissues and organisms to produce electrical fields and the response of cells to electromagnetic fields thermal effects most of the molecules in the human body interact weakly with electromagnetic fields in the radiofrequency or extremely low frequency bands one such interaction is absorption of energy from the fields which can cause tissue to heat up more intense fields will produce greater heating this can lead to biological effects ranging from muscle relaxation as produced by a diathermy device to burns many nations and regulatory bodies like the international commission on non ionizing radiation protection have established safety guidelines to limit emf exposure to a non thermal level this can be defined as either heating only to the point where the excess heat can be dissipated or as a fixed increase in temperature not detectable with current instruments like however biological effects have been shown to be present for these non thermal exposures various mechanisms have been proposed to explain these and there may be several mechanisms underlying the differing phenomena observed biological effects of weak electromagnetic fields are the subject of study in magnetobiology behavioral effects many behavioral effects at different intensities have been reported from exposure to magnetic fields particularly with pulsed magnetic fields the specific pulseform used appears to be an important factor for the behavioural effect seen for example a pulsed magnetic field originally designed for spectroscopic mri was found to alleviate symptoms in bipolar patients while another mri pulse had no effect a whole body exposure to a pulsed magnetic field was found to alter standing balance and pain perception in other studies tms and related effects a strong changing magnetic field can induce electrical currents in conductive tissue such as the brain since the magnetic field penetrates tissue it can be generated outside of the head to induce currents within causing transcranial magnetic stimulation tms these currents depolarize neurons in a selected part of the brain leading to changes in the patterns of neural activation hence tms changes the information content in neurons there is no structural or heating effect that may damage the tissue with only action potentials generated in the target area any risks are due to the arrival of action potentials to synapses and the natural activation of the postsynaptic cell a number of scientists and clinicians are attempting to use tms to replace electroconvulsive therapy ect to treat disorders such as severe depression instead of one strong electric shock through the head as in ect a large number of relatively weak pulses are delivered in tms therapy typically at the rate of about pulses per second if very strong pulses at a rapid rate are delivered to the brain the induced currents can cause convulsions sometimes this is done deliberately in order to treat depression such as in ect 
the l field is a name proposed by the yale professor of anatomy harold saxton burr for the electromagnetic field of any organism burr held that the study of this field offered great promise for medicine since it exhibited measurable qualities that might be used in prognosis of disease mood and viability the voltage measurements he used are not in doubt but the scientific community has all but ignored burr s term and his interpretation of the field as a blueprint like mold for all life those having produced notable research along the same lines include becker marino and selden lund and athenstaedt progress has also been made in the use of electromagnets to aid the healing of broken bones details beginning in the h s burr s seminal work at yale aimed at a gradual accumulation of hard data to support the hypothesis of the bio electric field as having emergent unexplained qualities and acting as a causal agent in development healing mood and health burr set up a series of experiments later repeated by other researchers which demonstrated some properties of these em fields which he called life fields l fields he showed that changes in the electrical potential of the l field were associated with changes in the health of the organism by leaving some trees hooked up to his l field detectors for decades he found correlations between such things as the phases of the moon sunspot activity thunderstorms and readings from the trees he found that the axis of em polarity in a frog s egg could predict the spinal axis of foetal development which he interpreted as suggesting that the l field was the organizing matrix for the body his insistence that the l field has primacy over the physical aspects of the organism eventually resulted in burr being accused of wishful vitalism in his work with humans he wrote papers detailing his successes in charting and predicting the ovulation cycles of women locating internal scar tissue and diagnosing potential physical ailments through the reading of the individual s l field as there was little interest in burr s work few other scientists even attempted to duplicate burr s results student and colleague leonard ravitz carried burr s work forward ravitz focused on the human dimension beginning with an investigation of the effects of the lunar cycle on the human l field he concluded that the human l field reaches a peak of activity at the full moon through work with hypnosis he became convinced that changes in the l field directly relate to changes in a person s mental and emotional states both emotional activity and stimuli of any sort involve mobilization of electrical energy as indicated on the galvanometer hence both emotions and stimuli evoke the same energy emotions can be equated with energy most intriguingly ravitz showed that the l field as a whole disappears before physical death 
nuclear magnetic resonance spectroscopy of proteins usually abbreviated protein nmr is a field of structural biology in which nmr spectroscopy is used to obtain information about the structure and dynamics of proteins the field was pioneered by richard r ernst and kurt among others protein nmr techniques are continually being used and improved in both academia and the biotech industry structure determination by nmr spectroscopy usually consists of several following phases each using a separate set of highly specialized techniques the sample is prepared resonances are assigned restraints are generated and a structure is calculated and validated sample preparation protein nuclear magnetic resonance is performed on aqueous samples of highly purified protein usually the sample consist of between and microlitres with a protein concentration in the range millimolar the source of the protein can be either natural or produced in an expression system using recombinant dna techniques through genetic engineering recombinantly expressed proteins are usually easier to produce in sufficient quantity and makes isotopic labelling possible the purified protein is usually dissolved in a buffer solution and adjusted to the desired solvent conditions the nmr sample is prepared in a thin walled glass tube data collection protein nmr utilizes multidimensional nuclear magnetic resonance experiments to obtain information about the protein ideally each distinct nucleus in the molecule experiences a distinct chemical environment and thus has a distinct chemical shift by which it can be recognized however in large molecules such as proteins the number of resonances can typically be several thousand and a one dimensional spectrum inevitably has incidental overlaps therefore multidimensional experiments are performed which correlate the frequencies of distinct nuclei the additional dimensions decrease the chance of overlap and have a larger information content since they correlate signals from nuclei within a specific part of the molecule magnetization is transferred into the sample using pulses of electromagnetic radiofrequency energy and between nuclei using delays the process is described with so called pulse sequences pulse sequences allow the experimenter to investigate and select specific types of connections between nuclei the array of nuclear magnetic resonance experiments used on proteins fall in two main categories one where magnetization is transferred through the chemical bonds and one where the transfer is through space irrespective of the bonding structure the first category is used to assign the different chemical shifts to a specific nucleus and the second is primarily used to generate the distance restraints used in the structure calculation and in the assignment with unlabelled protein depending on the concentration of the sample on the magnetic field of the spectrometer and on the type of experiment a single multidimensional nuclear magnetic resonance experiment on a protein sample may take hours or even several days to obtain suitable signal to noise ratio through signal averaging and to allow for sufficient evolution of magnetization transfer through the various dimensions of the experiment other things being equal higher dimensional experiments will take longer than lower dimensional experiments typically the first experiment to be measured with an isotope labelled protein is a heteronuclear single quantum correlation hsqc spectrum where heteronuclear refers to nuclei other than in theory the heteronuclear single quantum correlation has one peak for each h bound to a heteronucleus thus in the hsqc one signal is expected for each amino acid residue with the exception of proline which has no amide hydrogen due to the cyclic nature of its backbone tryptophan and certain other residues with n containing sidechains also give rise to additional signals the hsqc is often referred to as the fingerprint of a protein because each protein has a unique pattern of signal positions analysis of the hsqc allows researchers to evaluate whether the expected number of peaks is present and thus to identify possible problems due to multiple conformations or sample heterogeneity the relatively quick heteronuclear single quantum correlation experiment helps determine the feasibility of doing subsequent longer more expensive and more elaborate experiments it is not possible to assign peaks to specific atoms from the heteronuclear single quantum correlation alone resonance assignment in order to analyze the nuclear magnetic resonance data it is important to get a resonance assignment for the protein that is to find out which chemical shift corresponds to which atom this is typically achieved by sequential walking using information derived from several different types of nmr experiment the exact procedure depends on whether the protein is isotopically labelled or not since a lot of the assignment experiments depend on carbon and nitrogen homonuclear nuclear magnetic resonance with unlabelled protein the usual procedure is to record a set of two dimensional homonuclear nuclear magnetic resonance experiments through correlation spectroscopy cosy of which several types include conventional correlation spectroscopy total correlation spectroscopy tocsy and nuclear overhauser effect spectroscopy noesy a two dimensional nuclear magnetic resonance experiment produces a two dimensional spectrum the units of both axes are chemical shifts the cosy and tocsy transfer magnetization through the chemical bonds between adjacent protons the conventional correlation spectroscopy experiment is only able to transfer magnetization between protons on adjacent atoms whereas in the total correlation spectroscopy experiment the protons are able to relay the magnetization so it is transferred among all the protons that are connected by adjacent atoms thus in a conventional correlation spectroscopy an alpha proton transfers magnetization to the beta protons the beta protons transfers to the alpha and gamma protons if any are present then the gamma proton transfers to the beta and the delta protons and the process continues in total correlation spectroscopy the alpha and all the other protons are able to transfer magnetization to the beta gamma delta epsilon if they are connected by a continuous chain of protons the continuous chain of protons are the sidechain of the individual amino acids thus these two experiments are used to build so called spin systems that is build a list of resonances of the chemical shift of the peptide proton the alpha protons and all the protons from each sidechain which chemical shifts corresponds to which nuclei in the spin system is determined by the conventional correlation spectroscopy connectivities and the fact that different types of protons have characteristic chemical shifts to connect the different spinsystems in a sequential order the nuclear overhauser effect spectroscopy experiment has to be used because this experiment transfers magnetization through space it will show crosspeaks for all protons that are close in space regardless of whether they are in the same spin system or not the neighbouring residues are inherently close in space so the assignments can be made by the peaks in the noesy with other spin systems one important problem using homonuclear nuclear magnetic resonance is overlap between peaks this occurs when different protons have the same or very similar chemical shifts this problem becomes greater as the protein becomes larger so homonuclear nuclear magnetic resonance is usually restricted to small proteins or peptides carbon and nitrogen nuclear magnetic resonance when the protein is labelled with carbon and nitrogen it is possible to record experiments that transfers magnetisation over the peptide bond and thus connect different spin systems through bonds this is usually done using some of the following experiments hnco hn ca co hnca hn co ca hncacb and cbca co nh all six experiments consist of a plane similar to a hsqc spectrum expanded with a carbon dimension in the hn ca co each hn plane contains the peaks from the carbonyl carbon from its residue as well the preceding one in the sequence the hnco contains the carbonyl carbon chemical shift from only the preceding residue but is much more sensitive than hn ca co these experiments allow each peak to be linked to the preceding carbonyl carbon and sequential assignment can then be undertaken by matching the shifts of each spin system s own and previous carbons the hnca and hn co ca works similarly just with the alpha carbons rather than the carbonyls and the hncacb and the cbca co nh contains both the alpha carbon and the beta carbon usually several of these experiments are required to resolve overlap in the carbon dimension this procedure is usually less ambiguous than the noesy based method since it is based on through bond transfer in the noesy based methods additional peaks that are close in space but not belonging to the sequential residues will appear confusing the assignment process when the sequential assignment has been done it is usually possible to extend the assignment from the and to the rest of the sidechain using experiments such as hcch tocsy which is basically a tocsy experiment resolved in an additional carbon dimension restraint generation in order to make structure calculations a number of experimentally determined restraints have to be generated these fall into different categories the most widely used is distance restraints and angle restraints distance restraints a crosspeak in a noesy experiment signifies spatial proximity between the two nuclei in question thus each peak can be converted in to a maximum distance between the nuclei usually between and angstroms the intensity of a noesy peak is proportional to the distance to the minus power so the distance is determined according to intensity of the peak the intensity distance relationship is not exact so usually a distance range is used it is of great importance to assign the noesy peaks to the correct nuclei based on the chemical shifts if this task is performed manually it is usually very labor intensive since proteins usually have thousands of noesy peaks some computer programs such as unio cyana and aria cns perform this task automatically on manually pre processed listings of peak positions and peak volumes coupled to a structure calculation direct access to the raw noesy data without the cumbersome need of iteratively refined peak lists is so far only granted by the atnos candid approach implemented in the unio software package and thus indeed guarantees objective and efficient noesy spectral analysis to obtain as accurate assignments as possible it is a great advantage to have access to carbon and nitrogen noesy experiments since they help to resolve overlap in the proton dimension this leads to faster and more reliable assignments and in turn to better structures angle restraints in addition to distance restraints restraints on the torsion angles of the chemical bonds typically the psi and phi angles can be generated one approach is to use the karplus equation to generate angle restraints from coupling constants another approach uses the chemical shifts to generate angle restraints both methods use the fact that the geometry around the alpha carbon affects the coupling constants and chemical shifts so given the coupling constants or the chemical shifts a qualified guess can be made about the torsion angles orientation restraints the analyte molecules in a sample can be partially ordered with respect to the external magnetic field of the spectrometer by manipulating the sample conditions common techniques include addition of bacteriophages or bicelles to the sample or preparation of the sample in a stretched polyacrylamide gel this creates a local environment that favours certain orientations of nonspherical molecules normally in solution nmr the dipolar couplings between nuclei are averaged out because of the fast tumbling of the molecule the slight overpopulation of one orientation means that a residual dipolar coupling remains to be observed the dipolar coupling is commonly used in solid state nmr and provides information about the relative orientation of the bond vectors relative to a single global reference frame typically the orientation of the n h vector is probed in a hsqc like experiment initially residual dipolar couplings were used for refinement of previously determined structures but attempts at de novo structure determination have also been made exchange nmr spectroscopy is nucleus specific thus it can distinguish between hydrogen and deuterium the amide protons in the protein exchange readily with the solvent and if the solvent contains a different isotope typically deuterium the reaction can be monitored by nmr spectroscopy how rapidly a given amide exchanges reflects its solvent accessibility thus amide exchange rates can give information on which parts of the protein are buried hydrogen bonded etc a common application is to compare the exchange of a free form versus a complex the amides that become protected in the complex are assumed to be in the interaction interface structure calculation the experimentially determined restraints can be used as input for the structure calculation process researchers using computer programs such as cyana software or xplor nih attempt to satisfy as many of the restraints as possible in addition to general properties of proteins such as bond lengths and angles the algorithms convert the restraints and the general protein properties into energy terms and thus tries to minimize the energy the process results in an ensemble of structures that if the data were sufficient to dictate a certain fold will converge structure validation is important to note that the ensemble of structures obtained is an experimental model i e a representation of certain kind of experimental data to acknowledge this fact is really important because it means that the model could be a good or bad representations of that experimental data in general the quality of a model will depend on both the quantity and quality of experimental data used to generate it and the correct interpretation of such data it is important to keep track that every experiment has associated errors random errors will affect the reproducibility and precision of the resulting structures instead if the errors are systematic the accuracy of the model will be affected the precision indicates the degree of reproducibility of the measurement and is often expressed as the variance of the measured data set under the same conditions the accuracy however indicates the degree to which a measurement approaches its true value ideally a model of a protein will be more accurate the more fit the actual molecule that represents and will be more precise as there is less uncertainty about the positions of their atoms in practice there is no standard molecule against which to compare models of proteins so the accuracy of a model is given by the degree of agreement between the model and a set of experimental data historically the structures determined by nmr have been generally but not necessarily of lower quality than those determined by x ray diffraction this is due in part to the least amount of information contained in data obtained by nmr because of this fact it has become common practice to establish the quality of nmr ensembles by comparing it against the unique conformation determined by x ray diffraction for the same protein however the x ray diffraction structure may not exist and more importantly since the proteins in solution are flexible molecules a protein represented by a single structure may lead to underestimate the intrinsic variation of the atomic positions of a protein a set of conformations determined by nmr or x ray crystallographic may be a better representation of the experimental data of a protein than a unique conformation the utility of a model will be given at least in part by the degree of accuracy and precision of the model an accurate model with relatively poor precision could be useful to study the evolutionary relationships between the structures of a set of proteins while the rational drug design requires both precise and accurate models a model that is not accurate regardless of the degree of precision with which it was obtained will not be to much useful since protein structures are experimental models that can contain errors it is very important to be able to detect these errors the process aimed at the detection of errors is known as validation there are several methods to validate structures some are statistical like procheck and what if while others are based on physical principles as cheshift or a mixture of statistical and physics principles psvs dynamics in addition to structures nuclear magnetic resonance can yield information on the dynamics of various parts of the protein this usually involves measuring relaxation times such as and to determine order parameters correlation times and chemical exchange rates nmr relaxation is a consequence of local fluctuating magnetic fields within a molecule local fluctuating magnetic fields are generated by molecular motions in this way measurements of relaxation times can provide information of motions within a molecule on the atomic level in nmr studies of protein dynamics the nitrogen isotope is the preferred nucleus to study because its relaxation times are relatively simple to relate to molecular motions this however requires isotope labeling of the protein the and relaxation times can be measured using various types of hsqc based experiments the types of motions which can be detected are motions that occur on a time scale ranging from about picoseconds to about nanoseconds in addition slower motions which take place on a time scale ranging from about microseconds to milliseconds can also be studied however since nitrogen atoms are mainly found in the backbone of a protein the results mainly reflect the motions of the backbone which is the most rigid part of a protein molecule thus the results obtained from nitrogen relaxation measurements may not be representative for the whole protein therefore techniques utilising relaxation measurements of carbon and deuterium have recently been developed which enables systematic studies of motions of the amino acid side chains in proteins a challenging and special case of study regarding dynamics and flexibility of peptides and full length proteins is represented by disordered structures nowadays it is an accepted concept that proteins can exhibit a more flexible behaviour known as disorder or lack of structure however it is possible to describe an ensemble of structures instead of a static picture representing a fully functional state of the protein many advances are represented in this field particularly in terms of new pulse sequences technological improvement and rigorous training of researchers in the field nmr spectroscopy on large proteins traditionally nuclear magnetic resonance spectroscopy has been limited to relatively small proteins or protein domains this is in part caused by problems resolving overlapping peaks in larger proteins but this has been alleviated by the introduction of isotope labelling and multidimensional experiments another more serious problem is the fact that in large proteins the magnetization relaxes faster which means there is less time to detect the signal this in turn causes the peaks to become broader and weaker and eventually disappear two techniques have been introduced to attenuate the relaxation transverse relaxation optimized spectroscopy trosy and deuteration of proteins by using these techniques it has been possible to study proteins in complex with the kda chaperone groes groel automation of the process structure determination by nmr has traditionally been a time consuming process requiring interactive analysis of the data by a highly trained scientist there has been a considerable interest in automating the process to increase the throughput of structure determination and to make protein nmr accessible to non experts see structural genomics the two most time consuming processes involved are the sequence specific resonance assignment backbone and side chain assignment and the noe assignment tasks several different computer programs have been published that target individual parts of the overall nmr structure determination process in an automated fashion most progress has been achieved for the task of automated noe assignment so far only the flya and the unio approach were proposed to perform the entire protein nmr structure determination process in an automated manner without any human intervention efforts have also been made to standardize the structure calculation protocol to make it quicker and more amenable to automation 
hnca is a nmr experiment commonly used in the field of protein nmr the name derives from the experiment s magnetization transfer pathway the magnetization of the amide proton of an amino acid residue is transferred to the amide nitrogen and then to the alpha carbons of both the starting residue and the previous residue in the protein s amino acid sequence in contrast the complementary hncoca experiment transfers magnetization only to the alpha carbon of the previous residue the hnca experiment is used often in tandem with hncoca to assign alpha carbon resonance signals to specific residues in the protein this experiment requires a purified sample of protein prepared with and isotopic labelling at a concentration greater than mm and is thus generally only applied to recombinant proteins the spectrum produced by this experiment has dimensions a proton axis a axis and a axis for residue i peaks will appear at hn i n i calpha i and hn i n i calpha i while for the complementary hncoca experiment peaks appear only at hn i n i calpha i together these two experiments reveal the alpha carbon chemical shift for each amino acid residue in a protein and provide information linking adjacent residues in the protein s sequence references general references protein nmr spectroscopy principles and practice john cavanagh wayne j fairbrother arthur g palmer iii nicholas j skelton academic press 
hncoca is a nmr experiment commonly used in the field of protein nmr the name derives from the experiment s magnetization transfer pathway the magnetization of the amide proton of an amino acid residue is transferred to the amide nitrogen and then to the alpha carbon of the previous residue in the protein s amino acid sequence in contrast the complementary hnca experiment transfers magnetization to the alpha carbons of both the starting residue and the previous residue in the sequence the hncoca experiment is used often in tandem with hnca to assign alpha carbon resonance signals to specific residues in the protein this experiment requires a purified sample of protein prepared with and isotopic labelling at a concentration greater than mm and is thus generally only applied to recombinant proteins the spectrum produced by this experiment has dimensions a proton axis a axis and a axis for residue i peaks will appear at hn i n i i only while for the complementary hnca experiment peaks appear at hn i n i i and hn i n i i together these two experiments reveal the alpha carbon chemical shift for each amino acid residue in a protein and provide information linking adjacent residues in the protein s sequence 
the biophysical society is an american scientific society that exists to encourage the development and dissemination of knowledge in biophysics founded in by ernest c pollard the society currently consists of over researchers in academia government and industry although based in the united states its international membership has grown to about one third of the total origins the biophysical society was founded in response to the growth of the field of biophysics after world war two as well as concerns that the american physiological society had become too large to serve the community of biophysicists discussions between prominent biophysicists in and led to the planning of the society s first meeting in columbus ohio in with about attendees among the scientists involved in the early effort were ernest c pollard samuel talbot otto schmitt kenneth stewart cole w a selle max lauffer ralph stacy herman p schwan and robley c williams this meeting was described by cole as a biophysics meeting with the ulterior motive of finding out if there was such a thing as biophysics and if so what sort of thing this biophysics might be the biophysical journal was established two years later as a specialized journal in the field of biophysics this was seen as important because other existing journals were perceived as being unsympathetic to submissions by biophysicists activities since the biophysical society has sponsored edited and published the biophysical journal which is currently published semi monthly the society also published a monthly newsletter as well as an annual membership directory and products guide the biophysical society sponsors an annual meeting which brings together more than for symposia workshops industrial and educational exhibits subgroup meetings and awards presentations the small size of the conference as compared to those of similar scientific societies is promoted as causing a more intimate environment during the conference since the meeting has featured a talk by that year s national lecturer chosen for significance in biophysical research and excellence in presentation the lectures are published in the biophysical journal and those since are available on video starting in with calcium signaling in beijing the society now also sponsors smaller thematic meetings across the world the society offers eight society awards each year to distinguished biophysicists in different categories the society also offers awards for students such as a number of poster competitions at its annual meeting as well as other scientific conferences in addition the society sponsors biophysics awards at high school science fairs across the nation organization the biophysical society contains subgroups focusing on smaller areas within biophysics the current subgroups are bioenergetics biological fluorescence biopolymers in vivo exocytosis endocytosis intrinsically disordered proteins membrane biophysics membrane structure assembly molecular biophysics motility nanoscale biophysics and permeation transport in addition to the scientific subgroups the society has a number of committees which help to implement the its mission the committees are awards early careers education finance international relations member services membership minority affairs nominating professional opportunities for women program public affairs publications and thematic meetings the society is governed by four officers the president president elect secretary and treasurer as well as by a council of seven members these offices are elected by the membership of the society there is also an executive board consisting of the four officers and four of the council members plus the past president public policy the biophysical society also maintains a public affairs committee which responds to science policy issues such as research careers and science education and has adopted a number of positions in february the society released a statement supporting freedom of communication of scientific data supporting the existing policy that prior classification strictly for national security reasons should be the only reason communication of scientific data should be restricted the society also urged a reexamination of visa policy in the wake of several foreign born scientists being denied permission to travel to the united states citing the importance of their importance to the economy and security of the united states in may the society released a statement opposing the teaching of intelligent design in science classrooms calling it an effort to blur the distinction between science and theology the society is also active in supporting federal funding of science and provides materials to assist scientists in communicating with elected officials the society participates in the annual science engineering technology congressional visits day in which scientists engineers and business leaders meet with elected officials in the united states congress 
biomagnetism is the phenomenon of magnetic fields produced by living organisms it is a subset of bioelectromagnetism in contrast organisms use of magnetism in navigation is magnetoception and the study of the magnetic fields effects on organisms is magnetobiology the word biomagnetism has also been used loosely to include magnetobiology further encompassing almost any combination of the words magnetism cosmology and biology i e magnetoastrobiology the origin of the word biomagnetism is unclear but seems to have appeared several hundred years ago linked to the expression animal magnetism the present scientific definition took form in the when an increasing number of researchers began to measure the magnetic fields produced by the human body the first valid measurement was actually made in but the field began to expand only after a low noise technique was developed in today the community of biomagnetic researchers does not have a formal organization but international conferences are held every two years with about attendees most conference activity centers around the meg magnetoencephalogram the measurement of the magnetic field of the brain 
a stopped flow instrument is a rapid mixing device used to study the chemical kinetics of a reaction in solution after two or more solutions containing the reagents are mixed they are studied by whatever experimental methods are deemed suitable different forms of spectroscopy and scattering of radiation are common methods used the dead time is the time between the end of mixing the two solutions and the beginning of observation of the kinetics of the reaction the usual dead time of a stopped flow apparatus is milliseconds but some new devices have been developed that have dead times of ms a stopped flow instrument coupled to either a circular dichroism spectrometer or a fluorescence spectrometer is often used in the field of protein folding to observe rapid unfolding and or refolding of proteins in a quenched flow instrument the reaction is stopped after a certain amount of time has passed after mixing the stopping of the reaction is called quenching and it can be achieved by various means for example by mixing with another solution which stops the reaction chemical quenching quickly lowering the temperature freeze quenching or even by exposing the sample to light of a certain wavelength optical quenching 
isothermal titration calorimetry itc is a physical technique used to determine the thermodynamic parameters of interactions in solution it is most often used to study the binding of small molecules such as medicinal compounds to larger macromolecules proteins dna etc thermodynamic measurements where r is the gas constant and t is the absolute temperature the instrument an isothermal titration calorimeter is composed of two identical cells made of a highly efficient thermal conducting and chemically inert material such as hastelloy alloy or gold surrounded by an adiabatic jacket sensitive thermopile thermocouple circuits are used to detect temperature differences between the reference cell filled with buffer or water and the sample cell containing the macromolecule prior to addition of ligand a constant power mw is applied to the reference cell this directs a feedback circuit activating a heater located on the sample cell during the experiment ligand is titrated into the sample cell in precisely known aliquots causing heat to be either taken up or evolved depending on the nature of the reaction measurements consist of the time dependent input of power required to maintain equal temperatures between the sample and reference cells in an exothermic reaction the temperature in the sample cell increases upon addition of ligand this causes the feedback power to the sample cell to be decreased remember a reference power is applied to the reference cell in order to maintain an equal temperature between the two cells in an endothermic reaction the opposite occurs the feedback circuit increases the power in order to maintain a constant temperature isothermic isothermal operation observations are plotted as the power needed to maintain the reference and the sample cell at an identical temperature against time as a result the experimental raw data consists of a series of spikes of heat flow power with every spike corresponding to one ligand injection these heat flow spikes pulses are integrated with respect to time giving the total heat exchanged per injection the pattern of these heat effects as a function of the molar ratio can then be analysed to give the thermodynamic parameters of the interaction under study it should be noted that degassing samples is often necessary in order to obtain good measurements as the presence of gas bubbles within the sample cell will lead to abnormal data plots in the recorded results the entire experiment takes place under computer control external links alternative itc data analysis software ic itc scripted utility for isothermal titration calorimetry analysis stoichiometric examination suitcase 
the max planck institute for biophysics is located in frankfurt am main germany it was founded as kaiser wilhelm institute for biophysics in and moved into a new building in it is one of institutes in the max planck society max planck gesellschaft an essential prerequisite for the understanding of the fundamental processes of life is the knowledge of the structure of the participating macromolecules two of the fourt departments are devoted to the challenging task of determining the structure of membrane proteins under the direction of hartmut michel nobel prize in chemistry of for the first structure determination of a membrane protein the department of molecular membrane biology approaches this problem primarily by x ray crystallography whereas the department of structural biology headed by werner uses the complementary technique of electron microscopy the department of biophysical chemistry directed by ernst bamberg studies the function of these proteins in native or reconstituted membranes by electrophysiological and spectroscopic methods the fourth department molecular neurogenetics under the direction of peter mombaerts has started its work in since the institute hosts two max planck research groups computational structural biology led by lucy r forrest and theoretical molecular biophysics directed by d faraldo since april the institute s four departments are housed in the same building resulting in improved scientific interaction between the research groups scientific links to fellow researchers at frankfurt university have been strengthened further as the institute is now situated next to the university s biology chemistry and physics laboratories together with the max planck institute for brain research and the goethe university of frankfurt am main the institute runs the international max planck research school impres on the structure and function of biological membranes a graduate program offering a ph d a note on the institute s history the institute was founded in frankfurt as the kaiser wilhelm institute for biophysics in however it had a predecessor the institut physikalische grundlagen der medizin which had been established in by friedrich dessauer an admirer of wilhelm roentgen who endeavored to apply radiation physics to medicine and biology being a conservative member of parliament for the democratic zentrumspartei dessauer opposed the national socialists rise to power and was then forced to emigrate in his successor dessauer s colleague and long standing collaborator boris rajewsky was the first director of the kaiser wilhelm institute for biophysics rajewsky firstly coined the term biophysics and consequently the institute became one of the first to be known by this name 
the flux equation or ghk flux equation describes the ionic flux carried by an ionic species across a cell membrane as a function of the transmembrane potential and the concentrations of the ion inside and outside of the cell since both the voltage and the concentration gradients influence the movement of ions this process is a simplified version of electrodiffusion electrodiffusion is most accurately defined by the nernst planck equation and the ghk flux equation is a solution to the nernst planck equation with the assumptions listed below origin the american david e goldman of columbia university and the english nobel laureates alan lloyd hodgkin and bernard katz derived this equation equation where implicit definition of reversal potential the reversal potential is shown to be contained in the ghk flux equation flax the proof is replicated from the reference flax here we wish to show that when the flux is zero the transmembrane potential is not zero formally it is written formula which is equivalent to writing formula which states that when the transmembrane potential is zero the flux is not zero however due to the form of the ghk flux equation when formula formula this is a problem as the value of formula is unknown it is evident from the previous equation that when formula formula if formula and thus which is the definition of the reversal potential rectification since one of the assumptions of the ghk flux equation is that the ions move independently of each other the total flow of ions across the membrane is simply equal to the sum of two oppositely directed fluxes each flux approaches an asymptotic value as the membrane potential diverges from zero these asymptotes are and where subscripts i and o denote the intra and extracellular compartments respectively keeping all terms except v m constant the equation yields a straight line when plotting formula s against v m it is evident that the ratio between the two asymptotes is merely the ratio between the two concentrations of s and so thus if the two concentrations are identical the slope will be identical and constant throughout the voltage range corresponding to ohm s law scaled by the surface area as the ratio between the two concentrations increases so does the difference between the two slopes meaning that the current is larger in one direction than the other given an equal driving force of opposite signs this is contrary to the result obtained if using ohm s law scaled by the surface area and the effect is called rectification the ghk flux equation is mostly used by electrophysiologists when the ratio between and so is large and or when one or both of the concentrations change considerably during an action potential the most common example is probably intracellular calcium which during a cardiac action potential cycle can change fold or more and the ratio between o and i can reach or more 
in proteins avidity is a term used to describe the combined strength of multiple bond interactions avidity is distinct from affinity which is a term used to describe the strength of a single bond as such avidity is the combined synergistic strength of bond affinities rather than the sum of bonds it is commonly applied to antibody interactions in which multiple antigen binding sites simultaneously interact with a target individually each binding interaction may be readily broken however when many binding interactions are present at the same time transient unbinding of a single site does not allow the molecule to diffuse away and binding of that site is likely to be reinstated the overall effect is synergistic strong binding of antigen to antibody e g igm is said to have low affinity but high avidity because it has weak binding sites as opposed to the strong binding sites of igg ige and igd if the clustered proteins form a matrix such as a clathrin coat the interaction is described by the term matricity 
this section covers radionuclides hydrocarbons and organic solvents and information on the plants used for their remediation 
medical biophysics refers to the domain of study that uses physics to describe or effect biological process for the purpose of medical application like many areas of study that have emerged in recent times it relies heavily on broad interdisciplinary knowledge between the so called traditional fields such as physics i e medical physics radiation physics or imaging physics and advanced biology fields such as biochemistry biophysics physiology neuroscience etc some important areas of research in medical biophysics which have evolved from medical physics and diagnostic imaging include medical imaging e g mri computed tomography and pet oncology and cancer diagnosis using radiolabelling and molecular imaging and vasculature and circulatory system function some important areas of research in medical biophysics which have evolved from biology and biophysics include using targeted nanomedicine not only for medical imaging but to also deliver energy to disease for treatment the field of interdisciplinary medical dosimetry imd combines the fields of radiation biology and radiation physics with biochemistry and biophysics to define the proper way of comparing and properly prescribing the deposition of energy into biological tissue and molecular and cellular damage from diverse sources i e x rays electrons protons hyperthermia ultrasound oxidative stress etc for medical applications and is an example of a newly emergent subfield of medical biophysics 
davydov soliton is a quantum quasiparticle representing an excitation propagating along the protein helix self trapped amide i it is a solution of the davydov hamiltonian it is named for the soviet and ukrainian physicist alexander davydov the davydov model describes the interaction of the amide i vibrations with the hydrogen bonds that stabilize the helix of proteins the elementary excitations within the helix are given by the phonons which correspond to the deformational oscillations of the lattice and the excitons which describe the internal amide i excitations of the peptide groups referring to the atomic structure of an helix region of protein the mechanism that creates the davydov soliton polaron exciton can be described as follows vibrational energy of the c o stretching or amide i oscillators that is localized on the helix acts through a phonon coupling effect to distort the structure of the helix while the helical distortion reacts again through phonon coupling to trap the amide i oscillation energy and prevent its dispersion this effect is called self localization or self trapping solitons in which the energy is distributed in a fashion preserving the helical symmetry are dynamically unstable and such symmetrical solitons once formed decay rapidly when they propagate on the other hand an asymmetric soliton which spontaneously breaks the local translational and helical symmetries possesses the lowest energy and is a robust localized entity davydov s hamiltonian is formally similar to the holstein hamiltonian for the interaction of electrons with a polarizable lattice thus the hamiltonian of the energy operator formula is where formula is the quasiparticle exciton hamiltonian which describes the motion of the amide i excitations between adjacent sites formula is the phonon hamiltonian which describes the vibrations of the lattice and formula is the interaction hamiltonian which describes the interaction of the amide i excitation with the lattice where the index formula counts the peptide groups along the helix spine the index formula counts each helix spine formula j is the energy of the amide i vibration co stretching formula j is the dipole dipole coupling energy between a particular amide i bond and those ahead and behind along the same spine formula j is the dipole dipole coupling energy between a particular amide i bond and those on adjacent spines in the same unit cell of the protein helix formula and formula are respectively the boson creation and annihilation operator for a quasiparticle at the peptide group formula the phonon hamiltonian formula is where formula is the displacement operator from the equilibrium position of the peptide group formula formula is the momentum operator of the peptide group formula m is the mass of each peptide group and formula n mformula is an effective elasticity coefficient of the lattice the spring constant of a hydrogen bond finally the interaction hamiltonian formula is where formula pn is an anharmonic parameter arising from the coupling between the quasiparticle exciton and the lattice displacements phonon and parameterizes the strength of the exciton phonon interaction the value of this parameter for helix has been determined via comparison of the theoretically calculated absorption line shapes with the experimentally measured ones the mathematical techniques that are used to analyze davydov s soliton are similar to some that have been developed in polaron theory in this context the davydov s soliton corresponds to a polaron that is i large so the continuum limit approximation is justified ii acoustic because the self localization arises from interactions with acoustic modes of the lattice and iii weakly coupled because the anharmonic energy is small compared with the phonon bandwidth the davydov soliton is a quantum quasiparticle and it obeys heisenberg s uncertainty principle thus any model that does not impose translational invariance is flawed by construction supposing that the davydov soliton is localized to turns of the helix results in significant uncertainty in the velocity of the soliton formula m s a fact that is obscured if one models the davydov soliton as a classical object there are three possible fundamental approaches towards davydov model i the quantum theory in which both the amide i vibration excitons and the lattice site motion phonons are treated quantum mechanically ii the mixed quantum classical theory in which the amide i vibration is treated quantum mechanically but the lattice is classical and iii the classical theory in which both the amide i and the lattice motions are treated classically 
the model named after richard fitzhugh who suggested the system in and j nagumo et al who created the equivalent circuit the following year describes a prototype of an excitable system e g a neuron if the external stimulus formula exceeds a certain threshold value the system will exhibit a characteristic excursion in phase space before the variables formula and formula relax back to their rest values this behaviour is typical for spike generations short elevation of membrane voltage formula in a neuron after stimulation by an external input current the equations for this dynamical system read the dynamics of this system can be nicely described by zapping between the left and right branch of the cubic nullcline the model is a simplified version of the model which models in a detailed manner activation and deactivation dynamics of a spiking neuron in the original papers of fitzhugh this model was called der pol oscillator named after karl friedrich bonhoeffer and balthasar van der pol because it contains the van der pol oscillator as a special case for formula the equivalent circuit was suggested by jin ichi nagumo suguru arimoto and shuji yoshizawa http www siam org news news php id 
vibrational circular dichroism vcd is a spectroscopic technique which detects differences in attenuation of left and right circularly polarized light passing through a sample it is the extension of circular dichroism spectroscopy into the infrared and near infrared ranges because vcd is sensitive to the mutual orientation of distinct groups in a molecule it provides three dimensional structural information thus it is a powerful technique as vcd spectra of enantiomers can be simulated using ab initio calculations thereby allowing the identification of absolute configurations of small molecules in solution from vcd spectra among such quantum computations of vcd spectra resulting from the chiral properties of small organic molecules are those based on density functional theory dft and gauge invariant atomic orbitals giao as a simple example of the experimental results that were obtained by vcd are the spectral data obtained within the carbon hydrogen c h stretching region of amino acids in heavy water solutions measurements of vibrational optical activity voa have thus numerous applications not only for small molecules but also for large and complex biopolymers such as muscle proteins myosin for example and dna theory of vcd while the fundamental quantity associated with the infrared absorption is the dipole strength the differential absorption is proportional also to the rotational strength a quantity which depends on both the electric and magnetic dipole transition moments sensitivity of the handedness of a molecule toward circularly polarized light results from the form of the rotational strength vcd of peptides and proteins extensive vcd studies have been reported for both polypeptides and several proteins in solution several recent reviews were also compiled an extensive but not comprehensive vcd publications list is also provided in the references section the published reports over the last years have established vcd as a powerful technique with improved results over those previously obtained by visible uv circular dichroism cd or optical rotatory dispersion ord for proteins and nucleic acids vcd of nucleic acids vcd spectra of nucleotides synthetic polynucleotides and several nucleic acids including dna have been reported and assigned in terms of the type and number of helices present in a b and z dna vcd instrumentation for biopolymers such as proteins and nucleic acids the difference in absorbance between the levo and dextro configurations is five orders of magnitude smaller than the corresonding unpolarized absorbance therefore vcd of biopolymers requires the use of very sensitive specially built instrumentation as well as time averaging over relatively long intervals of time even with such sensitive vcd spectrometers most cd instruments produce left and right circularly polarized light which is then either sine wave or square wave modulated with subsequent phase sensitive detection and lock in amplification of the detected signal in the case of ft vcd a photo elastic modulator pem is employed in conjunction with an ftir interferometer set up an example is that of a bomem model mb ftir interferometer equipped with additional polarizing optics accessories needed for recording vcd spectra a parallel beam emerges through a side port of the interferometer which passes first through a wire grid linear polarizer and then through an octagonal shaped znse crystal pem which modulates the polarized beam at a fixed lower frequency such as a mechanically stressed crystal such as znse exhibits birefringence when stressed by an adjacent piezoelectric transducer the linear polarizer is positioned close to and at degrees with respect to the znse crystal axis the polarized radiation focused onto the detector is doubly modulated both by the pem and by the interferometer setup a very low noise detector such as mct hgcdte is also selected for the vcd signal phase sensitive detection the first dedicated vcd spectrometer brought to market was the chiralir from bomem biotools inc in today thermo electron bruker jasco and biotools offer either vcd accessories or stand alone instrumentation to prevent detector saturation an appropriate long wave pass filter is placed before the very low noise mct detector which allows only radiation below to reach the mct detector the latter however measures radiation only down to ft vcd spectra accumulation of the selected sample solution is then carried out digitized and stored by an in line computer published reviews that compare various vcd methods are also available magnetic vcd vcd spectra have also been reported in the presence of an applied external magnetic field this method can enhance the vcd spectral resolution for small molecules raman optical activity roa roa is a technique complementary to vcd especially useful in the spectral region it is considered as the technique of choice for determining optical activity for photon energies less than 
magnetic tweezers mt are scientific instruments for the manipulation of single biomolecules this apparatus allows to exert forces and torques to the molecules the extension of the molecules corresponds to the response of the system to the applied stress in experiments the biomolecule of interest is attached at one end to a tethering surface and at the other to a magnetic microparticle the magnetic tweezers apparatus is also equipped with magnets that are used to manipulate the magnetic particles whose position is measured with the help of video microscopy most commonly magnetic tweezers are used to study mechanic properties of biological macromolecules like dna or proteins in single molecule experiments other applications are the rheology of soft matter and studies of force regulated processes in living cells forces are typically on the order of pico to nanonewtons due to their simple architecture magnetic tweezers are a popular biophysical technique construction principle and physics of magnetic tweezers a magnetic tweezers apparatus consists of magnetic micro particles which can be manipulated with the help of an external magnetic field the position of the magnetic particles is then determined by a microscopic objective with a camera magnetic particles magnetic particles for the operation in magnetic tweezers come with a wide range of properties and have to be chosen in function of the intended application two basic types of magnetic particles are described in the following paragraphs however there are also others like magnetic nanoparticles in ferrofluids which allow experiments inside a cell superparamagnetic beads are commercially available with a number of different characteristics the most common is the use of spherical particles of a diameter in the micrometer range they consist of a porous latex matrix in which magnetic nanoparticles have been embedded latex is auto fluorescent and may therefore be advantageous for the later imaging irregular shaped particles present a larger surface and hence a higher probability to bind to the molecules to be studied the coating of the microbeads contains also ligands to be able to attach the molecules of interest for example the coating may contain streptavidin which couples strongly to biotin which itself may be bound to the molecules of interest formula formula the outer magnetic field can be evaluated numerically with the help of finite element analysis or by simply measuring the magnetic field with the help of a hall effect sensor theoretically it would be possible to calculate the force on the beads with these formulae however the results are not very reliable due to uncertainties of the involved variables but they allow estimating the order of magnitude and help to better understand the system more accurate numerical values can be obtained considering the brownian motion of the beads formula the torques generated by this method are typically much greater than formula which is more than necessary to twist the molecules of interest the use of ferromagnetic nanowires for the operation of magnetic tweezers enlarges their experimental application range the length of these wires typically is in the order of tens of nanometers up to tens of micrometers which is much more than their diameter in comparison with superparamagnetic beads they allow the application of much larger forces and torques in addition to that they present a remnant magnetic moment this allows the operation in weak magnetic field strengths it is possible to produce nanowires with surface segments that present different chemical properties which allows controlling the position where the studied molecules can bind to the wire magnets for being able to exert torques on the microbeads at least two magnets are necessary but many other configurations have been realized reaching from only one magnet that only pulls the magnetic microbeads to a system of six electromagnets that allows fully controlling the dimensional positioning and rotation via a digital feedback loop the magnetic field strength decreases roughly exponentially with the distance from the axis linking the two magnets on a typical scale of about the width of the gap between the magnets since this scale is rather large in comparison to the distances the microbeads move in an experiment the force acting on a bead may be treated as constant therefore magnetic tweezers are passive force clamps due to the nature of their construction in contrast to optical tweezers although they may be used as position clamps too when combined with a feedback loop the field strength may be increased by sharpening the pole face of the magnet which however also diminishes the area where the field may be considered as constant an iron ring connection the outer poles of the magnets may help to reduce stray fields magnetic tweezers can be operated with both permanent magnets and electromagnets the two techniques have their specific advantages permanent magnets of magnetic tweezers are usually out of rare earth materials like neodym and can reach field strengths exceeding tesla the force on the beads may be controlled by moving the magnets along the vertical axis moving them up decreases the field strength at the position of the bead and vice versa torques on the magnetic beads may be exerted by turning the magnets around the vertical axis to change the direction of the field the size of the magnets is in the order of millimeters as well as their spacing the use of electromagnets in magnetic tweezers has the advantage that the field strength and direction can be changed just by adjusting the amplitude and the phase of the current for the magnets for this reason the magnets do not need to be moved which allows a faster control of the system and reduces mechanical noise in order to increase the maximum field strength a core of a soft paramagnetic material with high saturation and low remanance may be added to the solenoid in any case however the typical field strengths are much lower compared to those of permanent magnets of comparable size additionally using electromagnets requires high currents that produce heat that may necessitate a cooling system bead tracking system the displacement of the magnetic beads corresponds to the response of the system to the imposed magnetic field and hence needs to be precisely measured in a typical set up the experimental volume is illuminated from the top so that the beads produce diffraction rings in the focal plane of an objective which is placed under the tethering surface the diffraction pattern is then recorded by a ccd camera the image can be analyzed in real time by a computer the detection of the position in the plane of the tethering surface is not complicated since it corresponds to the center of the diffraction rings the precision can be up to a few nanometers for the position along the vertical axis the diffraction pattern needs to be compared with reference images which show the diffraction pattern of the considered bead in a number of known distances from the focal plane these calibration images are obtained by keeping a bead fixed while displacing the objective i e the focal plane with the help of piezoelectric elements by known distances with the help of interpolation the resolution can reach precision of up nm along this axis the obtained coordinates maybe used as input for a digital feedback loop that controls the magnetic field strength for example in order to keep the bead at a certain position non magnetic beads are usually also added to the sample as a reference to provide a background displacement vector they have a different diameter as the magnetic beads so that they are optically distinguishable this is necessary to detect potential drift of the fluid for example if the density of magnetic particles is too high they may drag the surrounding viscous fluid with them the displacement vector of a magnetic bead can be determined by subtracting its initial position vector and this background displacement vector from its current position force calibration formula formula formula formula formula the term formula corresponds to the stokes friction force for a spherical particle of radius formula in a medium of viscosity formula and formula is the restoring force which is opposed to the stochastic force formula due to the brownian motion here one may neglect the inertial term formula because the system is in a regime of very low reynolds number formula formula formula formula formula the only unknown in this expression formula can be determined by fitting this expression to the experimental power spectrum for more accurate results one may subtract the effect due to finite camera integration time from the experimental spectrum before doing the fit formula the velocity formula can be determined by using the recorded velocity values the force obtained via this formula can then be related to a given configuration of the magnets which may serve as a calibration comparison to other techniques this section compares the features of magnetic tweezers with those of the most important other single molecule experimental methods optical tweezers and atomic force microscopy the magnetic interaction is highly specific to the used superparamagnetic microbeads the magnetic field does practically not affect the sample optical tweezers have the problem that the laser beam may also interact with other particles of the biological sample due to contrasts in the refractive index in addition to that the laser may cause photodamage and sample heating in the case of atomic force microscopy it may also be hard to discriminate the interaction of the tip with the studied molecule from other nonspecific interactions thanks to the low trap stiffness the range of forces accessible with magnetic tweezers is lower in comparison with the two other techniques the possibility to exert torque with magnetic tweezers is not unique optically tweezers may also offer this feature when operated with birefringent microbeads in combination with a circularly polarized laser beam another advantage of magnetic tweezers is that it is easy to carry out in parallel many single molecule measurements an important drawback of magnetic tweezers is the low temporal and spatial resolution due to the data acquisition via video microscopy typical experimental set up this section gives an example for an experiment carried out with the help of magnetic tweezers a double stranded dna molecule is fixed with multiple binding sites on one end to a glass surface and on the other to a magnetic micro bead which can be manipulated in a magnetic tweezers apparatus by turning the magnets torsional stress can be applied to the dna molecule rotations in the sense of the dna helix are counted positively and vice versa while twisting the magnetic tweezers also allow stretching the dna molecule this way torsion extension curves may be recorded at different stretching forces for low forces less than about pn the dna forms supercoils so called plectonemes which decrease the extension of the dna molecule quite symmetrically for positive and negative twists augmenting the pulling force already increases the extension for zero imposed torsion positive twists lead again to plectoneme formation that reduces the extension negative twist however does not change the extension of the dna molecule a lot this can be interpreted as the separation of the two strands which corresponds to the denaturation of the molecule in the high force regime the extension is nearly independent of the applied torsional stress the interpretation is the apparition of local regions of highly overwound dna an important parameter of this experiment is also the ionic strength of the solution which affects the critical values of the applied pulling force that separate the three force regimes examples of research groups using magnetic tweezers http www childrenshospital org research ingber ingber lab harvard medical school http engr ucsb edu saleh saleh lab material science department uc santa barbara http web mit edu meche mb mechanobiology group mit roger kamm http www engr pitt edu mems nanosystems measurement controls lab department of mechanical engineering material science university of pittsburgh http web mit edu solab so lab mit http forgacslab missouri edu tweezers html forgacs lab department of physics astronomy university of missouri http www jhu edu cheme wirtz wirtz lab johns hopkins university http alice berkeley edu bustamante lab university of california berkeley http markolab bmbcb northwestern edu marko marko lab northwestern university http www mecheng osu edu pmcl precision measurement control lab the ohio state university http www sfu ca fordelab forde lab simon fraser university british columbia http www phys waseda ac jp index html kazuhiko kinosita waseda university tokyo http www physics nus edu sg biosmm index html jie yan national university of singapore singapore http www biological physics manchester ac uk biological physics research group university of manchester uk http www ijm fr en ijm research research groups single molecule nanomanipulation single molecule nanomanipulation institut jacques monod paris france http www mnp leeds ac uk molecular and nanoscale physics group university of leeds leeds england http www bio ph tum de erich sackmann technical university of munich germany http bpe tnw utwente nl biophysical engineering group university of twente the netherlands http lpmt biomed uni erlangen de fabry lab university of erlangen nuremberg germany http www pci uni heidelberg de bpc biophysik html joachim spatz university of heidelberg germany http www curie fr recherche themes detail equipe cfm lang gb id equipe htm sykes curie institute paris france http www phys ens fr biolps david bensimon and vincent croquette normale paris france http www bn tudelft nl nynke dekker molecular biophysics technical university delft the netherlands http www biotec tu dresden de seidel ralf seidel dna motors group technical university of dresden germany http www physik uni bielefeld de biophysik dario anselmetti experimental biophysics and applied nanosciences bielefeld university germany 
oja s learning rule or simply oja s rule named after a finnish computer scientist erkki oja is a model of how neurons in the brain or in artificial neural networks change connection strength or learn over time it is a modification of the standard hebb s rule see hebbian learning that through multiplicative normalization solves all stability problems and generates an algorithm for principal components analysis this is a computational form of an effect which is believed to happen in biological neurons theory oja s rule requires a number of simplifications to derive but in its final form it is demonstrably stable unlike hebb s rule it is a single neuron special case of the generalized hebbian algorithm however oja s rule can also be generalized in other ways to varying degrees of stability and success formula oja s rule defines the change in presynaptic weights given the output response formula of a neuron to its inputs to be where is the learning rate which can also change with time note that the bold symbols are vectors and defines a discrete time iteration the rule can also be made for continuous iterations as derivation the simplest learning rule known is hebb s rule which states in conceptual terms that neurons that fire together wire together in component form as a difference equation it is written or with implicit dependence where is again the output this time explicitly dependent on its input vector hebb s rule has synaptic weights approaching infinity with a positive learning rate we can stop this by normalizing the weights so that each weight s magnitude is restricted between corresponding to no weight and corresponding to being the only input neuron with any weight mathematically this takes the form note that in oja s original paper corresponding to quadrature root sum of squares which is the familiar cartesian normalization rule however any type of normalization even linear will give the same result without loss of generality our next step is to expand this into a taylor series for a small learning rate formula giving for small our higher order terms go to zero we again make the specification of a linear neuron that is the output of the neuron is equal to the sum of the product of each input and its synaptic weight or we also specify that our weights normalize to which will be a necessary condition for stability so which when substituted into our expansion gives oja s rule or stability and pca in analyzing the convergence of a single neuron evolving by oja s rule one extracts the first principal component or feature of a data set furthermore with extensions using the generalized hebbian algorithm one can create a multi oja neural network that can extract as many features as desired allowing for principal components analysis a principal component is extracted from a dataset through some associated vector or and we can restore our original dataset by taking in the case of a single neuron trained by oja s rule we find the weight vector converges to or the first principal component as time or number of iterations approaches infinity we can also define given a set of input vectors that its correlation matrix has an associated eigenvector given by with eigenvalue the variance of outputs of our oja neuron then converges with time iterations to the principal eigenvalue or these results are derived using lyapunov function analysis and they show that oja s neuron necessarily converges on strictly the first principal component if certain conditions are met in our original learning rule most importantly our learning rate is allowed to vary with time but only such that its sum is divergent but its power sum is convergent that is our output activation function is also allowed to be nonlinear and nonstatic but it must be continuously differentiable in both and and have derivatives bounded in time applications oja s rule was originally described in oja s paper but the principle of self organization to which it is applied is first attributed to alan turing in pca has also had a long history of use before oja s rule formalized its use in network computation in the model can thus be applied to any problem of self organizing mapping in particular those in which feature extraction is of primary interest therefore oja s rule has an important place in image and speech processing it is also useful as it expands easily to higher dimensions of processing thus being able to integrate multiple outputs quickly a canonical example is its use in binocular vision biology and oja s subspace rule there is clear evidence for both long term potentiation and long term depression in biological neural networks along with a normalization effect in both input weights and neuron outputs however while as of yet there is no direct experimental evidence of oja s rule active in a biological neural network a biophysical derivation of a generalization of the rule is possible such a derivation requires retrograde signalling from the postsynaptic neuron which is biologically plausible see neural backpropagation and takes the form of where as before is the synaptic weight between the th input and th output neurons is the input is the postsynaptic output and we define to be a constant analogous the learning rate and and are presynaptic and postsynaptic functions that model the weakening of signals over time note that the angle brackets denote the average and the operator is a convolution by taking the pre and post synaptic functions into frequency space and combining integration terms with the convolution we find that this gives an arbitrary dimensional generalization of oja s rule known as oja s subspace namely 
the soliton model in neuroscience is a recently developed model that attempts to explain how signals are conducted within neurons it proposes that the signals travel along the cell s membrane in the form of certain kinds of sound or density pulses known as solitons as such the model presents a direct challenge to the widely accepted model which proposes that signals travel as action potentials voltage gated ion channels in the membrane open and allow ions to rush into the cell thereby leading to the opening of other nearby ion channels and thus propagating the signal in an essentially electrical manner history the soliton model was developed beginning in by thomas heimburg and andrew d jackson both at the niels bohr institute of the university of copenhagen heimburg heads the institute s membrane biophysics group and as of early all published articles on the model come from this group justification the model starts with the observation that cell membranes always have a freezing point the temperature below which the consistency changes from fluid to gel like only slightly below the organism s body temperature and this allows for the propagation of solitons it has been known for several decades that an action potential traveling along a neuron results in a slight increase in temperature followed by a decrease in temperature no net heat is released during the overall pulse the decrease during the second phase of the action potential is not explained by the model electrical charges traveling through a resistor always produce heat but traveling solitons do not lose energy in this way and the observed temperature profile is consistent with the soliton model further it has been observed that a signal traveling along a neuron results in a slight local thickening of the membrane and a force acting outwards this effect is not explained by the model but is clearly consistent with the soliton model it is undeniable that an electrical signal can be observed when an action potential propagates along a neuron the soliton model explains this as follows the traveling soliton locally changes density and thickness of the membrane and since the membrane contains many charged and polar substances this will result in an electrical effect akin to piezoelectricity formalism the soliton representing the action potential of nerves is the solution of the partial differential equation formula where formula is time and formula is the position along the nerve axon formula is the change in membrane density under the influence of the action potential formula is the sound velocity of the nerve membrane formula and formula describe the nature of the phase transition and thereby the nonlinearity of the elastic constants of the nerve membrane the parameters formula formula and formula are dictated by the thermodynamic properties of the nerve membrane and cannot be adjusted freely they have to be determined experimentally the parameter formula describes the frequency dependence of the sound velocity of the membrane dispersion relation the above equation does not contain any fit parameters it is formally related to the boussinesq approximation water waves for solitons in water canals the solutions of the above equation possess a limiting maximum amplitude and a minimum propagation velocity that is similar to the pulse velocity in myelinated nerves there also exist periodic solutions that display hyperpolarization and refractory periods role of ion channels the soliton model explains several aspects of the action potential which are not explained by since it is of thermodynamic nature it does not address the properties of single macromolecules like ion channel proteins on a molecular scale it is rather assumed that their properties are implicitly contained in the macroscopic thermodynamic properties of the nerve membranes the model is therefore neither in conflict with the action of membrane proteins nor with pharmacology the soliton model predicts membrane current fluctuations during the action potential these currents are of similar appearance as those reported for ion channel proteins they are thought to be caused by lipid membrane pores spontaneously generated by the thermal fluctuations which are at the core of the soliton model application to anesthesia the authors claim that their model explains the previously obscure mode of action of numerous anesthetics the meyer overton observation holds that the strength of a wide variety of chemically diverse anesthetics is proportional to their lipid solubility suggesting that they do not act by binding to specific proteins such as ion channels but instead by dissolving in and changing the properties of the lipid membrane dissolving substances in the membrane lowers the membrane s freezing point and the resulting larger difference between body temperature and freezing point inhibits the propagation of solitons by increasing pressure lowering ph or lowering temperature this difference can be restored back to normal which should cancel the action of anesthetics this is indeed observed the amount of pressure needed to cancel the action of an anesthetic of a given lipid solubility can be computed from the soliton model and agrees reasonably well with experimental observations 
polymorphism in biophysics is the aspect of the behaviour of lipids that influences their long range order i e how they aggregate this can be in the form of spheres of lipid molecules micelles pairs of layers that face one another lamellar phase observed in biological system as a lipid bilayer a tubular arrangement hexagonal or various cubic phases and being those discovered so far more complicated aggregations have also been observed rhombohedral tetragonal and orthorhombic phases have been observed it forms an important part of current academic research in the fields of membrane biophysics polymorphism biochemistry biological impact and organic chemistry synthesis determination of the topology of a lipid system is possible by a number of methods the most reliable of which is x ray diffraction this uses a beam of x rays that are scattered by the sample giving a diffraction pattern as a set of rings the ratio of the distances of these rings from the central point indicates which phase s are present the structural phase of the aggregation is influenced by the ratio of lipids present temperature hydration pressure and ionic strength and type hexagonal phases in the lipid polymorphism in lipid polymorphism if the packing ratio of lipids is greater or less than one lipid membranes can form two separate hexagonal phases or nonlamellar phases in which long tubular aggregates form according to the environment the lipid is introduced hexagonal i phase hi this phase is favored in detergent in water solutions and has a packing ratio of less than one the micellar population in a detergent water mixture cannot increase without limit as the detergent to water ratio increases therefore in the presence of low amounts of water lipids that would normally form micelles actually form a larger aggregate in the form of micellar tubules in order to satisfy requirements of the hydrophobic effect these aggregates can be thought of micelles that are fused together these tubes have polar head groups facing out and the hydrophobic hydrocarbon chains facing the interior this phase is only seen under unique specialized conditions and most likely is not relevant for biological membranes hexagonal ii phase hii lipid molecules in the hii phase pack inversely to the packing observed in the hexagonal i phase described above this phase has the polar head groups on the inside and the hydrophobic hydrocarbon tails on the outside in solution the packing ratio for this phase is less than one which is synonymous with an inverse cone packing extended arrays of long tubes as in the hexagonal i phase and due to the nature of the polar head groups packing aqueous channels are formed these arrays can stack together like pipes this way of packing may leave a finite hydrophobic surface in contact with water on the outside of the array however the otherwise energetically favorable packing apparently stabilizes this phase as a whole it is also possible that an outer monolayer of lipid coats the surface of the collection of tubes to protect the hydrophobic surface from interaction with the aqueous phase it is suggested that this phase is formed by lipids in solution in order to compensate for the hydrophobic effect this structures tight packing of the lipid head groups reduces their contact with the aqueous phase this in turn reduces the amount of ordered but unbound water molecules the most common lipids that form this phase include phospatidylethanolamine pe which has unsaturated hydrocarbon chains diphosphatidylglycerol dpg in the presence of calcium is also capable of forming this phase techniques for detection there are several techniques used to map out which phase is present during perturbations done on the lipid these perturbations include ph changes temperature changes pressure changes volume changes etc the most common technique used to study phospholipid phase presence is phosphorus nuclear magnetic resonance nmr in this technique different and unique powder diffraction patterns are observed for lamellar hexagonal and isotropic phases other techniques that are used and do offer definitive evidence of existence of lamellar and hexagonal phases include freeze fracture electron microscopy x ray diffraction differential scanning calorimetry dsc and deuterium nuclear magnetic resonance nmr 
membrane biophysics is the study of biological membranes using physical computational mathematical and biophysical methods references membrane biophysics is the use of physical methods applied to cell membrane functions 
the term k mer or x mer where x can be virtually any consonant of choice usually refers to a specific n tuple or n gram of nucleic acid or amino acid sequences that can be used to identify certain regions within biomolecules like dna e g for gene prediction or proteins either k mer strings as such can be used for finding regions of interest or k mer statistics giving discrete probability distributions of a number of possible k mer combinations or rather permutations with repetitions are used specific short k mers are called oligomers or oligos for short 
bioelectromagnetics is a peer reviewed scientific journal published by wiley liss that specializes in articles about the biological effects from and applications of electromagnetic fields in biology and medicine it is the official journal of the bioelectromagnetics society the european bioelectromagnetics association and the society for physical regulation in biology and medicine abstracting and indexing the journal is abstracted and indexed in medline searchable via pubmed and indexed in index medicus see also bioelectrochemistry journal 
the flory convention for defining the variables involved on modeling the position vectors of atoms in macromolecules it is often necessary to convert from cartesian coordinates x y z to generalized coordinates it is named after nobel prize winning paul flory as an example of its use a peptide bond can be described by the x y z positions of every atom in this bond or the flory convention can be used here one must know the bond lengths formula bond angles formula and the dihedral angles formula note that the sequence of dihedral angles is specified using angle as trans applying a vector conversion from the cartesian coordinates to the generalized coordinates will describe the same three dimensional structure using the flory convention 
quantum biology refers to applications of quantum mechanics to biological objects and problems usually it is taken to refer to applications of the non trivial quantum features such as superposition nonlocality entanglement and tunneling as opposed to the trivial applications such as chemical bonding which apply to biology only indirectly by dictating quantum chemistry erwin is one of the first scientists to suggest a study of quantum biology in his book what is life applications many biological processes involve the conversion of energy into forms that are usable for chemical transformations and are quantum mechanical in nature such processes involve chemical reactions light absorption formation of excited electronic states transfer of excitation energy and the transfer of electrons and protons hydrogen ions in chemical processes such as photosynthesis and cellular respiration quantum biology uses computation to model biological interactions in light of quantum mechanical effects some examples of the biological phenomena that have been studied in terms of quantum processes are the absorbance of frequency specific radiation i e photosynthesis and vision the conversion of chemical energy into motion magnetoreception in animals and brownian motors in many cellular processes recent studies have identified quantum coherence and entanglement between the excited states of different pigments in the light harvesting stage of photosynthesis although this stage of photosynthesis is highly efficient it remains unclear exactly how or if these quantum effects are relevant biologically 
hosaka cohen transformation also called h c transformation is a mathematical method of converting a particular two dimensional scalar magnetic field map to a particular two dimensional vector map the scalar field map is of the component of magnetic field which is normal to a two dimensional surface of a volume conductor this volume conductor contains the currents producing the magnetic field the resulting vector map sometimes called roughly mimics those currents under the surface which are parallel to the surface which produced the field therefore the purpose in performing the transformation is to allow a rough visualization of the underlying parallel currents the transformation was proposed by cohen and hosaka of the biomagnetism group at mit then was used by hosaka and cohen to visualize the current sources of the magnetocardiogram each arrow is defined as br br br br br br br br br br where z of the local x y z coordinate system is normal to the volume conductor surface x circumflex and y circumflex are unit vectors and bz is the normal component of magnetic field this is a form of dimensional gradient of the scalar quantity bz and is rotated by degrees from the conventional gradient almost any scalar field magnetic or otherwise can be displayed in this way if desired as an aid to the eye to help see the underlying sources of the field 
bcm theory bcm synaptic modification or the bcm rule named for elie bienenstock leon cooper and paul munro is a physical theory of learning in the visual cortex developed in due to its successful experimental predictions the theory is arguably the most accurate model of synaptic plasticity to date the bcm model proposes a sliding threshold for long term potentiation or long term depression induction and states that synaptic plasticity is stabilized by a dynamic adaptation of the time averaged postsynaptic activity according to the bcm model reducing the postsynaptic activity decreases the ltp threshold and increases the ltd threshold the opposite applies to the increase in postsynaptic activity development in donald hebb proposed a working mechanism for memory and computational adaption in the brain now called hebbian learning or the maxim that cells that fire together wire together this law formed the basis of the brain as the modern neural network theoretically capable of turing complete computational complexity and thus became a standard materialist model for the mind however hebb s rule has problems namely that it has no mechanism for connections to get weaker and no upper bound for how strong they can get in other words the model is unstable both theoretically and computationally later modifications gradually improved hebb s rule normalizing it and allowing for decay of synapses where no activity or unsynchronized activity between neurons results in a loss of connection strength new biological evidence brought this activity to a peak in the where theorists formalized various approximations in the theory such as the use of firing frequency instead of potential in determining neuron excitation and the assumption of ideal and more importantly linear synaptic integration of signals that is there is no unexpected behavior in the adding of input currents to determine whether or not a cell will fire these approximations resulted in the basic form of bcm below in but the final step came in the form of mathematical analysis to prove stability and computational analysis to prove applicability culminating in bienenstock cooper and munro s paper since then experiments have shown evidence for bcm behavior in both the visual cortex and the hippocampus the latter of which plays an important role in the formation and storage of memories both of these areas are well studied experimentally but both theory and experiment have yet to establish conclusive synaptic behavior in other areas of the brain furthermore a biological mechanism for synaptic plasticity in bcm has yet to be established theory the basic bcm rule takes the form where formula is the synaptic weight of the formula synapse formula is that synapse s input current formula is the weighted presynaptic output vector formula is the postsynaptic activation function that changes sign at some output threshold formula and formula is the often negligible time constant of uniform decay of all synapses this model is merely a modified form of the hebbian learning rule formula and requires a suitable choice of activation function or rather the output threshold to avoid the hebbian problems of instability this threshold was derived rigorously in bcm noting that with formula and the approximation of the average output formula for one to have stable learning it is sufficient that or equivalently that the threshold formula where formula and formula are fixed positive constants when implemented the theory is often taken such that where angle brackets are a time average and formula is the time constant of selectivity the model has drawbacks as it requires both long term potentiation and long term depression or increases and decreases in synaptic strength something which has not been observed in all cortical systems further it requires a variable activation threshold and depends strongly on stability of the selected fixed points formula and formula however the model s strength is that it incorporates all these requirements from independently derived rules of stability such as normalizability and a decay function with time proportional to the square of the output experiment the first major experimental confirmation of bcm came in in investigating ltp and ltd in the hippocampus the data showed qualitative agreement with the final form of the bcm activation function this experiment was later replicated in the visual cortex which bcm was originally designed to model this work provided further evidence of the necessity for a variable threshold function for stability in hebbian type learning bcm or others experimental evidence has been non specific to bcm until rittenhouse et al confirmed bcm s prediction of synapse modification in the visual cortex when one eye is selectively closed specifically where formula describes the variance in spontaneous activity or noise in the closed eye and formula is time since closure experiment agreed with the general shape of this prediction and provided an explanation for the dynamics of monocular eye closure monocular deprivation versus binocular eye closure the experimental results are far from conclusive but so far have favored bcm over competing theories of plasticity applications while the algorithm of bcm is too complicated for large scale parallel distributed processing it has been put to use in lateral networks with some success furthermore some existing computational network learning algorithms have been made to correspond to bcm learning 
a biological neuron model also known as spiking neuron model is a mathematical description of the properties of nerve cells or neurons that is designed to accurately describe and predict biological processes this is in contrast to the artificial neuron which aims for computational effectiveness although these goals sometimes overlap artificial neuron abstraction the most basic model of a neuron consists of an input with some synaptic weight vector and an activation function or transfer function inside the neuron determining output this is the basic structure used in artificial neurons which in a neural network often looks like formula where is the output of the th neuron is the th input neuron signal is the synaptic weight between the neurons and and is the activation function some of the earliest biological models took this form until kinetic models such as the hodgkin huxley model became dominant biological abstraction in the case of modelling a biological neuron physical analogues are used in place of abstractions such as weight and transfer function the input to a neuron is often described by an ion current through the cell membrane that occurs when neurotransmitters cause an activation of ion channels in the cell we describe this by a physical time dependent current the cell itself is bound by an insulating cell membrane with a concentration of charged ions on either side that determines a capacitance finally a neuron responds to such a signal with a change in voltage or an electrical potential energy difference between the cell and its surroundings which is observed to sometimes result in a voltage spike called an action potential this quantity then is the quantity of interest and is given by integrate and fire one of the earliest models of a neuron was first investigated in by louis lapicque a neuron is represented in time by which is just the time derivative of the law of capacitance when an input current is applied the membrane voltage increases with time until it reaches a constant threshold at which point a delta function spike occurs and the voltage is reset to its resting potential after which the model continues to run the firing frequency of the model thus increases linearly without bound as input current increases the model can be made more accurate by introducing a refractory period that limits the firing frequency of a neuron by preventing it from firing during that period through some calculus involving a fourier transform the firing frequency as a function of a constant input current thus looks like a remaining shortcoming of this model is that it implements no time dependent memory if the model receives a below threshold signal at some time it will retain that voltage boost forever until it fires again this characteristic is clearly not in line with observed neuronal behavior leaky integrate and fire in the leaky integrate and fire model the memory problem is solved by adding a leak term to the membrane potential reflecting the diffusion of ions that occurs through the membrane when some equilibrium is not reached in the cell the model looks like where is the membrane resistance as we find it is not a perfect insulator as assumed previously this forces the input current to exceed some threshold in order to cause the cell to fire else it will simply leak out any change in potential the firing frequency thus looks like which converges for large input currents to the previous leak free model with refractory period hodgkin huxley each current is given by ohm s law as where is the conductance or inverse resistance which can be expanded in terms of its constant average and the activation and inactivation fractions and respectively that determine how many ions can flow through available membrane channels this expansion is given by and our fractions follow the first order kinetics with similar dynamics for where we can use either and or and to define our gate fractions with such a form all that remains is to individually investigate each current one wants to include typically these include inward and na input currents and several varieties of k outward currents including a leak current the end result can be at the small end parameters which one must estimate or measure for an accurate model and for complex systems of neurons not easily tractable by computer careful simplifications of the hodgkin huxley model are therefore needed fitzhugh nagumo sweeping simplifications to hodgkin huxley were introduced by fitzhugh and nagumo in and seeking to describe regenerative self excitation by a nonlinear positive feedback membrane voltage and recovery by a linear negative feedback gate voltage they developed the model described by where we again have a membrane like voltage and input current with a slower general gate voltage and experimentally determined parameters although not clearly derivable from biology the model allows for a simplified immediately available dynamic without being a trivial simplification morris lecar in morris and lecar combined hodgkin huxley and fitzhugh nagumo into a voltage gated calcium channel model with a delayed rectifier potassium channel represented by where formula hindmarsh rose with and so that the variable only changes very slowly this extra mathematical complexity allows a great variety of dynamic behaviors for the membrane potential described by the variable of the model which include chaotic dynamics this makes the hindmarsh rose neuron model very useful because being still simple allows a good qualitative description of the many different patterns of the action potential observed in experiments expanded neuron models while the success of integrating and kinetic models is undisputed much has to be determined experimentally before accurate predictions can be made the theory of neuron integration and firing response to inputs is therefore expanded by accounting for the nonideal conditions of cell structure cable theory cable theory describes the dendritic arbor as a cylindrical structure undergoing a regular pattern of bifurcation like branches in a tree for a single cylinder or an entire tree the input conductance at the base where the tree meets the cell body or any such boundary is defined as where is the electrotonic length of the cylinder which depends on its length diameter and resistance a simple recursive algorithm scales linearly with the number of branches and can be used to calculate the effective conductance of the tree this is given by where is the total surface area of the tree of total length and is its total electrotonic length for an entire neuron in which the cell body conductance is and the membrane conductance per unit area is we find the total neuron conductance for dendrite trees by adding up all tree and soma conductances given by where we can find the general correction factor experimentally by noting compartmental models the cable model makes a number of simplifications to give closed analytic results namely that the dendritic arbor must branch in diminishing pairs in a fixed pattern a compartmental model allows for any desired tree topology with arbitrary branches and lengths but makes simplifications in the interactions between branches to compensate thus the two models give complementary results neither of which is necessarily more accurate each individual piece or compartment of a dendrite is modeled by a straight cylinder of arbitrary length and diameter which connects with fixed resistance to any number of branching cylinders we define the conductance ratio of the th cylinder as where formula and is the resistance between the current compartment and the next we obtain a series of equations for conductance ratios in and out of a compartment by making corrections to the normal dynamic as where the last equation deals with parents and daughters at branches and formula we can iterate these equations through the tree until we get the point where the dendrites connect to the cell body soma where the conductance ratio is then our total neuron conductance is given by synaptic transmission where is the maximal conductance around and is the equilibrium potential of the given ion or transmitter amda nmda cl or k while describes the fraction of receptors that are open for nmda there is a significant effect of magnesium block that depends sigmoidally on the concentration of intracellular magnesium by for gabab is the concentration of the g protein and describes the dissociation of g in binding to the potassium gates the dynamics of this more complicated model have been well studied experimentally and produce important results in terms of very quick synaptic potentiation and depression that is fast short term learning other conditions the models above are still idealizations corrections must be made for the increased membrane surface area given by numerous dendritic spines temperatures significantly hotter than room temperature experimental data and nonuniformity in the cell s internal structure many problems in the temperature and geometry dynamics of the cell during action potential propagation as well as problems in explaining some pharmacology are still unsolved some of which have required unorthodox new models such as the soliton model to explain 
some authors use the values range from to sources envi users guide john gamon penuelas j and field c b a narrow waveband spectral index that tracks diurnal changes in photosynthetic efficiency remote sensing of environment drolet g g heummrich k f hall f g middleton e m black t a barr a g and margolis h a a modis derived photochemical reflectance index to detect inter annual variations in the photosynthetic light use efficiency of a boreal deciduous forest remote sensing of environment 
computational epigenetics uses bioinformatic methods to complement experimental research in epigenetics due to the recent explosion of epigenome datasets computational methods play an increasing role in all areas of epigenetic research definition research in computational epigenetics comprises the development and application of bioinformatic methods for solving epigenetic questions as well as computational data analysis and theoretical modeling in the context of epigenetics current research areas epigenetic data processing and analysis various experimental techniques have been developed for genome wide mapping of epigenetic information the most widely used being chip on chip chip seq and bisulfite sequencing all of these methods generate large amounts of data and require efficient ways of data processing and quality control by bioinformatic methods epigenome prediction a substantial amount of bioinformatic research has been devoted to the prediction of epigenetic information from characteristics of the genome sequence such predictions serve a dual purpose first accurate epigenome predictions can substitute for experimental data to some degree which is particularly relevant for newly discovered epigenetic mechanisms and for species other than human and mouse second prediction algorithms build statistical models of epigenetic information from training data and can therefore act as a first step toward quantitative modeling of an epigenetic mechanism successful computational prediction of dna and lysine methylation and acetylation has been achieved by combinations of various features applications in cancer epigenetics the important role of epigenetic defects for cancer opens up new opportunities for improved diagnosis and therapy these active areas of research give rise to two questions that are particularly amenable to bioinformatic analysis first given a list of genomic regions exhibiting epigenetic differences between tumor cells and controls or between different disease subtypes can we detect common patterns or find evidence of a functional relationship of these regions to cancer second can we use bioinformatic methods in order to improve diagnosis and therapy by detecting and classifying important disease subtypes emerging topics the first wave of research in the field of computational epigenetics was driven by rapid progress of experimental methods for data generation which required adequate computational methods for data processing and quality control prompted epigenome prediction studies as a means of understanding the genomic distribution of epigenetic information and provided the foundation for initial projects on cancer epigenetics while these topics will continue to be major areas of research and the mere quantity of epigenetic data arising from epigenome projects poses a significant bioinformatic challenge several additional topics are currently emerging 
slow after hyperpolarisation sahp refers to a prolonged period of hyperpolarisation in a neuron or cardiomyocyte following an action potential or other depolarising spike in neural circuitry a train of action potentials may be required to induce a sahp this is unlike fast ahps which require no more than a single action potential sahps are due to an extended potassium flux via calcium activated potassium channels ikca the resulting hyperpolarisation lasts for several seconds in a sahp and effectively inhibits neural activity for this period fast and medium ahps have shorter periods see also calcium activated potassium channel 
where the model has six parameters a b r s xr and i it is very common to fix some of them and let the other to be control parameters usually the parameter i which means the current that enters the neuron is taken as a control parameter other control parameters used often in the literature are a b or r the first two modeling the working of the fast ion channels and the last one the slow ion channels respectively very frequently the parameters held fixed are s and xr when a or b are fixed the values given are a and b the parameter r is something of the order of and i ranges between and allows a great variety of dynamic behaviors of the membrane potential described by variable x including unpredictable behavior which is referred to as chaotic dynamics this makes the model very useful because it is relatively simple and provides a good qualitative description of the many different patterns that are observed empirically 
quantum aspects of life is a science text with a foreword by sir roger penrose which explores the open question of the role of quantum mechanics at molecular scales of relevance to biology the book adopts a debate like style and contains chapters written by various world experts giving rise to a mix of both skeptical and sympathetic viewpoints the book addresses questions of quantum physics biophysics nanoscience quantum chemistry mathematical biology complexity theory and philosophy that are inspired by the seminal book what is life by erwin 
tethered particle motion tpm is a biophysical method that is used for studying various polymers such as dna and their interaction with other entities such as proteins the method allows observers to measure various physical properties on the substances as well as to measure the properties of biochemical interactions with other substances such as proteins and enzymes tpm is a single molecule experiment method history tpm was first introduced by schafer gelles sheetz and landick in in their research they attached rna polymerase to the surface and gold beads were attached to one end of the dna molecules in the beginning the rna polymerase captures the dna near the gold bead during the transcription the dna slides on the rna polymerase so the distance between the rna polymerase and the gold bead the tether length is increased using an optical microscope the area that the bead moves in was detected the transcription rate was extracted from data segall d e et al volume exclusion effects in tethered particle experiments bead size matters physical review letters p ref will be less than n r equiv frac r sqrt l cdot l p math where formula is the bead radius formula is the contour length of the polymer and formula is the persistence length in physiological conditions of the polymer it is possible to work also when formula but it should be treated carefully bead types all of the bead types and diameters with the biochemistry marker look at the tether assembly section are manufactured by commercial companies and can purchased easily chip and tether assembly chip assembly a chip can be made of two coverslips one of them should be drilled to make two hole allowing the reagents to be injected into the flowcell the slides should be cleaned to remove dirt a bath sonicator is a good tool for that minutes in isopropanol should do the trick next the a channel should be made one way of doing so is to cut parafilm in the center leaving a frame of parafilm that would be used as a spacer between the slides the slides should be assembled one on the other with the cut parafilm between them the final step is to heat the chip so that the parafilm will melt and glue the slides together tether assembly first the chip has to be passivated so that the polymer won t stick to the glass there are plenty of blocking reagents available bsa alpha casein etc and one should find what works best for the specific situation next the surface should be coated with an antibody or other reactive molecule such as anti digoxigenin that will bind to an antigen digoxigenin at one end of the polymer after an incubation of about the excess antibody has to be washed away after washing the excess antibody the polymer should be injected into the chip and incubated for about the same time the polymer had been modified before at the ends one end has a biotin tail and the other has a digoxigenin tail after incubation unbound polymer has to be washed out from the cell then anti biotin coated beads should be injected to the flowcell and incubate for about excess beads should be washed out data analysis tracking as mentioned above the image doesn t show the bead itself but a larger spot according to its psf point spread function in addition the pixel size on the camera may reduce the resolution of the measure vec r cm frac i tot cdot sum k n i k cdot vec r k math p x dx sqrt frac pi ll p exp left frac p right dx p r dr pi r frac pi ll p exp left frac p right dr math l math is the contour length and formula is the persistence length p x propto exp left frac frac kx k bt right math displaystyle k alpha k bt math where formula is the coefficient of formula from the parabola fit 
the phenomenon of macromolecular crowding alters the properties of molecules in a solution when high concentrations of macromolecules such as proteins are present such conditions occur routinely in living cells for instance the cytosol of escherichia coli contains about milligrammes per millilitre mg ml of macromolecules crowding occurs since these high concentrations of macromolecules reduce the volume of solvent available for other molecules in the solution which has the result of increasing their effective concentrations this crowding effect can make molecules in cells behave in radically different ways than in test tube assays consequently measurements of the properties of enzymes or processes in metabolism that are made in the laboratory in dilute solutions may be different by many orders of magnitude from the true values seen in living cells the study of biochemical processes under realistically crowded conditions is very important since these conditions are a ubiquitous property of all cells and crowding may be essential for the efficient operation of metabolism cause and effects the interior of cells is a crowded environment for example an escherichia coli cell is only about long and in diameter with a cell volume of however e coli can contain up to different types of proteins and about of these types are produced at a high enough level to be easily detected added to this mix are various forms of rna and the cell s dna chromosome giving a total concentration of macromolecules of between to ml in eukaryotes the cell s interior is further crowded by the protein filaments that make up the cytoskeleton this meshwork divides the cytosol into a network of narrow pores these high concentrations of macromolecules occupy a large proportion of the volume of the cell which reduces the volume of solvent that is available for other macromolecules this excluded volume effect increases the effective concentration of macromolecules increasing their chemical activity which in turn alters the rates and equilibrium constants of their reactions in particular this effect alters dissociation constants by favoring the association of macromolecules such as when multiple proteins come together to form protein complexes or when dna binding proteins bind to their targets in the genome crowding may also affect enzyme reactions involving small molecules if the reaction involves a large change in the shape of the enzyme the size of the crowding effect depends on both the molecular mass and shape of the molecule involved although mass seems to be the major factor with the effect being stronger with larger molecules notably the size of the effect is non linear so macromolecules are much more strongly affected than are small molecules such as amino acids or simple sugars macromolecular crowding is therefore an effect exerted by large molecules on the properties of other large molecules importance macromolecular crowding is an important effect in biochemistry and cell biology for example the increase in the strength of interactions between proteins and dna produced by crowding may be of key importance in processes such as transcription and dna replication crowding has also been suggested to be involved in processes as diverse as the aggregation of hemoglobin in sickle cell disease and the responses of cells to changes in their volume the importance of crowding in protein folding is of particular interest in biophysics here the crowding effect can accelerate the folding process since a compact folded protein will occupy less volume than an unfolded protein chain however crowding can reduce the yield of correctly folded protein by increasing protein aggregation crowding may also increase the effectiveness of chaperone proteins such as groel in the cell which could counteract this reduction in folding efficiency it has also been shown that macromolecular crowding affects protein folding dynamics as well as overall protein shape where distinct conformational changes are accompanied by secondary structure alterations implying that crowding induced shape changes may be important for protein function and malfunction in vivo a particularly striking example of the importance of crowding effects involves the crystallins that fill the interior of the lens these proteins have to remain stable and in solution for the lens to be transparent precipitation or aggregation of crystallins causes cataracts crystallins are present in the lens at extremely high concentrations over ml and at these levels crowding effects are very strong the large crowding effect adds to the thermal stability of the crystallins increasing their resistance to denaturation this effect may partly explain the extraordinary resistance shown by the lens to damage caused by high temperatures study due to macromolecular crowding enzyme assays and biophysical measurements performed in dilute solution may fail to reflect the actual process and its kinetics taking place in the cytosol one approach to produce more accurate measurements would be to use highly concentrated extracts of cells to try to maintain the cell contents in a more natural state however when using such extracts it is very difficult to study one process in isolation consequently the crowded effects may be mimicked in vitro by adding high concentrations of an inert molecule such as polyethylene glycol or ficoll to assays containing purified components however using such artificial crowding agents can be complicated as these crowding molecules can sometimes interact in other ways with the process being examined such as by binding weakly to one of the components 
membrane fusion is a key biophysical process that is essential for the functioning of life itself it is defined as the event where two lipid bilayers approach each other and then merge to form a single continuous structure in living beings cells are made of an outer coat made of lipid bilayers which then cause fusion to take place in events such as fertilization embryogenesis and even infections by various types of bacteria and viruses it is therefore an extremely important event to study from an evolutionary angle fusion is an extremely controlled phenomenon random fusion can result in severe problems to the normal functioning of the human body regardless of the complexity of the system fusion essentially occurs due to the interplay of various interfacial forces namely hydration repulsion hydrophobic attraction and van der waals forces it is interesting to note here that in some cases fusion is also mediated by proteins nevertheless the interfacial forces mentioned above act in parallel with proteins on the cell surface to cause fusion hence the underlying forces of membrane fusion essentially remain the same toc inter bilayer forces lipid bilayers are structures of lipid molecules consisting of a hydrophobic tail and a hydrophilic head group therefore these structures experience all the characteristic interbilayer forces involved in that regime hydration repulsion two hydrated bilayers experience strong repulsion as they approach each other these forces have been measured using the surface forces apparatus s f a an instrument used for measuring forces between surfaces this repulsion was first proposed by langmuir and was thought to arise due to water molecules that hydrate the bilayers hydration repulsion can thus be defined as the work required in removing the water molecules around hydrophilic molecules like lipid head groups in the bilayer system as water molecules have an affinity towards hydrophilic head groups they try to arrange themselves around the head groups of the lipid molecules and it becomes very hard to separate this favorable combination experiments performed through sfa have confirmed that the nature of this force is an exponential decline the potential vr is given by where cr is a measure of the hydration interaction energy for hydrophilic molecules of the given system is a characteristic length scale of hydration repulsion and z is the distance of separation in other words it is on distances up to this length that molecules surfaces fully experience this repulsion hydrophobic attraction hydrophobic forces are the attractive forces between any two hydrophobic groups in aqueous media e g the forces between two long hydrocarbon chains in aqueous solutions the magnitude of these forces depends on the hydrophobicity of the interacting groups as well as the distance separating them they are found to decrease roughly exponentially with the distance the physical origin of these forces is a debated issue but they have been found to be long ranged and are the strongest among all the physical interaction forces operating between biological surfaces and molecules due to their long range nature they are responsible for rapid coagulation of hydrophobic particles in water and play important roles in various biological phenomena including folding and stabilization of macromolecules such as proteins and fusion of cell membranes the potential va is given by where ca is a measure of the hydrophobic interaction energy for the given system is a characteristic length scale of hydrophobic attraction and z is the distance of separation van der waals forces in bilayers these forces arise due to dipole dipole interactions induced permanent between molecules of bilayers as molecules come closer this attractive force arises due to the ordering of these dipoles like in the case of magnets that align and attract each other as they approach this also implies that any surface would experience a van der waals attraction in bilayers the form taken by van der waals interaction potential vvdw is given by where h is the hamaker constant and d and z are the bilayers thickness and the distance of separation respectively background for fusion to take place it has to overcome huge repulsive forces due to the strong hydration repulsion between hydrophilic lipid head groups however it has been hard to exactly determine the connection between adhesion fusion and interbilayer forces the forces that promote cell adhesion are not the same as the ones that promote membrane fusion studies show that by creating a stress on the interacting bilayers fusion can be achieved without disrupting the interbilayer interactions it has also been suggested that membrane fusion takes place through a sequence of structural rearrangements that help to overcome the barrier that prevents fusion thus interbilayer fusion takes place through interbilayer interactions during membrane fusion when two lipid bilayers approach each other they experience weak van der waals attractive forces and much stronger repulsive forces due to hydration repulsion these forces are normally dominant over the hydrophobic attractive forces between the membranes studies done on membrane bilayers using surface forces apparatus sfa indicate that membrane fusion can instantaneously occur when two bilayers are still at a finite distance from each other without them having to overcome the short range repulsive force barrier this is attributed to the molecular rearrangements that occur resulting in the bypassing of these forces by the membranes during fusion the hydrophobic tails of a small patch of lipids on the cell membrane are exposed to the aqueous phase surrounding them this results in very strong hydrophobic attractions which dominate the repulsive force between the exposed groups leading to membrane fusion the attractive van der waals forces play a negligible role in membrane fusion thus fusion is a result of the hydrophobic attractions between internal hydrocarbon chain groups that are exposed to the normally inaccessible aqueous environment fusion is observed to start at points on the membranes where the membrane stresses are either the weakest or the strongest applications interbilayer forces play a key role in mediating membrane fusion which has extremely important biomedical applications 
a comparator system or simply comparator in the fields of biophysics biology and neurology is a term used to describe particular organisations of neurons 
fusion is the process by which two initially distinct lipid bilayers merge their hydrophobic cores resulting in one interconnected structure if this fusion proceeds completely through both leaflets of both bilayers an aqueous bridge is formed and the internal contents of the two structures can mix alternatively if only one leaflet from each bilayer is involved in the fusion process the bilayers are said to be hemifused in hemifusion the lipid constituents of the outer leaflet of the two bilayers can mix but the inner leaflets remain distinct the aqueous contents enclosed by each bilayer also remain separated fusion is involved in many cellular processes particularly in eukaryotes since the eukaryotic cell is extensively sub divided by lipid bilayer membranes exocytosis fertilization of an egg by sperm and transport of waste products to the lysozome are a few of the many eukaryotic processes that rely on some form of fusion fusion is also an important mechanism for transport of lipids from their site of synthesis to the membrane where they are needed even the entry of pathogens can be governed by fusion as many bilayer coated viruses have dedicated fusion proteins to gain entry into the host cell lipid mechanism there are four fundamental steps in the fusion process although each of these steps actually represents a complex sequence of events first the involved membranes must aggregate approaching each other to within several nanometers second the two bilayers must come into very close contact within a few angstroms to achieve this close contact the two surfaces must become at least partially dehydrated as the bound surface water normally present causes bilayers to strongly repel at this distance third a destabilization must nucleate at one point between the two bilayers inducing a highly localized rearrangement of the two bilayers finally as this point defect grows the components of the two bilayers mix and diffuse away from the site of contact depending on whether hemifusion or full fusion occurs the internal contents of the membranes may mix at this point as well the exact mechanisms behind this complex sequence of events are still a matter of debate to simplify the system and allow more definitive study many experiments have been performed in vitro with synthetic lipid vesicles these studies have shown that divalent cations play a critical role in the fusion process by binding to negatively charged lipids such as phosphatidylserine phosphatidylglycerol and cardiolipin one role on these ions in the fusion process is to shield the negative charge on the surface of the bilayer diminishing electrostatic repulsion and allowing the membranes to approach each other this is clearly not the only role however since there is an extensively documented difference in the ability of versus to induce fusion although will induce extensive aggregation it will not induce fusion while induces both it has been proposed that this discrepancy is due to a difference in extent of dehydration under this theory calcium ions bind more strongly to charged lipids but less strongly to water the resulting displacement of calcium for water destabilizes the lipid water interface and promotes intimate interbilayer contact a recently proposed alternative hypothesis is that the binding of calcium induces a destabilizing lateral tension whatever the mechanism of calcium induced fusion the initial interaction is clearly electrostatic since zwitterionic lipids are not susceptible to this effect the role of lipid headgroup in the fusion process extends beyond charge density and can affect dehydration and defect nucleation independent of the effect of ions the presence of the uncharged headgroup phosphatidylethanolamine pe increases fusion when incorporated into a phosphatidylcholine bilayer this phenomenon has been explained by some as a dehydration effect similar to the influence of calcium the pe headgroup binds water less tightly than pc and therefore may allow close apposition more easily an alternate explanation is that the physical rather than chemical nature of pe may help induce fusion according to the stalk hypothesis of fusion a highly curved bridge must form between the two bilayers for fusion to occur since pe has a small headgroup and readily forms inverted micelle phases it should according to the stalk model promote the formation of these stalks further evidence cited in favor of this theory is the fact that certain lipid mixtures have been shown to only support fusion when raised above the transition temperature of these inverted phases this topic also remains controversial and even if there is a curved structure present in the fusion process there is debate in the literature over whether it is a cubic hexagonal or more exotic extended phase fusion proteins the situation is further complicated when considering fusion in vivo since biological fusion is almost always regulated by the action of membrane associated proteins the first of these proteins to be studied were the viral fusion proteins which allow an enveloped virus to insert its genetic material into the host cell enveloped viruses are those surrounded by a lipid bilayer some others have only a protein coat broadly there are two classes of viral fusion proteins acidic and ph independent ph independent fusion proteins can function under neutral conditions and fuse with the plasma membrane allowing viral entry into the cell viruses utilizing this scheme included hiv measles and herpes acidic fusion proteins such as those found on influenza are only activated when in the low ph of acidic endosomes and must first be endocytosed to gain entry into the cell eukaryotic cells use entirely different classes of fusion proteins the best studied of which are the snares snare proteins are used to direct all vesicular intracellular trafficking despite years of study much is still unknown about the function of this protein class in fact there is still an active debate regarding whether snares are linked to early docking or participate later in the fusion process by facilitating hemifusion even once the role of snares or other specific proteins is illuminated a unified understanding of fusion proteins is unlikely as there is an enormous diversity of structure and function within these classes and very few themes are conserved fusion in laboratory practice in studies of molecular and cellular biology it is often desirable to artificially induce fusion although this can be accomplished with the addition of calcium as discussed earlier this procedure is often not feasible because calcium regulates many other biochemical processes and its addition would be a strong confound also as mentioned calcium induces massive aggregation as well as fusion the addition of polyethylene glycol peg causes fusion without significant aggregation or biochemical disruption this procedure is now used extensively for example by fusing b cells with myeloma cells the resulting from this combination expresses a desired antibody as determined by the b cell involved but is immortalized due to the myeloma component the mechanism of peg fusion has not been definitively identified but some researchers believe that the peg by binding a large number of water molecules effectively decreases the chemical activity of the water and thus dehydrates the lipid headgroups fusion can also be artificially induced through electroporation in a process known as electrofusion it is believed that this phenomenon results from the energetically active edges formed during electroporation which can act as the local defect point to nucleate stalk growth between two bilayers assays to measure membrane fusion there are two levels of fusion mixing of membrane lipids and mixing of contents assays of membrane fusion report either the mixing of membrane lipids or the mixing of the aqueous contents of the fused entities assays for measuring lipid mixing assays evaluating lipid mixing make use of concentration dependent effects such as nonradioactive energy transfer fluorescence quenching and pyrene eximer formation in this method membrane labeled with both nbd donor and rhodamine acceptor combine with unlabeled membrane when nbd and rhodamine are within a certain distance the fluorescence resonance energy transfer fret happens after fusion resonance energy transfer fret decreases when the average distance between probes increases while nbd fluorescence increases pyrene monomer and excimer emission wavelengths are different the emission wavelength of monomer is around and that of excimer is around in this method membrane labeled with pyrene combines with unlabeled membrane pyrene self associates in membrane and then excited pyrene excites other pyrene before fusion the majority of the emission is excimer emission after fusion the distance between probes increases and the ratio of excimer emission decreases octadecyl rhodamine b self quenching this assay is based on self quenching of octadecyl rhodamine b octadecyl rhodamine b self quenching occurs when the probe is incorporated into membrane lipids at concentrations of mole percent because rhodamine dimmers quench fluorescence in this method membrane labeled rhodamine combines with unlabeled membrane fusion with unlabeled membranes resulting in dilution of the probe which is accompanies by increasing fluorescence the major problem of this assay is spontaneous transfer assays for measuring content mixing mixing of aqueous contents from vesicles as a result of lysis fusion or physiological permeability can be detected fluorometrically using low molecular weight soluble tracers ants is a polyanionic fluorophore while dpx is a cationic quencher the assay is based on the collisional quenching of them separate vesicle populations are loaded with ants or dpx respectively when content mixing happens ants and dpx collide and fluorescence of ants monitored at with excitation at is quenched this method is performed at acidic ph and high concentration this method is based on the fact that chelate of dpa is times more fluorescent than alone in the dpa assay separate vesicle populations are loaded with or dpa the formation of dpa chelate can be used to indicate vesicle fusion this method is good for protein free membranes single molecule dna assay a dna hairpin composed of base pair stem and poly thymidine loop that is labeled with a donor and an acceptor at the ends of the stem was encapsulated in the v snare vesicle we separately encapsulated multiple unlabeled poly adenosine dna strands in the t snare vesicle if the two vesicles both in diameter dock and a large enough fusion pore forms between them the two dna molecules should hybridize opening up the stem region of the hairpin and switching the resonance energy transfer fret efficiency e between and from a high to a low value 
hydrophobicity scales are values that define relative hydrophobicity of amino acid residues the more positive the value the more hydrophobic are the amino acids located in that region of the protein these scales are commonly used to predict the transmembrane alpha helices of membrane proteins when consecutively measuring amino acids of a protein changes in value indicate attraction of specific protein regions towards the hydrophobic region inside lipid bilayer hydrophobicity and the hydrophobic effect the hydrophobic effect represents the tendency of water to exclude non polar molecules the effect originates from the disruption of highly dynamic hydrogen bonds between molecules of liquid water polar chemical groups such as oh group in methanol do not cause the hydrophobic effect however a pure hydrocarbon molecule for example hexane cannot accept or donate hydrogen bonds to water introduction of hexane into water causes disruption of the hydrogen bonding network between water molecules the hydrogen bonds are partially reconstructed by building a water cage around the hexane molecule similar to that in clathrate hydrates formed at lower temperatures the mobility of water molecules in the cage or solvation shell is strongly restricted this leads to significant losses in translational and rotational entropy of water molecules and makes the process unfavorable in terms of free energy of the system types of amino acid hydrophobicity scales a number of different hydrophobicity scales have been developed there are clear differences between the four scales shown in the table both the second and fourth scales place cysteine as the most hydrophobic residue unlike the other two scales this difference is due to the different methods used to measure hydrophobicity the method used to obtain the janin and rose et al scales was to examine proteins with known d structures and define the hydrophobic character as the tendency for a residue to be found inside of a protein rather than on its surface since cysteine forms disulfide bonds that must occur inside a globular structure cysteine is ranked as the most hydrophobic the first and third scales are derived from the physiochemical properties of the amino acid side chains these scales result mainly from inspection of the amino acid structures biswas et al divided the scales based on the method used to obtain the scale in to five different categories partitioning methods the most common method of measuring amino acid hydrophobicity is partitioning between two immiscible liquid phases different organic solvents are most widely used to mimic the protein interior however organic solvents are slightly miscible with water and the characteristics of both phases change making it difficult to obtain pure hydrophobicity scale nozaki and tanford proposed the first major hydrophobicity scale for nine amino acids ethanol and dioxane are used as the organic solvents and the free energy of transfer of each amino acid was calculated non liquid phases can also be used with partitioning methods such as micellar phases and vapor phases two scales have been developed using micellar phases fendler et al measured the partitioning of radiolabeled amino acids using sodium dodecyl sulfate sds micelles also amino acid side chain affinity for water was measured using vapor phases vapor phases represent the simplest non polar phases because it has no interaction with the solute the hydration potential and its correlation to the appearance of amino acids on the surface of proteins was studied by wolfenden aqueous and polymer phases were used in the development of a novel partitioning scale partitioning methods have many drawbacks first it is difficult to mimic the protein interior in addition the role of self solvation makes using free amino acids very difficult moreover hydrogen bonds that are lost in the transfer to organic solvents are not reformed but often in the interior of protein accessible surface area methods hydrophobicity scales can also be obtained by calculating the solvent accessible surface areas for amino acid residues in the expended polypeptide chain or in alpha helix and multiplying the surface areas by the empirical solvation parameters for the corresponding types of atoms a differential solvent accessible surface area hydrophobicity scale based on proteins as compacted networks near a critical point due to self organization by evolution was constructed based on asymptotic power law self similar behavior this scale is based on a bioinformatic survey of high resolution structures from the protein data bank this differential scale has two comparative advantages it is especially useful for treating changes in water protein interactions that are too small to be accessible to conventional force field calculations and for homologous structures it can yield correlations with changes in properties from mutations in the amino acid sequences alone without determining corresponding structural changes either in vitro or in vivo chromatographic methods reversed phase liquid chromatography rplc is the most important chromatographic method for measuring solute hydrophobicity the non polar stationary phase mimics biological membranes peptide usage has many advantages because partition is not extended by the terminal charges in rplc also secondary structures formation is avoided by suing short sequence peptides derivatization of amino acids is necessary to ease its partition in to a bonded phase another scale had been developed in and used peptide retention on hydrophillic gel butanol and pyridine were used as the mobile phase in this particular scale and glycine was used as the reference value pliska and his cowrkers used thin layer chromatography to relate mobility values of free amino acids to their hydrophobicities about a decade ago another hydrophilicity scale was published this scale used normal phase liquid chromatography and showed the retention of peptides on an amide column the absolute values and relative rankings of hydrophobicity determined by chromatographic methods can be affected by a number of parameters these parameters include the silica surface area and pore diameter the choice and ph of aqueous buffer temperature and the bonding density of stationary phase chains site directed mutagenesis this method use dna recombinant technology and it gives an actual measurement of protein stability in his detailed site directed mutagenesis studies utani and his coworkers substituted amino acids at of the tryptophan synthase and he measured the free energy of unfolding interestingly they found that the increased stability is directly proportional to increase in hydrophobicity up to a certain size limit the main disadvantage of site directed mutagenesis method is that not all the naturally occurring amino acids can substitute a single residue in a protein moreover these methods have cost problems and is useful only for measuring protein stability physical property methods the hydrophobicity scales developed by physical property methods are based on the measurement of different physical properties examples include partial molar heat capacity transition temperature and surface tension physical methods are easy to use and flexible in terms of solute the most popular hydrophobicity scale was developed by measuring surface tension values for the naturally occurring amino acids in nacl solution the main drawbacks of surface tension measurements is that the broken hydrogen bonds and the neutalized charged groups remain at the solution air interface another physical property method involve measuring the solvation free energy the solvation free energy is estimated as a product of an accessibility of an atom to the solvent and an atomic solvation parameter results indicate the solvation free energy lowers by an average of kcal residue upon folding recent applications palliser and parry have examined about scales and found that they can use them for locating b strands on the surface of proteins hydrophobicity scales were also used to predict the preservation of the genetic code trinquier observed a new order of the bases that better reflect the conserved character of the genetic code they believed new ordering of the bases was uracil guanine cystosine adenine ugca better reflected the conserved character of the genetic code compared to the commonly seen ordering ucag whole residue hydrophobicity scales the whole residue hydrophobicity scales are significant for two reasons first they include the contributions of the peptide bonds as well as the sidechains providing absolute values second they are based on direct experimentally determined values for transfer free energies of polypeptides two whole residue hydrophobicity scales have been measured one for the transfer of unfolded chains from water to the bilayer interface referred to as the interfacial hydrophobicity scale and one for the transfer of unfolded chains into octanol which is relevant to the hydrocarbon core of a bilayer the stephen h white website provides an example of whole residue hydrophobicity scales showing the free energy of transfer kcal mol from water to popc interface and to n octanol these two scales are then used together to make whole residue hydropathy plots the hydropathy plot constructed using shows favorable peaks on the absolute scale that correspond to the known tm helices thus the whole residue hydropathy plots proved the fact that transmembrane segments prefer a transmembrane location rather than a surface one 
cholesterol depletion is when cholesterol levels in the body have been artificially lowered too far natural low cholesterol levels and associated clinical symptoms are defined as the hypocholesterolemia medically induced hypocholesterolemia is increasingly associated in the elderly with long term statin use the inhibition of de novo cholesterol is associated with functional failure of cholesterol rich lipid rafts in processes such as exocytosis and endocytosis cholesterol mediated membrane processes with the emergence of the lipid raft hypothesis the role of cholesterol in membrane function became a focus of new research into exocytosis and endocytosis it was later clarified that the rafts were cholesterol and sphingolipid based domains supporting a variety of trans membrane functions the cholesterol has been shown to condense the domain and stabilises its functional structure the physical consequences of cholesterol enrichment on strength and thickness of lipid rafts were modelled and demonstrated by de meyer et al effect of cholesterol lowering on cell function the mediation of lipid membrane form and function by cholesterol affect the ability of a cell to perform exocytosis and endocytosis the current trend in cardiovascular medicine to promote cholesterol reduction has caused a number of researchers in other fields to comment on the non cardiovascular effect this has on lipid raft functions in cell membranes cytoskeleton depletion of membrane cholesterol has been shown to activate the formation of stress fibers and the membrane cholesterol level was shown to be a critical regulator of membrane cytoskeletal dynamics and function membrane cholesterol a key enzyme target for the control of cholesterol biosynthesis is hmg coa reductase which is found in membrane walls of the endoplasmic reticulum and the mitochondrion wall this is significant because the cell membrane contains between and cholesterol molecules large amounts of de novo cholesterol are required to create the form and function of the membranes throughout the cell snare protein processes and vesicle associated membrane protein vamp processes exocytosis endocytosis and ion channel all require cholesterol rich lipid rafts to function 
a zero mode waveguide is an optical waveguide that guides light energy into a volume that is small in all dimensions compared to the wavelength of the light zero mode waveguides have been developed for rapid parallel sensing of zeptolitre sample volumes as applied to gene sequencing by pacific biosciences previously named nanofluidics inc a waveguide operated at frequencies lower than its cutoff frequency wavelengths longer than its cutoff wavelength and used as a precision attenuator is also known as a waveguide below cutoff attenuator 
low frequency collective motion in proteins and dna refers to the application of statistical thermodynamics to understand low frequency vibrations in biomolecules the concept of low frequency phonons or internal motion in proteins was originally proposed by professor kuo chen chou and professor nian yi chen in order to solve a perplexing energy problem in protein binding in studying the binding interaction between proteins such as insulin and insulin receptor it was noted that enumerating the known explanations for the free energy change such as translational and rotational entropy hydrogen bonds van der waals interactions and hydrophobic interactions did not fully account for the observed free energy change for the reaction it was inferred that the deficit could be explained by the creation of extra vibrational modes with very low wave numbers in the range of corresponding to the range of terahertz frequency to hz subsequently the aforementioned low frequency modes have been indeed observed by raman spectroscopy for a number of protein molecules and different types of dna these observed results have also been further confirmed by neutron scattering experiments quasi continuum model the quasi continuum model is one model developed to identify and analyze this kind of low frequency motions in protein and dna molecules this model operates on an intermediate level of complexity between the elastic global model which treats the biomolecule as a continuous elastic sphere and atomistic normal mode methods it treats the biomolecule s backbone as a continuous mass distribution with discrete interactions representing hydrogen bonds modeling the effects of internal conformation this has the advantage of being simpler than explicit atom methods and providing a much more intuitive physical picture of the dynamics involved it has been successfully used to simulate various low frequency collective motions in protein and dna molecules such as accordion like motion pulsation or breathing motion as reflected by the fact that the low frequency wave numbers thus derived were quite close to the experimental observations application to biological functions and medical treatments many biological functions and their profound dynamic mechanisms can be revealed through the low frequency collective motion or resonance in protein and dna molecules such as cooperative effects allosteric transition and intercalation of drugs into dna in this regard some phenomenological theories were established meanwhile the solitary wave motion was also used to address the internal motion during microtubule growth the relationship between self reinforcing solitary wave a wave packet or pulse that maintains its shape while it travels at constant the low frequency phonons in proteins have been discussed in a recent paper this kind of low frequency collective motion has also been observed in calmodulin by nmr and applied in medical treatments 
a catch bond is a type of noncovalent bond whose dissociation lifetime increases with tensile force applied to the bond normally bond lifetimes are expected to diminish with force in the case of catch bonds the lifetime of the bond actually increases up to a maxima before it decreases like in a normal bond catch bonds were suspected for many years to play a role in the rolling of leukocytes being strong enough to roll in presence of high forces caused by high shear stresses while avoiding getting stuck in capillaries where the fluid flow and therefore shear stress is low the existence of catch bonds was debated for many years until strong evidence of their existence was found in bacteria definite proof of their existence came shortly thereafter in leukocytes discovery catch bonds were first proposed in in the proceedings of the royal society by m dembo et al while at harvard university the concept of catch bonds were used to explain unexpected observations in which the strength of certain bonds increased as force was applied no decisive evidence of catch bonds was found until then marshall and coworkers found that l selectin psgl bonds exhibited increasing bond lifetime as step loads were applied between and pn and fell exponentially at higher loads these data were collected using an atomic force microscope and have subsequently been duplicated using a biomembrane force probe and in shear flow assays l selectin psgl bonds display catch bond behavior at low loads and slip bond behavior meaning that bond lifetime decreases with increasing load at high loads catch slip bonds catch slip behavior has also been reported for dissociation of l selectin from endoglycan p selectin from psgl fimh from mannose and myosin from actin emphasizing their importance and general acceptance in the three years following their discovery there were at least articles published on catch bonds 
biophysical chemistry is a relatively new branch of chemistry that covers a broad spectrum of research activities involving biological systems the most common feature of the research in this subject is to seek explanation of the various phenomena in biological systems in terms of either the molecules that make up the system or the supra molecular structure of these systems biophysical chemists employ various techniques used in physical chemistry to probe the structure of biological systems these techniques include spectroscopic methods like nuclear magnetic resonance nmr and x ray diffraction for example the work for which nobel prize was awarded in to three chemists was based on x ray diffraction studies of ribosomes some of the areas in which biophysical chemists engage themselves are protein structure and the functional structure of cell membranes for example enzyme action can be explained in terms of the shape of a pocket in the protein molecule that matches the shape of the substrate molecule or its modification due to binding of a metal ion similarly the structure and function of the biomembranes may be understood through the study of model supramolecular structures as liposomes or phospholipid vesicles of different compositions and sizes the oldest reputed institute for biophysical chemistry is max planck institute for biophysical chemistry in 
the near infrared nir window also known as optical window or therapeutic window defines the range of wavelengths where light has its maximum depth of penetration in tissue within the nir window scattering is the most dominant light tissue interaction and therefore the propagating light becomes diffused rapidly since scattering increases the distance travelled by photons within tissue the probability of photon absorption also increases because scattering has weak dependence on wavelength the nir window is primarily limited by the light absorption of blood at short wavelengths and water at long wavelengths the technique using this window is called nirs absorption properties of tissue components the absorption coefficient formula is defined as the probability of photon absorption in tissue per unit path length different tissue components have different formula values moreover formula is a function of wavelength below are discussed the absorption properties of the most important chromophores in tissue blood blood consists of two different types of hemoglobin oxyhemoglobin formula is bound to oxygen while deoxyhemoglobin formula is unbound to oxygen these two different types of hemoglobin exhibit different absorption spectra that are normally represented in terms of molar extinction coefficients as shown in figure the molar extinction coefficient of hb has its highest absorption peak at and a second peak at its spectrum then gradually decreases as light wavelength increases on the other hand formula shows its highest absorption peak at and two secondary peaks at and as light wavelengths passes formula absorption decays much faster than hb absorption the points where the molar extinction coefficient spectra of formula and formula intersect are called isosbestic points here formula and formula are the two wavelengths formula and formula are the molar extinction coefficients of formula and formula respectively formula and formula are the molar concentrations of formula and formula in tissue respectively oxygen saturation formula can then be computed as water although water is nearly transparent in the range of visible light it becomes absorbing over the near infrared region water is a critical component since its concentration is high in human tissue the absorption spectrum of water in the range from to is shown in figure although absorption is rather low in this spectral range it still contributes to the overall attenuation of tissue other tissue components with less significant contributions to the total absorption spectrum of tissue are melanin and fat melanin melanin is a chromophore that exists in the human epidermal layer of skin responsible for protection from harmful uv radiation when melanocytes are stimulated by solar radiation melanin is produced melanin is one of the major absorbers of light in some biological tissue although its contribution is smaller than other components there are two types of melanin eumelanin which is black brown and pheomelanin which is red yellow the molar extinction coefficient spectra corresponding to both types are shown in figure fat fat is one of the major components in tissue that can comprise of tissue although not many mammalian fat spectra are available figure shows an example extracted from scattering properties of tissue components optical scattering occurs due to mismatches in refractive index of the different tissue components ranging from cell membranes to whole cells cell nuclei and mitochondria are the most important scatterers their dimensions range from to and thus fall within the nir window most of these organelles fall in the mie regime and exhibit highly anisotropic forward directed scattering light scattering in biological tissue is denoted by the scattering coefficient formula which is defined as the probability of photon scattering in tissue per unit path length figure shows a plot of the scattering spectrum effective attenuation coefficient attenuation of light in deep biological tissue depends on the effective attenuation coefficient formula which is defined as where formula is the transport scattering coefficient defined as where formula is the anisotropy of biological tissue which has a representative value of the effective attenuation coefficient is the dominant factor for determining light attenuation at depth formula formula estimation of the nir window in tissue nir window can be computed based on the absorption coefficient spectrum or the effective attenuation coefficient spectrum a possible criterion for selecting the nir window is given by the fwhm of the inverse of these spectra oxygen saturation will define the concentration of oxy and deoxyhemoglobin in tissue and so the total absorption spectrum depending on the type of tissue we can consider different situations absorption spectrum for arteries in this case formula arterial oxygen saturation then oxyhemoglobin will be dominant in the total absorption black and the effective attenuation magenta coefficient spectra as shown in figure a absorption spectrum for veins in this case formula venous oxygen saturation then oxyhemoglobin and deoxyhemoglobin will have similar contributions to the total absorption black and the effective attenuation magenta coefficient spectra as shown in figure b absorption spectrum for brain tissue to define formula tissue oxygen saturation or formula tissue saturation index it is necessary to define a distribution of arteries and veins in tissue for brain tissue the ratio is given by for arteries and veins respectively thus tissue oxygen saturation can be defined as formula x formula x formula the total absorption black and the effective attenuation magenta coefficient spectra for brain tissue is shown in figure c 
the optical window also known as therapeutic window defines the range of wavelengths where light has its maximum depth of penetration in tissue within the nir window scattering is the most dominant light tissue interaction and therefore the propagating light becomes diffuse rapidly since scattering increases the distance travelled by photons within tissue the probability of photon absorption also increases because scattering has weak dependence on wavelength the optical window is primarily limited by absorption due to either blood at short wavelengths or water at long wavelengths the technique using this window is called nirs absorption properties of tissue components the absorption coefficient formula is defined as the probability of photon absorption in tissue per unit path length different tissue components have different formula values moreover formula is a function of wavelength below are discussed the absorption properties of the most important chromophores in tissue also molar extinction coefficient formula is another parameter that is often used to describe photon absorption in tissue by multiplying e by the molar concentration and by ln one can convert formula to formula blood blood consists of two different types of hemoglobin oxyhemoglobin formula is bound to oxygen while deoxyhemoglobin formula is unbound to oxygen these two different types of hemoglobin exhibit different absorption spectra that are normally represented in terms of molar extinction coefficients as shown in figure the molar extinction coefficient of hb has its highest absorption peak at nm and a second peak at nm its spectrum then gradually decreases as light wavelength increases on the other hand formula shows its highest absorption peak at nm and two secondary peaks at nm and nm as light wavelengths passes nm formula absorption decays much faster than hb absorption the points where the molar extinction coefficient spectra of formula and formula intersect are called isosbestic points here formula and formula are the two wavelengths formula and formula are the molar extinction coefficients of formula and formula respectively formula and formula are the molar concentrations of formula and formula in tissue respectively oxygen saturation formula can then be computed as water although water is nearly transparent in the range of visible light it becomes absorbing over the near infrared region water is a critical component since its concentration is high in human tissue the absorption spectrum of water in the range from to nm is shown in figure other tissue components with less significant contributions to the total absorption spectrum of tissue are melanin and fat melanin melanin is a chromophore that exists in the human epidermal layer of skin responsible for protection from harmful uv radiation when melanocytes are stimulated by solar radiation melanin is produced melanin is one of the major absorbers of light in some biological tissue although its contribution is smaller than other components there are two types of melanin eumelanin which is black brown and pheomelanin which is red yellow the molar extinction coefficient spectra corresponding to both types are shown in figure fat fat is one of the major components in tissue that can comprise of tissue although not many mammalian fat spectra are available figure shows an example extracted from this plot was obtained from clear purified oil from pig lard the detailed purification process is explained in scattering properties of tissue components optical scattering occurs due to mismatches in refractive index of the different tissue components ranging from cell membranes to whole cells cell nuclei and mitochondria are the most important scatterers their dimensions range from nm to most of these organelles exhibit highly forward directed scattering light scattering in biological tissue is denoted by the scattering coefficient formula which is defined as the probability of photon scattering in tissue per unit path length effective attenuation coefficient attenuation of light in deep biological tissue depends on the effective attenuation coefficient formula which is defined as where formula is the transport scattering coefficient defined as where formula is the anisotropy of biological tissue which has a representative value of figure shows a plot of transport scattering coefficient spectrum in breast tissue which has a wavelength dependence of formula the effective attenuation coefficient is the dominant factor for determining light attenuation at depth formula formula estimation of the optical window in tissue the optical window can be computed based on the absorption coefficient spectrum or the effective attenuation coefficient spectrum a possible criterion for selecting the nir window is given by the fwhm of the inverse of these spectra as shown in figure in addition to the total concentration of hemoglobin the oxygen saturation will define the concentration of oxy and deoxyhemoglobin in tissue and so the total absorption spectrum depending on the type of tissue we can consider different situations below the total concentration of hemoglobin is assumed to be mm absorption spectrum for arteries in this case formula arterial oxygen saturation then oxyhemoglobin will be dominant in the total absorption black and the effective attenuation magenta coefficient spectra as shown in figure a absorption spectrum for veins in this case formula venous oxygen saturation then oxyhemoglobin and deoxyhemoglobin will have similar contributions to the total absorption black and the effective attenuation magenta coefficient spectra as shown in figure b absorption spectrum for breast tissue to define formula tissue oxygen saturation or formula tissue saturation index it is necessary to define a distribution of arteries and veins in tissue an arterial venous blood volume ratio of can be adopted thus tissue oxygen saturation can be defined as formula x formula x formula the total absorption black and the effective attenuation magenta coefficient spectra for breast tissue is shown in figure c in addition the effective penetration depth is plotted in figure 
optical mapping is a technique for constructing ordered genome wide high resolution restriction maps from single stained molecules of dna called optical maps by mapping the location of restriction enzyme sites along the unknown dna of an organism the spectrum of resulting dna fragments collectively serve as a unique fingerprint or barcode for that sequence originally developed by dr david c schwartz and his lab at nyu in the this method has since been integral to the assembly process of many large scale sequencing projects for both microbial and eukaryotic genomes technology the modern optical mapping platform works as follows history of optical mapping platform early system dna molecules were fixed on molten agarose developed between a cover slip and a microscope slide restriction enzyme was pre mixed with the molten agarose before dna placement and cleavage was triggered by addition of magnesium using charged surfaces rather than being immobilized within a gel matrix dna molecules were held in place by electrostatic interactions on a positively charged surface resolution improved such that fragments from to as small as could sized automated system this involved the development and integration of an automated spotting system to spot multiple single molecules on a slide like a microarray for parallel enzymatic processing automated fluorescence microscopy for image acquisition image procession vision to handle images algorithms for optical map construction cluster computing for processing large amounts of data high throughput system using microfluidics observing that microarrays spotted with single molecules did not work well for large genomic dna molecules microfluidic devices using soft lithography possessing a series of parallel microchannels was developed next generation system using nanocoding technology an improvement on optical mapping called nanocoding has potential to boost throughput by trapping elongated dna molecules in nanoconfinements comparisons other mapping techniques the advantage of om over traditional mapping techniques is that it preserves the order of the dna fragment whereas the order needs to be reconstructed using restriction mapping in addition since maps are constructed directly from genomic dna molecules cloning or pcr artifacts are avoided however each om process is still affected by false positive and negative sites because not all restriction sites are cleaved in each molecule and some sites may be incorrectly cut in practice multiple optical maps are created from molecules of the same genomic region and an algorithm is used to determine the best consensus map other genome analysis methods there are a variety of approaches to identifying large scale genomic variations such as indels duplications inversions translocations between genomes other categories of methods include using microarrays pulsed field gel electrophoresis cytogenetics and paired end tags uses initially the optical mapping system has been used to construct whole genome restriction maps of bacteria parasites and fungi it has also been used to scaffold and validate bacterial genomes to serve as scaffolds for assembly assembled sequence contigs can be scanned for restriction sites in silico using known sequence data and aligning them to the assembled genomic optical map commercial company opgen has provided optical mappings for microbial genomes for larger eukaryotic genomes only the david c schwartz lab now at madison wisconsin has produced optical maps for mouse human rice and maize optical sequencing optical sequencing is a single molecule dna sequencing technique that follows sequence by synthesis and uses optical mapping technology similar to other single molecular sequencing approaches such as smrt sequencing this technique analyzes a single dna molecule rather than amplify the initial sample and sequence multiple copies of the dna during synthesis fluorochrome labeled nucleotides are incorporated through the use of dna polymerases and tracked by fluorescence microscopy this technique was originally proposed by david c schwartz and arvind ramanathan in optical sequencing cycle the following is an overview of each cycle in the optical sequencing process step dna barcoding cells are lysed to release genomic dna these dna molecules are untangled placed onto optical mapping surface containing microfluidic channels and the dna is allowed to flow through the channels these molecules are then barcoded by restriction enzymes to allow for genomic localization through the technique of optical mapping see the above section on technology for those steps step template nicking dnase i is added to randomly nick the mounted dna molecules a wash is then performed to remove the dnase i the mean number of nicks that occur per template is dependent on the concentration of dnase i as well as the incubation time step gap formation exonuclease is added which uses the nicks in the dna molecules to expand the gaps in a direction amount of exonuclease must be carefully controlled to avoid overly high levels of double stranded breaks step fluorochrome incorporation dna polymerase is used to incorporate fluorochrome labelled nucleotides fdntps into the multiple gapped sites along each dna molecule during each cycle the reaction mixture contains a single type of fdntp and allows for multiple additions of that nucleotide type various washes are then performed to remove unincorporated fdntps in preparation for imaging and the next cycle of fdntp addition step imaging this step counts the number of incorporated fluorochrome labeled nucleotides at the gap regions using fluorescence microscopy step photobleaching the laser illumination that is used to excite the fluorochrome is also used here to destroy the fluorochrome signal this essentially resets the fluorochrome counter and prepares the counter for the next cycle this step is a unique aspect of optical sequencing as it does not actually remove the fluorochrome label of the nucleotide after its incorporation not removing the fluorochrome label makes sequencing more economical but it results in the need to incorporate fluorochrome labels consecutively which can result in problems due to the bulkiness of the labels step repeat steps steps are repeated with step using a reaction mixture that contains a different fluorochrome labeled nucleotide fdntp each time this is repeated until the desired region is sequenced optimization strategies in addition different polymerase preference for different fluorochromes linker length on fluorochrome nucleotides and buffer compositions are also important factors to be considered to optimize the base addition process and maximize number of consecutive fdntp incorporations advantages single molecule analysis since minimal dna sample required time consuming and costly amplification step is avoided to streamline sample preparation process large dna molecule templates kb vs short dna molecule templates cracker team the cracker team is a taiwanese based research team led by engineer chung fan chiou that uses fluorescence based optical genome sequencing technology in combination with nanopore methodology they are currently part of archon x prize for genomics competition there approach relies on a novel optical design technique called sequencing on top e http www biotechniques com news q and a hubert renauld talks about cracker taiwans new genome sequencing team biotechniques html q a hubert renauld talks about cracker taiwan s new genome sequencing team ref their technique does not rely on any optical microscopes or the capturing of numerous images additionally where current optical detection systems are limited in the number of observable reactions the stop chip can handle the millions of nanosequencing information happening at the same time their aim is to have a prototype by and be commercially available by 
the gibbs society of biological thermodynamics is composed of scientists involved in the field of biophysics with the group meeting annually in the fall recent meetings have been held at the touch of nature conference center in carbondale illinois current incarnations have begun with a meet and greet on saturday night followed by two and a half days of lectures and two nights of poster sessions poster sessions often end with several attendees creating a bonfire at a nearby beach the last night of the meeting is known for its food a selection of beef cooked over an open flame with attendees affectionately calling it the buffalo tro the annual meeting of the gibbs society of biological thermodynamics will be held from september the keynote speaker is c nick pace texas a m university in the gary k ackers lecture in biothermodynamics was instituted with michael brenowitz giving the inaugural lecture tim lohman washington university in st louis will be giving the ackers lecture at the meeting 
the secondary structure of a nucleic acid molecule refers to the basepairing interactions within a single molecule or set of interacting molecules and can be represented as a list of bases which are paired in a nucleic acid molecule the secondary structures of biological dna s and rna s tend to be different biological dna mostly exists as fully base paired double helices while biological rna is single stranded and often forms complicated base pairing interactions due to its increased ability to form hydrogen bonds stemming from the extra hydroxyl group in the ribose sugar in a non biological context secondary structure is a vital consideration in the rational design of nucleic acid structures for dna nanotechnology and dna computing since the pattern of basepairing ultimately determines the overall structure of the molecules fundamental concepts base pairing in molecular biology two nucleotides on opposite complementary dna or rna strands that are connected via hydrogen bonds are called a base pair often abbreviated bp in the canonical watson crick base pairing adenine a forms a base pair with thymine t as and guanine g with cytosine c in dna in rna thymine is replaced by uracil u alternate hydrogen bonding patterns such as the wobble base pair and hoogsteen base pair also in rise to complex and functional tertiary structures importantly pairing is the mechanism by which codons on messenger rna molecules are recognized by anticodons on transfer rna during protein translation some dna or rna binding enzymes can recognize specific base pairing patterns that identify particular regulatory regions of genes hydrogen bonding is the chemical mechanism that underlies the base pairing rules described above appropriate geometrical correspondence of hydrogen bond donors and acceptors allows only the right pairs to form stably dna with high gc content is more stable than dna with low gc content but contrary to popular belief the hydrogen bonds do not stabilize the dna significantly and stabilization is mainly due to stacking interactions the larger nucleobases adenine and guanine are members of a class of doubly ringed chemical structures called purines the smaller nucleobases cytosine and thymine and uracil are members of a class of singly ringed chemical structures called pyrimidines purines are only complementary with pyrimidines pyrimidine pyrimidine pairings are energetically unfavorable because the molecules are too far apart for hydrogen bonding to be established purine purine pairings are energetically unfavorable because the molecules are too close leading to overlap repulsion the only other possible pairings are gt and ac these pairings are mismatches because the pattern of hydrogen donors and acceptors do not correspond the gu wobble base pair with two hydrogen bonds does occur fairly often in rna nucleic acid hybridization hybridization is the process of complementary base pairs binding to form a double helix melting is the process by which the interactions between the strands of the double helix are broken separating the two nucleic acid strands these bonds are weak easily separated by gentle heating enzymes or physical force melting occurs preferentially at certain points in the nucleic acid t and a rich sequences are more easily melted than c and g rich regions particular base steps are also susceptible to dna melting particularly t a and t g base steps these mechanical features are reflected by the use of sequences such as tataa at the start of many genes to assist rna polymerase in melting the dna for transcription strand separation by gentle heating as used in pcr is simple providing the molecules have fewer than about base pairs kilobase pairs or kbp the intertwining of the dna strands makes long segments difficult to separate the cell avoids this problem by allowing its dna melting enzymes helicases to work concurrently with topoisomerases which can chemically cleave the phosphate backbone of one of the strands so that it can swivel around the other helicases unwind the strands to facilitate the advance of sequence reading enzymes such as dna polymerase secondary structure motifs nucleic acid secondary structure is generally divided into helices contiguous base pairs and various kinds of loops unpaired nucleotides surrounded by helices frequently these elements or combinations of them can be further classified for example tetraloops pseudoknots and stem loops double helix the double helix is an important tertiary structure in nuleic acid molecules which is intimately connected with the molecule s secondary structure a double helix is formed by regions of many consecutive base pairs the nucleic acid double helix is a spiral polymer usually right handed containing two nucleotide strands which base pair together a single turn of the helix constitutes about ten nucleotides and contains a major groove and minor groove the major groove being wider than the minor groove given the difference in widths of the major groove and minor groove many proteins which bind to dna do so through the wider major groove many double helical forms are possible for dna the three biologically relevant forms are a dna b dna and z dna while rna double helices have structures similar to the a form of dna stem loop structures the secondary structure of nucleic acid molecules can often be uniquely decomposed into stems and loops the stem loop structure in which a base paired helix ends in a short unpaired loop is extremely common and is a building block for larger structural motifs such as cloverleaf structures which are four helix junctions such as those found in transfer rna internal loops a short series of unpaired bases in a longer paired helix and bulges regions in which one strand of a helix has extra inserted bases with no counterparts in the opposite strand are also frequent there are many secondary structure elements of functional importance to biological rna s some famous examples are the rho independent terminator stem loops and the trna cloverleaf there is a minor industry of researchers attempting to determine the secondary structure of rna molecules approaches include both experimental and computational methods see also the list of rna structure prediction software pseudoknots a pseudoknot is a nucleic acid secondary structure containing at least two stem loop structures in which half of one stem is intercalated between the two halves of another stem pseudoknots fold into knot shaped three dimensional conformations but are not true topological knots the base pairing in pseudoknots is not well nested that is base pairs occur that overlap one another in sequence position this makes the presence of general pseudoknots in nucleic acid sequences impossible to predict by the standard method of dynamic programming which uses a recursive scoring system to identify paired stems and consequently cannot detect non nested base pairs with the most common algorithms limited subclasses of pseudoknots can be predicted using dynamic programs described in newer structure prediction techniques such as stochastic context free grammars also do not take pseudoknots into account several important biological processes rely on rna molecules that form pseudoknots for example the rna component of human telomerase contains a pseudoknot that is critical for activity though dna can also form pseudoknots they are generally not present in biological dna secondary structure prediction most methods for nucleic acid secondary structure prediction rely on a nearest neighbor energy model a general method of calculating probable nucleic acid secondary structure is dynamic programming which is used to calculate structures by optimizing the thermodynamic free energy dynamic programming algorithms often forbid pseudoknots or other cases in which base pairs are not fully nested as considering these structures becomes computationally very expensive for even small nucleic acid molecules other methods such as stochastic context free grammars can also be used to predict nucleic acid secondary structure for many rna molecules the secondary structure is highly important to the correct function of the rna often more so than the actual sequence this fact aids in the analysis of non coding rna sometimes termed rna genes one application of bioinformatics uses predicted rna secondary structures in searching a genome for noncoding but functional forms of rna for example micrornas have canonical long stem loop structures interrupted by small internal loops 
mccutcheon index or chemotactic ratio is the ratio of the net displacement of a bacterium to the length of its actual path it is named after morton mccutcheon who introduced it to describe chemotaxis in leukocytes 
the model describes a lipid membrane as a thin layer of viscous fluid surrounded by a less viscous bulk liquid this picture was originally proposed to determine the diffusion coefficient of membrane proteins but has also been used to describe the dynamics of fluid domains within lipid membranes the formula is often applied to determine the size of an object embedded in a membrane from its observed diffusion coefficient and is characterized by the weak logarithmic dependence of diffusion constant on object radius origin in a three dimensional highly viscous liquid a spherical object of radius a has diffusion coefficient philip saffman and max calculated the diffusion coefficient for these three cases and showed that case was the relevant effect formula where the length formula and formula is the constant typical values of formula are to micrometres this result is an approximation applicable for radii formula which is appropriate for proteins formula nm but not for micrometre scale lipid domains the formula predicts that diffusion coefficients formula will only depend weakly on the size of the embedded object for example if formula changing formula from to only reduces the diffusion coefficient formula beyond the length hughes pailthorpe and white extended the theory of saffman and to inclusions with any radii formula for formula a useful formula that produces the correct diffusion coefficients between these two limits is where formula formula formula formula and formula experimental studies though the formula is commonly used to infer the sizes of nanometer scale objects recent experiments on proteins have suggested that the diffusion coefficient s dependence on radius formula should be formula instead of formula however for larger objects such as micrometre scale lipid domains the model with the extensions above is well established 
binary subcomplexes in proteins database bisc is a interaction database about binary subcomplexes 
diprodb is a database designed to collect and analyse thermodynamic structural and other dinucleotide properties 
regtransbase is database of regulatory interactions and transcription factor binding sites in prokaryotes 
magnetomyography mmg is a technique for mapping muscle activity by recording magnetic fields produced by electrical currents occurring naturally in the muscles using arrays of squids superconducting quantum interference devices it has a better capability than electromyography for detecting slow or direct currents history the development of this technique has been influenced by the development of squids 
the dragon database for human transcription co factors and transcription factor interacting proteins tcof db is a database that facilitates the exploration of proteins involved in the regulation of transcription in humans by binding to regulatory dna regions transcription factors and proteins involved in the regulation of transcription in humans by interacting with transcription factors and not binding to regulatory dna regions transcription co factors the database describes a total of potential human transcription co factors interacting with a total of human transcription factors 
flyfactorsurvey is a database of drosophila transcription factors primarily determined using the bacterial one hybrid system 
non b db is a database integrating annotations and analysis of non b dna forming sequence motifs 
hitpredict is a database of high confidence protein protein interactions 
in physics and chemistry and related fields a kinetic scheme is a network of states and connections among the states representing the scheme of a dynamical process usually a kinetic scheme represents a markovian process where when the process is not markovian the scheme is a generalized kinetic scheme see fig for an illustration of a kinetic scheme a markovian kinetic scheme description of the form of a kinetic scheme a kinetic scheme is a network of states each state is special usually has a special number representing a specific state in the system although repetitions of states may occur and this depends on the system each pair of connected states has at least one rate a rate formula is directional and connects states i with j indeed when detailed balance exists in a system the following relation holds formula for every connected states i and j the result represents the fact that any closed loop in a markovian network in equilibrium does not have a net flow mathematical description the kinetic scheme is described with a master equation a first order differential equation for the probability of a system to occupy each one its states at time t written in a matrix form we see formula where formula is a column vector where element i represents state i and formula is the matrix of connections in a markovian kinetic scheme the connections are simply numbers and any jumping time probability density function for state i is an exponential with a rate equal the value of all the exiting connections matrix formula can also represent birth and death meaning that probability is injected birth or taken from death the system where then the process is not in equilibrium these terms are different than a birth death process where there we have simply a linear kinetic scheme generalizations of markovian kinetic schemes an example for such a process is a reduced dimensions form 
cellular noise refers to random variability in quantities within cellular biology for example cells which are genetically identical even within the same tissue are often observed to have different expression levels of proteins different sizes and structures these apparently random differences can have important biological and medical consequences cellular noise was originally and is still often examined in the context of gene expression levels either the concentration or copy number of the products of genes within and between cells as gene expression levels are responsible for many fundamental properties in cellular biology including cells physical appearance behaviour in response to stimuli and ability to process information and control internal processes the presence of noise in gene expression has profound implications for many processes in cellular biology definitions formula where formula is the noise in a quantity formula formula is the mean value of formula and formula is the standard deviation of formula this measure is dimensionless allowing a relative comparison of the importance of noise without necessitating knowledge of the absolute mean formula intrinsic and extrinsic noise cellular noise is often investigated in the framework of intrinsic and extrinsic noise intrinsic noise refers to variation in identically regulated quantities within a single cell for example the intra cell variation in expression levels of two identically controlled genes extrinsic noise refers to variation in identically regulated quantities between different cells for example the cell to cell variation in expression of a given gene intrinsic and extrinsic noise levels are often compared in dual reporter studies in which the expression levels of two identically regulated genes often fluorescent reporters like gfp and yfp are plotted for each cell in a population sources of cellular noise note these lists are illustrative not exhaustive and identification of noise sources is an active and expanding area of research note that extrinsic noise can affect levels and types of intrinsic noise for example extrinsic differences in the mitochondrial content of cells lead through differences in atp levels to some cells transcribing faster than others affecting the rates of gene expression and the magnitude of intrinsic noise across the population effects of cellular noise note these lists are illustrative not exhaustive and identification of noise effects is an active and expanding area of research analysis as many quantities of cell biological interest are present in discrete copy number within the cell single dnas dozens of mrnas hundreds of proteins tools from discrete stochastic mathematics are often used to analyse and model cellular noise in particular master equation treatments where the probabilities formula of observing a system in a state formula at time formula are linked through odes have proved particularly fruitful a canonical model for noise gene expression where the processes of dna activation transcription and translation are all represented as poisson processes with given rates gives a master equation which may be solved exactly with generating functions under various assumptions or approximated with stochastic tools like van kampen s system size expansion numerically the gillespie algorithm or stochastic simulation algorithm is often used to create realisations of stochastic cellular processes from which statistics can be calculated the problem of inferring the values of parameters in stochastic models parametric inference for biological processes which are typically characterised by sparse and noisy experimental data is an active field of research with methods including bayesian mcmc and approximate bayesian computation proving adaptable and robust 
clinical biophysics is that branch of medical science that studies the action process and the effects of non ionising physical energies utilised for therapeutic purposes physical energy can be applied for diagnostic or therapeutic aims definition several papers show that the response of a biological system when exposed to non ionizing physical stimuli is not necessarily dependent on the amount of energy applied specific combinations of amplitude frequency and waveform may trigger the most intense response for example cell proliferation or activation of metabolic pathways several pre clinical experiences have laid the foundation to identify exposure conditions that may be used in humans to treat diseases or to promote tissue healing the identification of the best parameters to apply in any particular circumstance is the current goal of research activities in the field 
nucleic acid nmr is the use of nuclear magnetic resonance spectroscopy to obtain information about the structure and dynamics of nucleic acid molecules such as dna or rna it is useful for molecules of up to nucleotides and as of nearly half of all known rna structures had been determined by nmr spectroscopy nmr has advantages over x ray crystallography which is the other method for high resolution nucleic acid structure determination in that the molecules are being observed in their natural solution state rather than in a crystal lattice that may affect the molecule s structural properties it is also possible to investigate dynamics with nmr this comes at the cost of slightly less accurate and detailed structures than crystallography nucleic acid nmr uses techniques similar to those of protein nmr but has several differences nucleic acids have a smaller percentage of hydrogen atoms which are the atoms usually observed in nmr and because nucleic acid double helices are stiff and roughly linear they do not fold back on themselves to give long range correlations nucleic acids also tend to have resonances distributed over a smaller range than proteins making the spectra potentially more crowded and difficult to interpret experimental methods two dimensional nmr methods are almost always used with nucleic acids these include correlation spectroscopy cosy and total coherence transfer spectroscopy tocsy to detect through bond nuclear couplings and nuclear overhauser effect spectroscopy noesy to detect couplings between nuclei that are close to each other in space the types of nmr usually done with nucleic acids are nmr nmr nmr and nmr nmr is also useful if nonnatural nucleotides such as fluoro deoxyadenosine are incorporated into the nucleic acid strand as natural nucleic acids do not contain any fluorine atoms and have near natural abundance while and have low natural abundances for these latter two nuclei there is the capability of isotopically enriching desired atoms within the molecules either uniformly or in a site specific manner nucleotides uniformly enriched in and or can be obtained through biochemical methods by performing polymerase chain reaction using dntps or ntps derived from bacteria grown in an isotopically enriched environment site specific isotope enrichment must be done through chemical synthesis of the labeled nucleoside phosphoramidite monomer and of the full strand however these are difficult and expensive to synthesize because nucleic acids have a relatively large number of protons which are solvent exchangable nucleic acid nmr is generally not done in solvent as is common with other types of nmr this is because the deuterium in the solvent would replace the exchangeable protons and extinguish their signal is used as a solvent and other methods are used to eliminate the strong solvent signal such as saturating the solvent signal before the normal pulse sequence presaturation which works best a low temperature to prevent exchange of the saturated solvent protons with the nucleic acid protons or exciting only resonances of interest selective excitation which has the additional potentially undesired effect of distorting the peak amplitudes structure determination the exchangeable and non exchageable protons are usually assigned to their specific peaks as two independent groups for exchangeable protons which are for the most part the protons involved in base pairing noesy can be used to find through space correlations between on neighboring bases allowing an entire duplex molecule to be assigned through sequential walking for nonexchangable protons many of which are on the sugar moiety of the nucleic acid cosy and tocsy are used to identify systems of coupled nuclei while noesy is again used to correlate the sugar to the base and each base to its neighboring base for duplex dna nonexchangeable protons the protons on the base correlate to their counterparts on neighboring bases and to the proton on the sugar allowing sequential walking to be done for rna the differences in chemical structure and helix geometry make this assignment more technically difficult but still possible the sequential walking methodology is not possible for non double helical nucleic acid structures nor for the z dna form making assignment of resonances more difficult parameters taken from the spectrum mainly noesy cross peaks and coupling constants can be used to determine local structural features such as glycosidic bond angles dihedral angles using the karplus equation and sugar pucker conformations the presence or absence of inimo proton resonances or of coupling between atoms across a hydrogen bond indicates the presence or absence of basepairing for large scale structure these local parameters must be supplemented with other structural assumptions or models because errors add up as the double helix is traversed and unlike with proteins the double helix does not have a compact interior and does not fold back upon itself however long range orientation information can be obtained through residual dipolar coupling experiments in a medium which imposes a weak alignment on the nucleic acid molecules nmr is also useful for investigating nonstandard geometries such as bent helices non basepairing and coaxial stacking it has been especially useful in probing the structure of natural rna oligonucleotides which tend to adopt complex conformations such as stem loops and pseudoknots interactions between rna and metal ions can be probed by a number of methods including observing changes in chemical shift upon ion binding observing line broadening for paramagnetic ion species and observing intermolecular noe contacts for organometallic mimics of the metal ions nmr is also useful for probing the binding of nucleic acid molecules to other molecules such as proteins or drugs this can be done by chemical shift mapping which is seeing which resonances are shifted upon binding of the other molecule or by cross saturation experiments where one of the binding molecules is selectively saturated and if bound the saturation transfers to the other molecule in the complex dynamic properties such as strand equilibria and binding rates of other molecules to duplexes can also be determined by its effect on the relaxation time t but these methods are insensitive to intermediate rates of s which must be investigated with other methods such as solid state nmr dynamics of mechanical properties of a nucleic acid double helix such as bending and twisting can also be studied using nmr pulsed field gradient nmr experiments can be used to measure diffusion constants history early nucleic acid nmr studies were preformed as early as and focused on using imino proton resonances to probe base pairing interactions such as in trna with the advent of oligonucleotide synthesis the first nmr spectrum of double helical dna was published in and methods for sequential assignment of the resonances were published the following year 
friedrich hans beck february december was a german physicist his research interests were focused on superconductivity nuclear and elementary particle physics relativistic quantum field theory and late in his life biophysics and theory of consciousness early life and education beck was born in wiesbaden germany he was the son of the businessman fritz beck and his wife margaret cron beck attended the grammar school in darmstadt and after that studied physics at university of and darmstadt university of technology as a student of max von laue he performed research on superconductivity in the spring of beck started work on his phd thesis entitled the electrodynamic potential in the extended phenomenological theory of superconductivity which he defended at university of and obtained doctor rerum naturalium academic career from to beck worked as an assistant at the fritz haber institute in berlin followed a research visit in the u s from to as a research associate at the massachusetts institute of technology then beck went to the university of munich where in he wrote a habilitation thesis on nuclear reactions as a result of electromagnetic interactions from to he worked as a lecturer both at the university of munich and heidelberg university in beck was appointed an associate professor of theoretical physics at goethe university frankfurt in he became a professor of theoretical physics at darmstadt university of technology where in the same year he took over the management of the institute for theoretical nuclear physics beck held visiting professorship positions several times from to he taught at the lawrence berkeley national laboratory in at the universidade federal rural do rio de janeiro in at the university of maryland college park in at the weizmann institute of science in rehovot in at the university of washington in seattle in at the ben gurion university of the negev in beersheba and in at the university of the witwatersrand in johannesburg after beck s retirement in his successor at darmstadt university of technology became professor jochen wambach collaboration with john c eccles in friedrich beck met sir john carew eccles a nobel laureate in physiology or medicine during a summer school in northern italy organized by a german foundation for the promotion of outstanding students in collaboration they developed a quantum mechanical model of exocytosis and neurotransmitter release at synapses in the human cerebral cortex the model endorses interactionist dualism and postulates that human consciousness could affect the functioning of synapses in the brain through quantum tunneling of electrons between the lipid bilayers of the synaptic vesicle and the presynaptic membrane the tunneling of electrons triggers the process of exocytosis and thus initiates the transmission of information from the presynaptic neuron towards the postsynaptic neuron 
astrobiophysics is a field of intersection between astrophysics and biophysics concerned with the influence of the astrophysical phenomena upon life on planet earth or some other planet in general it differs from astrobiology which is concerned with the search of extraterrestrial life 
biophysics interdisciplinary science that uses the methods of physical science to study biological systems biophysical techniques biophysical techniques methods used for gaining information about biological systems on an atomic or molecular level they overlap with methods from many other branches of science 
cholesterol depletion is when cholesterol levels in the body have been artificially lowered too far natural low cholesterol levels and associated clinical symptoms are defined as the hypocholesterolemia iatrogenic or medically induced hypocholesterolemia is increasingly associated in the elderly with long term statin use the inhibition of de novo cholesterol is associated with functional failure of cholesterol rich lipid rafts in processes such as exocytosis and endocytosis cholesterol mediated membrane processes with the emergence of the lipid raft hypothesis the role of cholesterol in membrane function became a focus of new research into exocytosis and endocytosis it was later clarified that the rafts were cholesterol and sphingolipid based domains supporting a variety of trans membrane functions the cholesterol has been shown to condense the domain and stabilises its functional structure the physical consequences of cholesterol enrichment on strength and thickness of lipid rafts were modelled and demonstrated by de meyer et al effect of cholesterol lowering on cell function the mediation of lipid membrane form and function by cholesterol affect the ability of a cell to perform exocytosis and endocytosis the current trend in cardiovascular medicine to promote cholesterol reduction has caused a number of researchers in other fields to comment on the non cardiovascular effect this has on lipid raft functions in cell membranes cytoskeleton depletion of membrane cholesterol has been shown to activate the formation of stress fibers and the membrane cholesterol level was shown to be a critical regulator of membrane cytoskeletal dynamics and function membrane cholesterol a key enzyme target for the control of cholesterol biosynthesis is hmg coa reductase which is found in membrane walls of the endoplasmic reticulum and the mitochondrion wall this is significant because the cell membrane contains between and cholesterol molecules large amounts of de novo cholesterol are required to create the form and function of the membranes throughout the cell snare protein processes and vesicle associated membrane protein vamp processes exocytosis endocytosis and ion channel all require cholesterol rich lipid rafts to function self help organisations uk statin side effect sufferers 
the rna characterization of secondary structure motifs database rna cossmos is a repository of three dimensional nucleic acid pdb structures containing secondary structure motifs loops hairpin loops 
the european biophysical societies association is an association existing to promote biophysics in europe and to disseminate knowledge of the principles recent developments and applications of biophysics and to foster the exchange of scientific information among european biophysicists and biophysicists in general origins at the end of the there appeared to be an interest in creating a formal association of various existing european biophysical societies in november a meeting organized by the german biophysical society set the stage for developing concrete of the to promote scientific contacts and cooperation with the organization of joint meetings ebsa was established in at the iupab congress of biophysics held in bristol after representatives from european biophysical societies belgian british danish german italian netherlands swedish and swiss met to develop and sign a founding constitution and rules of association it was also in that the journal biophysics of structure and mechanism became the european biophysics journal published by springer verlag since ebj has been owned entirely by ebsa and it is currently available free of charge to its members ownership of the journal has provided a financial base from which ebsa undertakes many of its activities activities because biophysics is a unique combination of diverse scientific subjects particularly fertile with the advancement of biotechnology and quantitative and analytical methods in biology has a prolific range of activities geared towards bringing innovative people and ideas together every two years ebsa organizes the european biophysics congress the next congress will be held in lisbon portugal in the congress will be held in dresden germany in addition ebsa offers a variety of sponsorship to institutions and individuals working towards promoting biophysics in europe such as organisers of european biophysics meetings and schools with biophysics courses ebsa also offers bursaries to young scientists to attend scientific meetings and to participate in the biophysics congress as well as the young medal and prize which is awarded every congress in the recipient of the medal was justin molley from york uk in it was martin weik from grenoble france in it was gyorgy panyi from debrecen hungary in it was marc baldus from gottingen germany in it was michel valle from derio bizkaia spain and in it was kinneret keren organisation ebsa membership includes a total of societies throughout europe the executive committee is composed of ten members drawn from different member societies and includes a president vice president past president secretary treasurer and managing editor of ebj during a general assembly the voting representative of each member society and a non voting representative are invited to attend and consider a number of issues and the direction of 
this is a list of articles on biophysics 
hypnosis the power hypnosis the force non contact fight power hypnosis on distance nonverbal kill or neutralization victim citizen without touching the anatomical limbs and without the use of a known weapons by means of the invasion in the psyche the terminology introduced mr vladimir bekhterev secret technique of energy s fight psy agents psi operators and sleepers state s security services not is a clinical or therapeutic form techniques of hypnosis that used in official medicine this technique is not therapeutic hypnosis not is codified in law of any country in the world provides its owner use with impunity over defenseless citizens not is controlled by any law of application of this technique depends on the degree of moral maturity of the holder the image of the alleged junk science of parapsychology and unreality such techniques of hypnosis was created specifically in behalf of the self will state s special service intelligence service services criminalization business for the sake of lack of control civilian and democratic society control the information space disinformation about of this phenomenon the prohibition of publications documentary records them only in state s special service the lack of legislative acts have created the perfect crime situation for them did the difficulty of proof in a courts of hypnotic act 
